Automatically generated by Mendeley Desktop 1.19.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Hernan2006,
abstract = {In ideal randomised experiments, association is causation: association measures can be interpreted as effect measures because randomisation ensures that the exposed and the unexposed are exchangeable. On the other hand, in observational studies, association is not generally causation: association measures cannot be interpreted as effect measures because the exposed and the unexposed are not generally exchangeable. However, observational research is often the only alternative for causal inference. This article reviews a condition that permits the estimation of causal effects from observational data, and two methods -- standardisation and inverse probability weighting -- to estimate population causal effects under that condition. For simplicity, the main description is restricted to dichotomous variables and assumes that no random error attributable to sampling variability exists. The appendix provides a generalisation of inverse probability weighting.},
author = {Hern{\'{a}}n, Miguel A. and Robins, James M.},
booktitle = {Journal of Epidemiology and Community Health},
doi = {10.1136/jech.2004.029496},
isbn = {0143-005X (Print)$\backslash$n0143-005X (Linking)},
issn = {0143005X},
pmid = {16790829},
title = {{Estimating causal effects from epidemiological data}},
year = {2006}
}
@article{Greenland1996,
abstract = {Miettinen and Cook (Am J Epidemiol 1981; 114:593-603) showed that absence of confounding does not imply collapsibility of the odds ratio; that is, the crude odds ratio need not equal a common stratum-specific odds ratio even if the exposed and unexposed study groups have the same distribution of risk factors. Less well known is that absence of confounding does not correspond to collapsibility of the person-time rate ratio or rate difference. For example, two study groups can have the same distribution of all risk factors and yet the crude rate ratio need not equal a common stratum-specific rate ratio. The present paper provides an example and explanation of this phenomenon. The discrepancy between nonconfounding and collapsibility in rate comparisons arises when person-time is a post-exposure variable whose distribution can be altered by the effects of exposure and other risk factors.},
author = {Greenland, Sander},
doi = {10.1097/00001648-199609000-00008},
isbn = {10443983},
issn = {10443983},
journal = {Epidemiology},
keywords = {collapsibility,confounding,odds ratio,relative risk},
pmid = {8862980},
title = {{Absence of confounding does not correspond to collapsibility of the rate ratio or rate difference}},
year = {1996}
}
@misc{Robins2000,
author = {Robins, James M and Hernan, Miguel Angel and Brumback, Babette},
publisher = {LWW},
title = {{Marginal structural models and causal inference in epidemiology}},
year = {2000}
}
@article{Young2014,
abstract = {It is routinely argued that, unlike standard regression-based estimates, inverse probability weighted (IPW) estimates of the parameters of a correctly specified Cox marginal structural model (MSM) may remain unbiased in the presence of a time-varying confounder affected by prior treatment. Previously proposed methods for simulating from a known Cox MSM lack knowledge of the law of the observed outcome conditional on the measured past. Although unbiased IPW estimation does not require this knowledge, standard regression-based estimates rely on correct specification of this law. Thus, in typical high-dimensional settings, such simulation methods cannot isolate bias due to complex time-varying confounding as it may be conflated with bias due to misspecification of the outcome regression model. In this paper, we describe an approach to Cox MSM data generation that allows for a comparison of the bias of IPW estimates versus that of standard regression-based estimates in the complete absence of model misspecification. This approach involves simulating data from a standard parametrization of the likelihood and solving for the underlying Cox MSM. We prove that solutions exist and computations are tractable under many data-generating mechanisms. We show analytically and confirm in simulations that, in the absence of model misspecification, the bias of standard regression-based estimates for the parameters of a Cox MSM is indeed a function of the coefficients in observed data models quantifying the presence of a time-varying confounder affected by prior treatment. We discuss limitations of this approach including that implied by the 'g-null paradox'.},
author = {Young, Jessica G. and {Tchetgen Tchetgen}, Eric J.},
doi = {10.1002/sim.5994},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Causal inference,G-formula,G-null paradox,Marginal structural models,Simulation,Survival analysis},
pmid = {24151138},
title = {{Simulation from a known Cox MSM using standard parametric models for the g-formula}},
year = {2014}
}
@article{Cole2010,
abstract = {That conditioning on a common effect of exposure and outcome may cause selection, or collider-stratification, bias is not intuitive. We provide two hypothetical examples to convey concepts underlying bias due to conditioning on a collider. In the first example, fever is a common effect of influenza and consumption of a tainted egg-salad sandwich. In the second example, case-status is a common effect of a genotype and an environmental factor. In both examples, conditioning on the common effect imparts an association between two otherwise independent variables; we call this selection bias.},
author = {Cole, Stephen R. and Platt, Robert W. and Schisterman, Enrique F. and Chu, Haitao and Westreich, Daniel and Richardson, David and Poole, Charles},
doi = {10.1093/ije/dyp334},
isbn = {1464-3685 (Electronic)$\backslash$r0300-5771 (Linking)},
issn = {03005771},
journal = {International Journal of Epidemiology},
pmid = {19926667},
title = {{Illustrating bias due to conditioning on a collider}},
year = {2010}
}
@article{Messer2010,
author = {Messer, Lynne C and Oakes, J Michael and Mason, Susan},
journal = {American journal of epidemiology},
number = {6},
pages = {664--673},
publisher = {Oxford University Press},
title = {{Effects of socioeconomic and racial residential segregation on preterm birth: a cautionary tale of structural confounding}},
volume = {171},
year = {2010}
}
@article{Pearl2010,
abstract = {In 2 recent communications, Cole and Frangakis (Epidemiology. 2009;20:3-5) and VanderWeele (Epidemiology. 2009;20:880-883) conclude that the consistency rule used in causal inference is an assumption that precludes any side-effects of treatment/exposure on the outcomes of interest. They further develop auxiliary notation to make this assumption formal and explicit. I argue that the consistency rule is a theorem in the logic of counterfactuals and need not be altered. Instead, warnings of potential side-effects should be embodied in standard modeling practices that make causal assumptions explicit and transparent.},
author = {Pearl, Judea},
doi = {10.1097/EDE.0b013e3181f5d3fd},
isbn = {1531-5487; 1044-3983},
issn = {10443983},
journal = {Epidemiology},
pmid = {20864888},
title = {{On the consistency rule in causal inference: Axiom, definition, assumption, or theorem?}},
year = {2010}
}
@article{Robins1992,
abstract = {AIDS Clinical Trial Group Randomized Trial 002 compared the effect of high-dose with low-dose 3-azido-3-deoxythymidine (AZT) on the survival of AIDS patients. Embedded within the trial was an essentially uncontrolled observational study of the effect of prophylaxis therapy for pneumocystis carinii pneumonia on survival. In this paper, we estimate the causal effect of prophylaxis therapy on survival by using the method of G-estimation to estimate the parameters of a structural nested failure time model (SNFTM). Our SNFTM relates a subject's observed time of death and observed prophylaxis history to the time the subject would have died if, possibly contrary to fact, prophylaxis therapy had been withheld. We find that, under our assumptions, the data are consistent with prophylaxis therapy increasing survival by 16{\%} or decreasing survival by 18{\%} at the alpha = 0.05 level. The analytic approach proposed in this paper will be necessary to control bias in any epidemiologic study in which there exists a time-dependent risk factor for death, such as pneumocystis carinii pneumonia history, that (A1) influences subsequent exposure to the agent under study, for example, prophylaxis therapy, and (A2) is itself influenced by past exposure to the study agent. Conditions A1 and A2 will be true whenever there exists a time-dependent risk factor that is simultaneously a confounder and an intermediate variable.},
author = {Robins, James M. and Blevins, Donald and Ritter, Grant and Wulfsoh, Michael},
doi = {10.1097/00001648-199207000-00007},
isbn = {1044-3983},
issn = {15315487},
journal = {Epidemiology},
keywords = {Causality,Confounding,Counterfactuals,Epidemiologic methods,Intermediate variables,Longitudinal data,Semiparametric methods,Structural models,Survival analysis},
pmid = {1637895},
title = {{G-estimation of the effect of prophylaxis therapy for pneumocystis carinii pneumonia on the survival of AIDS patients}},
year = {1992}
}
@article{Sjoelander2016,
author = {Sj{\"{o}}lander, Arvid and Dahlqwist, Elisabeth and Zetterqvist, Johan},
journal = {Epidemiology},
number = {3},
pages = {356--359},
publisher = {LWW},
title = {{A note on the noncollapsibility of rate differences and rate ratios}},
volume = {27},
year = {2016}
}
@article{Westreich2010,
author = {Westreich, Daniel and Cole, Stephen R},
journal = {American journal of epidemiology},
number = {6},
pages = {674--677},
publisher = {Oxford University Press},
title = {{Invited commentary: positivity in practice}},
volume = {171},
year = {2010}
}
@article{Rubin1978,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Causal effects are comparisons among values that would have been observed under all possible assignments of treatments to experimental units. In an experiment, one assignment of treatments is chosen and only the values under that assignment can be observed. Bayesian inference for causal effects follows from finding the predictive distribution of the values under the other assignments of treatments. This perspective makes clear the role of mechanisms that sample experimental units, assign treatments and record data. Unless these mechanisms are ignorable (known probabi-listic functions of recorded values), the Bayesian must model them in the data analysis and, consequently, confront inferences for causal effects that are sensitive to the specification of the prior distribution of the data. More-over, not all ignorable mechanisms can yield data from which inferences for causal effects are insensitive to prior specifications. Classical random-ized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis.},
author = {Rubin, Donald B.},
doi = {10.1214/aos/1176344064},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
pmid = {531},
title = {{Bayesian Inference for Causal Effects: The Role of Randomization}},
year = {1978}
}
@article{Greenland1997,
abstract = {Computer simulation is an important and increasingly popular tool for studying epidemiologic methods. To interpret the results from a simulation study, however, one must understand the structure and limitations of such studies. As we discuss here, it is nearly impossible to capture all potentially relevant aspects of a real study with a simulation; as a consequence, simulation results have limited generalizability. Simulation results also may suffer from programming error, rounding error, algorithmic error, and random error. These problems reveal the danger of overgeneralizing from simulation results and the importance of viewing simulation studies as analogous to clinical trials (which demand corroboration and cautious interpretation), rather than as definitive mathematical studies.},
author = {Greenland, Sander and Maldonado, G},
isbn = {1044-3983 (Print)},
issn = {1044-3983},
journal = {Epidemiology (Cambridge, Mass.)},
keywords = {Algorithms,Computer Simulation,Computer Simulation: standards,Confidence Intervals,Epidemiologic Methods,Humans,Regression Analysis,Research Design,Research Design: standards,Stochastic Processes},
pmid = {9209864},
title = {{The importance of critically interpreting simulation studies.}},
year = {1997}
}
@misc{Pearl2014,
abstract = {I thank the editor, Ronald Christensen, for the opportunity to discuss this important topic and to comment on the article by Armistead. Simpson's paradox is often presented as a compelling demonstration of why we need statistics education in our schools. It is a ...},
author = {Pearl, Judea},
booktitle = {American Statistician},
doi = {10.1080/00031305.2014.876829},
isbn = {0003-1305},
issn = {15372731},
title = {{Comment: Understanding Simpson's paradox}},
year = {2014}
}
@article{Didelez2010,
abstract = {We consider situations where data have been collected such that the sampling depends on the outcome of interest and possibly further covariates, as for instance in case-control studies. Graphical models represent assumptions about the conditional independencies among the variables. By including a node for the sampling indicator, assumptions about sampling processes can be made explicit. We demonstrate how to read off such graphs whether consistent estimation of the association between exposure and outcome is possible. Moreover, we give sufficient graphical conditions for testing and estimating the causal effect of exposure on outcome. The practical use is illustrated with a number of examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1101.0901v1},
author = {Didelez, Vanessa and Kreiner, Svend and Keiding, Niels},
doi = {10.1214/10-STS340},
eprint = {arXiv:1101.0901v1},
issn = {0883-4237},
journal = {Statistical Science},
title = {{Graphical Models for Inference Under Outcome-Dependent Sampling}},
year = {2010}
}
@article{Howe2015,
abstract = {Estimating causal effects is a frequent goal of epi-demiologic studies. Traditionally, there have been three established systematic threats to consistent estimation of caus-al effects. These three threats are bias due to confounders, selection, and measurement error. Confounding, selection, and measurement bias have typically been characterized as distinct types of biases. However, each of these biases can also be characterized as missing data problems that can be ad-dressed with missing data solutions. Here we describe how the aforementioned systematic threats arise from missing data as well as review methods and their related assumptions for reducing each bias type. We also link the assumptions made by the reviewed methods to the missing completely at random (MCAR) and missing at random (MAR) assumptions made in the missing data framework that allow for valid inferences to be made based on the observed, incomplete data.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Howe, Chanelle J. and Cain, Lauren E. and Hogan, Joseph W.},
doi = {10.1007/s40471-015-0050-8},
eprint = {15334406},
isbn = {0000000000000},
issn = {2196-2995},
journal = {Current Epidemiology Reports},
pmid = {26928661},
title = {{Are All Biases Missing Data Problems?}},
year = {2015}
}
@article{Hernan2000,
abstract = {Standard methods for survival analysis, such as the time-dependent Cox model, may produce biased effect estimates when there exist time-dependent confounders that are themselves affected by previous treatment or exposure. Marginal structural models are a new class of causal models the parameters of which are estimated through inverse-probability-of-treatment weighting; these models allow for appropriate adjustment for confounding. We describe the marginal structural Cox proportional hazards model and use it to estimate the causal effect of zidovudine on the survival of human immunodeficiency virus-positive men participating in the Multicenter AIDS Cohort Study. In this study, CD4 lymphocyte count is both a time-dependent confounder of the causal effect of zidovudine on survival and is affected by past zidovudine treatment. The crude mortality rate ratio (95{\%} confidence interval) for zidovudine was 3.6 (3.0-4.3), which reflects the presence of confounding. After controlling for baseline CD4 count and other baseline covariates using standard methods, the mortality rate ratio decreased to 2.3 (1.9-2.8). Using a marginal structural Cox model to control further for time-dependent confounding due to CD4 count and other time-dependent covariates, the mortality rate ratio was 0.7 (95{\%} conservative confidence interval = 0.6-1.0). We compare marginal structural models with previously proposed causal methods.},
author = {Hernan, M A and Brumback, B and Robins, J M},
doi = {10.1097/00001648-200009000-00012},
isbn = {1044-3983 (Print)$\backslash$r1044-3983 (Linking)},
issn = {1044-3983},
journal = {Epidemiology},
keywords = {*Models, Statistical,Anti-HIV Agents/*therapeutic use,CD4 Lymphocyte Count,Causality,Confounding Factors (Epidemiology),Epidemiologic Methods,HIV Infections/*drug therapy/*mortality,Humans,Male,Proportional Hazards Models,Survival Analysis,Time Factors,Treatment Outcome,United States/epidemiology,Zidovudine/*therapeutic use},
pmid = {10955409},
title = {{Marginal structural models to estimate the causal effect of zidovudine on the survival of HIV-positive men}},
year = {2000}
}
@article{Havercroft2012,
author = {Havercroft, W G and Didelez, V},
journal = {Statistics in medicine},
number = {30},
pages = {4190--4206},
publisher = {Wiley Online Library},
title = {{Simulating from marginal structural models with time-dependent confounding}},
volume = {31},
year = {2012}
}
@article{Bryan2004,
abstract = {In this article we construct and study estimators of the causal effect of a time-dependent treatment on survival in longitudinal studies. We employ a particular marginal structural model (MSM), proposed by Robins (2000), and follow a general methodology for constructing estimating functions in censored data models. The inverse probability of treatment weighted (IPTW) estimator of Robins et al. (2000) is used as an initial estimator and forms the basis for an improved, one-step estimator that is consistent and asymptotically linear when the treatment mechanism is consistently estimated. We extend these methods to handle informative censoring. The proposed methodology is employed to estimate the causal effect of exercise on mortality in a longitudinal study of seniors in Sonoma County. A simulation study demonstrates the bias of naive estimators in the presence of time-dependent confounders and also shows the efficiency gain of the IPTW estimator, even in the absence such confounding. The efficiency gain of the improved, one-step estimator is demonstrated through simulation.},
author = {Bryan, Jenny and Yu, Zhuo and {Van Der Laan}, Mark J.},
doi = {10.1093/biostatistics/kxg041},
isbn = {1465-4644},
issn = {14654644},
journal = {Biostatistics},
keywords = {Causal inference,Counterfactual,Estimating function,IPTW estimator,Marginal structural model,One-step estimator,Propensity score,Sequential randomization},
pmid = {15208200},
title = {{Analysis of longitudinal marginal structural models}},
year = {2004}
}
@article{Cole2008,
author = {Cole, Stephen R and Hern{\'{a}}n, Miguel A},
journal = {American journal of epidemiology},
number = {6},
pages = {656--664},
publisher = {Oxford University Press},
title = {{Constructing inverse probability weights for marginal structural models}},
volume = {168},
year = {2008}
}
@article{Greenland1999,
author = {Greenland, Sander and Robins, James M and Pearl, Judea},
journal = {Statistical science},
pages = {29--46},
publisher = {JSTOR},
title = {{Confounding and collapsibility in causal inference}},
year = {1999}
}
@article{Greenland2011,
abstract = {We review probabilistic and graphical rules for detecting situations in which a dependence of one variable on another is altered by adjusting for a third variable (i.e., non-collapsibility or non- invariance under adjustment), whether that dependence is causal or purely predictive.We focus on distinguishing situations in which adjustment will reduce, increase, or leave unchanged the degree of bias in an association that is taken to represent a causal effect of one variable on the other. We then consider situations in which adjustment may partially remove or introduce a bias in estimating causal effects, and some additional special cases useful for case-control studies, cohort studies with loss, and trials with non-compliance (non-adherence).},
author = {Greenland, Sander and Pearl, Judea},
doi = {10.1111/j.1751-5823.2011.00158.x},
isbn = {0306-7734},
issn = {03067734},
journal = {International Statistical Review},
keywords = {Bias,Causal models,Causality,Collapsibility,Compliance,Confounding,Graphical models,Instrumental variables,Mediation analysis,Odds ratio},
title = {{Adjustments and their Consequences-Collapsibility Analysis using Graphical Models}},
year = {2011}
}
@article{Daniel2013,
abstract = {Longitudinal studies, where data are repeatedly collected on subjects over a period, are common in medical research. When estimating the effect of a time-varying treatment or exposure on an outcome of interest measured at a later time, standard methods fail to give consistent estimators in the presence of time-varying confounders if those confounders are themselves affected by the treatment. Robins and colleagues have proposed several alternative methods that, provided certain assumptions hold, avoid the problems associated with standard approaches. They include the g-computation formula, inverse probability weighted estimation of marginal structural models and g-estimation of structural nested models. In this tutorial, we give a description of each of these methods, exploring the links and differences between them and the reasons for choosing one over the others in different settings.},
author = {Daniel, R. M. and Cousens, S. N. and {De Stavola}, B. L. and Kenward, M. G. and Sterne, J. A.C.},
doi = {10.1002/sim.5686},
isbn = {1097-0258 (Electronic)$\backslash$r0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {G-computation formula,G-estimation,Inverse probability weighting,Marginal structural model,Structural nested model,Time-dependent confounding},
pmid = {23208861},
title = {{Methods for dealing with time-dependent confounding}},
year = {2013}
}
@article{Hernan2004,
author = {Hern{\'{a}}n, Miguel A and Hern{\'{a}}ndez-D$\backslash$'$\backslash$iaz, Sonia and Robins, James M},
journal = {Epidemiology},
number = {5},
pages = {615--625},
publisher = {LWW},
title = {{A structural approach to selection bias}},
volume = {15},
year = {2004}
}
@article{Pang2016,
abstract = {One approach to quantifying the magnitude of confounding in observational studies is to compare estimates with and without adjustment for a covariate, but this strategy is known to be defective for noncollapsible measures such as the odds ratio. Comparing estimates from marginal structural and standard logistic regression models, the total difference between crude and conditional effects can be decomposed into the sum of a noncollapsibility effect and confounding bias. We provide an analytic approach to assess the noncollapsibility effect in a point-exposure study and provide a general formula for expressing the noncollapsibility effect. Next, we provide a graphical approach that illustrates the relationship between the noncollapsibility effect and the baseline risk, and reveals the behavior of the noncollapsibility effect for a range of different exposure and covariate effects. Various observations about noncollapsibility can be made from the different scenarios with or without confounding; for example, the magnitude of effect of the covariate plays a more important role in the noncollapsibility effect than does that of the effect of the exposure. In order to explore the noncollapsibility effect of the odds ratio in the presence of time-varying confounding, we simulated an observational cohort study. The magnitude of noncollapsibility was generally comparable to the effect in the point-exposure study in our simulation settings. Finally, in an applied example we demonstrate that collapsibility can have an important impact on estimation in practice.},
author = {Pang, Menglan and Kaufman, Jay S. and Platt, Robert W.},
doi = {10.1177/0962280213505804},
isbn = {1477-0334 (Electronic)0962-2802 (Linking)},
issn = {14770334},
journal = {Statistical Methods in Medical Research},
keywords = {confounding bias,logistic regression model,marginal structural model,noncollapsibility,odds ratio},
pmid = {24108272},
title = {{Studying noncollapsibility of the odds ratio with marginal structural and logistic regression models}},
year = {2016}
}
@article{Mortimer2005,
author = {Mortimer, Kathleen M and Neugebauer, Romain and {Van Der Laan}, Mark and Tager, Ira B},
journal = {American Journal of Epidemiology},
number = {4},
pages = {382--388},
publisher = {Oxford University Press},
title = {{An application of model-fitting procedures for marginal structural models}},
volume = {162},
year = {2005}
}
@article{Neyman1923,
abstract = {This paper addresses the problem of aggregating a number of expert opinions which have been expressed in some numerical form in order to reflect individual uncertainty vis-a-vis a quantity of interest. The primary focus is consensus belief formation and expert use, although some relevant aspects of group decision making are also reviewed. A taxonomy of solutions is presented which serves as the framework for a survey of recent theoretical developments in the area. A number of current research directions are mentioned and an extensive, current annotated bibliography is included.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Neyman, J},
doi = {10.1214/ss/1177012031},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/morle001/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neyman - 1923 - On the application of probability theory to agricultural experiments principles (in Polish with German summary).pdf:pdf},
isbn = {0412343908},
issn = {0883-4237},
journal = {Roczniki Nauk Rolniczch},
number = {1},
pages = {21--51},
pmid = {20948974},
title = {{On the application of probability theory to agricultural experiments: principles (in Polish with German summary).}},
url = {http://projecteuclid.org/euclid.ss/1177010123},
volume = {10},
year = {1923}
}
@misc{Cole2009,
author = {Cole, Stephen R. and Frangakis, Constantine E.},
booktitle = {Epidemiology},
doi = {10.1097/EDE.0b013e31818ef366},
isbn = {1044-3983},
issn = {10443983},
pmid = {19234395},
title = {{The consistency statement in causal inference: A definition or an assumption?}},
year = {2009}
}
@book{Pearl2009,
author = {Pearl, Judea},
publisher = {Cambridge university press},
title = {{Causality}},
year = {2009}
}
@misc{Hernan2010,
abstract = {Editors' note: This series addresses topics that affect epidemiologists across a range of specialties. Commentaries start as invited talks at symposia organized by the Editors. This paper was presented at the 2009 Society for Epidemiologic Research Annual Meeting in Anaheim, CA.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hern{\'{a}}n, Miguel A.},
booktitle = {Epidemiology},
doi = {10.1097/EDE.0b013e3181c1ea43},
eprint = {NIHMS150003},
isbn = {1531-5487; 1531-5487},
issn = {10443983},
pmid = {20010207},
title = {{The hazards of hazard ratios}},
year = {2010}
}
@article{Robins1986,
abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect. {\textcopyright} 1986.},
author = {Robins, James},
doi = {10.1016/0270-0255(86)90088-6},
isbn = {0270-0255},
issn = {02700255},
journal = {Mathematical Modelling},
pmid = {3667861},
title = {{A new approach to causal inference in mortality studies with a sustained exposure period-application to control of the healthy worker survivor effect}},
year = {1986}
}
@article{HernanMA2018,
author = {Imbens, Guido W and Rubin, Donald B},
journal = {Boca Raton: Chapman {\&} Hall/CRC, forthcoming.},
keywords = {2},
number = {June},
pages = {39--58},
title = {{Causal Inference}},
year = {2008}
}
@article{VanderWeele2009,
abstract = {Cole and Frangakis (Epidemiology. 2009;20:3-5) introduced notation for the consistency assumption in causal inference. I extend this notation and propose a refinement of the consistency assumption that makes clear that the consistency statement, as ordinarily given, is in fact an assumption and not an axiom or definition. The refinement is also useful in showing that additional assumptions (referred to here as treatment-variation irrelevance assumptions), stronger than those given by Cole and Frangakis, are in fact necessary in articulating the ordinary assumptions of ignorability or exchangeability. The refinement furthermore sheds light on the distinction between intervention and choice in reasoning about causality. A distinction between the range of treatment variations for which potential outcomes can be defined and the range for which treatment comparisons are made is discussed in relation to issues of nonadherence. The use of stochastic counterfactuals can help relax what is effectively being presupposed by the treatment-variation irrelevance assumption and the consistency assumption.},
author = {{Vander Weele}, Tyler J.},
doi = {10.1097/EDE.0b013e3181bd5638},
isbn = {1044-3983},
issn = {10443983},
journal = {Epidemiology},
pmid = {19829187},
title = {{Concerning the consistency assumption in causal inference}},
year = {2009}
}
@article{Edwards2015,
author = {Edwards, Jessie K and Cole, Stephen R and Westreich, Daniel},
journal = {International journal of epidemiology},
number = {4},
pages = {1452--1459},
publisher = {Oxford University Press},
title = {{All your data are always missing: incorporating bias due to measurement error into the potential outcomes framework}},
volume = {44},
year = {2015}
}
@article{Pearl2001,
abstract = {This paper provides a conceptual introduction to causal inference, aimed to assist health services researchers benefit from recent advances in this area. The paper stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underlie all causal inferences, the languages used in formulating those assumptions, and the conditional nature of causal claims inferred from nonexperimental studies. These emphases are illustrated through a brief survey of recent results, including the control of confaunding, corrections for noncompliance, and a symbiosis between counterfactual and graphical methods of analysis.},
author = {Pearl, Judea},
doi = {10.1023/A:1020315127304},
isbn = {1387-3741},
issn = {13873741},
journal = {Health Services and Outcomes Research Methodology},
keywords = {Confounding,Counterfactuals,Graphical methods,Noncompliance,Structural equation models},
pmid = {228929339},
title = {{Causal inference in the health sciences: A conceptual introduction}},
year = {2001}
}
@article{Cheng2010,
author = {Cheng, Yvonne W and Hubbard, Alan and Caughey, Aaron B and Tager, Ira B},
journal = {American journal of epidemiology},
number = {6},
pages = {656--663},
publisher = {Oxford University Press},
title = {{The association between persistent fetal occiput posterior position and perinatal outcomes: an example of propensity score and covariate distance matching}},
volume = {171},
year = {2010}
}
@article{Ding2017,
abstract = {Inferring causal effects of treatments is a central goal in many disciplines. The potential outcomes framework is a main statistical approach to causal inference, in which a causal effect is defined as a comparison of the potential outcomes of the same units under different treatment conditions. Because for each unit at most one of the potential outcomes is observed and the rest are missing, causal inference is inherently a missing data problem. Indeed, there is a close analogy in the terminology and the inferential framework between causal inference and missing data. Despite the intrinsic connection between the two subjects, statistical analyses of causal inference and missing data also have marked differences in aims, settings and methods. This article provides a systematic review of causal inference from the missing data perspective. Focusing on ignorable treatment assignment mechanisms, we discuss a wide range of causal inference methods that have analogues in missing data analysis, such as imputation, inverse probability weighting and doubly-robust methods. Under each of the three modes of inference--Frequentist, Bayesian, and Fisherian randomization--we present the general structure of inference for both finite-sample and super-population estimands, and illustrate via specific examples. We identify open questions to motivate more research to bridge the two fields.},
archivePrefix = {arXiv},
arxivId = {1712.06170},
author = {Ding, Peng and Li, Fan},
doi = {10.1214/18-STS645},
eprint = {1712.06170},
file = {:C$\backslash$:/Users/morle001/OneDrive/Documents/leiden/thesis/literature/ding{\_}2017{\_}causal{\_}inference{\_}a{\_}missing{\_}data{\_}perspective.pdf:pdf},
issn = {0883-4237},
pages = {1--47},
title = {{Causal Inference: A Missing Data Perspective}},
url = {http://arxiv.org/abs/1712.06170},
year = {2017}
}
@article{Horvitz1952,
abstract = {This paper presents a general technique for the treatment of samples drawn without replacement from finite universes when unequal selection probabilities are used. Two sampling schemes are discussed in connection with the problem of determining optimum selection probabilities according to the information available in a supplementary variable. Admittedly, these two schemes have limited application. They should prove useful, however, for the first stage of sampling with multi-stage designs, since both permit unbiased estimation of the sampling variance without resorting to additional assumptions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Horvitz, D. G. and Thompson, D. J.},
doi = {10.1080/01621459.1952.10483446},
eprint = {arXiv:1011.1669v3},
isbn = {01621459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
pmid = {13050323},
title = {{A Generalization of Sampling Without Replacement from a Finite Universe}},
year = {1952}
}
@article{Naimi2011,
abstract = {In occupational epidemiologic studies, the healthy worker survivor effect refers to a process that leads to bias in the estimates of an association between cumulative exposure and a health outcome. In these settings, work status acts both as an intermediate and confounding variable and may violate the positivity assumption (the presence of exposed and unexposed observations in all strata of the confounder). Using Monte Carlo simulation, we assessed the degree to which crude, work-status adjusted, and weighted (marginal structural) Cox proportional hazards models are biased in the presence of time-varying confounding and nonpositivity. We simulated the data representing time-varying occupational exposure, work status, and mortality. Bias, coverage, and root mean squared error (MSE) were calculated relative to the true marginal exposure effect in a range of scenarios. For a base-case scenario, using crude, adjusted, and weighted Cox models, respectively, the hazard ratio was biased downward 19{\%}, 9{\%}, and 6{\%}; 95{\%} confidence interval coverage was 48{\%}, 85{\%}, and 91{\%}; and root MSE was 0.20, 0.13, and 0.11. Although marginal structural models were less biased in most scenarios studied, neither standard nor marginal structural Cox proportional hazards models fully resolve the bias encountered under conditions of time-varying confounding and nonpositivity.},
author = {Naimi, Ashley I. and Cole, Stephen R. and Westreich, Daniel J. and Richardson, David B.},
doi = {10.1097/EDE.0b013e31822549e8},
isbn = {1531-5487 (Electronic)$\backslash$r1044-3983 (Linking)},
issn = {10443983},
journal = {Epidemiology},
pmid = {21747286},
title = {{A comparison of methods to estimate the hazard ratio under conditions of time-varying confounding and nonpositivity}},
year = {2011}
}
@article{Westreich2012,
abstract = {Motivated by a previously published study of HIV treatment, we simulated data subject to time-varying confounding affected by prior treatment to examine some finite-sample properties of marginal structural Cox proportional hazards models. We compared (a) unadjusted, (b) regression-adjusted, (c) unstabilized, and (d) stabilized marginal structural (inverse probability-of-treatment [IPT] weighted) model estimators of effect in terms of bias, standard error, root mean squared error (MSE), and 95{\%} confidence limit coverage over a range of research scenarios, including relatively small sample sizes and 10 study assessments. In the base-case scenario resembling the motivating example, where the true hazard ratio was 0.5, both IPT-weighted analyses were unbiased, whereas crude and adjusted analyses showed substantial bias towards and across the null. Stabilized IPT-weighted analyses remained unbiased across a range of scenarios, including relatively small sample size; however, the standard error was generally smaller in crude and adjusted models. In many cases, unstabilized weighted analysis showed a substantial increase in standard error compared with other approaches. Root MSE was smallest in the IPT-weighted analyses for the base-case scenario. In situations where time-varying confounding affected by prior treatment was absent, IPT-weighted analyses were less precise and therefore had greater root MSE compared with adjusted analyses. The 95{\%} confidence limit coverage was close to nominal for all stabilized IPT-weighted but poor in crude, adjusted, and unstabilized IPT-weighted analysis. Under realistic scenarios, marginal structural Cox proportional hazards models performed according to expectations based on large-sample theory and provided accurate estimates of the hazard ratio.},
author = {Westreich, Daniel and Cole, Stephen R. and Schisterman, Enrique F. and Platt, Robert W.},
doi = {10.1002/sim.5317},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Bias,Causal inference,Marginal structural models,Monte Carlo study},
pmid = {22492660},
title = {{A simulation study of finite-sample properties of marginal structural Cox proportional hazards models}},
year = {2012}
}
@misc{Hernan2009,
abstract = {Causal inferences about the effect of an exposure on an outcome may be biased by errors in the measurement of either the exposure or the outcome. Measurement errors of exposure and outcome can be classified into 4 types: independent nondifferential, dependent nondifferential, independent differential, and dependent differential. Here the authors describe how causal diagrams can be used to represent these 4 types of measurement bias and discuss some problems that arise when using measured exposure variables (e.g., body mass index) to make inferences about the causal effects of unmeasured constructs (e.g., "adiposity"). The authors conclude that causal diagrams need to be used to represent biases arising not only from confounding and selection but also from measurement.},
author = {Hern{\'{a}}n, Miguel A. and Cole, Stephen R.},
booktitle = {American Journal of Epidemiology},
doi = {10.1093/aje/kwp293},
isbn = {1476-6256 (Electronic)$\backslash$n0002-9262 (Linking)},
issn = {00029262},
keywords = {Bias (epidemiology),Body mass index,Causality,Confounding factors (epidemiology)},
pmid = {19755635},
title = {{Invited commentary: Causal diagrams and measurement bias}},
year = {2009}
}
@incollection{Robins2000a,
author = {Robins, James M},
booktitle = {Statistical models in epidemiology, the environment, and clinical trials},
pages = {95--133},
publisher = {Springer},
title = {{Marginal structural models versus structural nested models as tools for causal inference}},
year = {2000}
}
@book{JohnP.Klein2003,
abstract = {Applied statisticians in many fields must frequently analyze time to event data. While the statistical tools presented in this book are applicable to data from medicine, biology, public health, epidemiology, engineering, economics, and demography, the focus here is on applications of the techniques to biology and medicine. The analysis of survival experiments is complicated by issues of censoring, where an individual{\&}apos;s life length is known to occur only in a certain period of time, and by truncation, where individuals enter the study only if they survive a sufficient length of time or individuals are included in the study only if the event has occurred by a given date. The use of counting process methodology has allowed for substantial advances in the statistical theory to account for censoring and truncation in survival experiments. This book makes these complex methods more accessible to applied researchers without an advanced mathematical background. The authors present the essence of these techniques, as well as classical techniques not based on counting processes, and apply them to data. Practical suggestions for implementing the various methods are set off in a series of Practical Notes at the end of each section. Technical details of the derivation of the techniques are sketched in a series of Technical Notes. This book will be useful for investigators who need to analyze censored or truncated life time data, and as a textbook for a graduate course in survival analysis. The prerequisite is a standard course in statistical methodology.},
author = {{John P. Klein}, Melvin L. Moeschberger},
booktitle = {Springer},
doi = {10.1145/390011.808243},
isbn = {038795399X},
issn = {03621340},
title = {{Survival Analysis - Techniques for Ceosnred and Truncated Data - 2nd Edition}},
year = {2003}
}
