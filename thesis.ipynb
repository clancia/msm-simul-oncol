{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aknowledgements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\\tableofcontents{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Introduction\n",
    "\n",
    "- set up problem and why we need to study positivty - give this motivation before anything else\n",
    "- exhchangeability/confounder control vs positivity\n",
    "- thresholding/positivty compliant doctors.\n",
    "- in a survival context, need to choose simulation algorithm carefull because survival models are noncollapsible.\n",
    "\n",
    "\n",
    "Marginal structural models (MSMs) are a popular class of models for performing causal inference in the presence of time dependent confounders. These models have an important application in areas of research such as epidemiology, social sciences and economics where randomised trials are prohibited by ethical or financial considerations, and hence confounding cannot be ruled out by randomization. Under these circumstances confounding can obscure the causal effect of treatment on outcome. An example of this, common in epidemiological studies, occurs when prognostic variables inform treatment decisions while at same time being predictors of the outcome of interest. In a longitudinal setting this is further complicated when the confounder itself is determined by earlier treatment. One consequence is that regression adjustment methods do not control for confounding in the longitudinal case and other techniques are required. \\linebreak\n",
    "\n",
    "The Inverse probability of treatment weighting (IPTW) estimator is a technique which leads to consistent estimates in the presence of  has been applied to censoring, missing data and survey design problems. The central idea is that by weighting the observed data in order to create a pseudo population is constructed in which treatemnt is assigned at random. Subsequent analysis where we ignore the confounder is then possible. which inference on the target population can be achieved. For example, when there is missing data weights can be used to create a pseudo-population in which there is no missingness. In the context of MSMs, the IPT weights relate to a pseudo-population in which there is no longer any confounding between the confounder and treatment and causal inferences can be made. \\linebreak\n",
    "\n",
    "Underlying the IPTW method for estimating MSMs are four assumptions: 1) consistency 2) exchangeability 3) positivity 4) and correct model specification. Exchangeability, also known as the no unmeasured confounding assumption,  is closely linked to causality?? Several studies have considered violations of exchangeability and corrected model specification. Positivity has received less attention because in typical observational study positivity violations are not suspected explain why. In the clinical context that we consider, protocols (give some examples, like Platt 2012) threaten to violate the positivity assumption and we investigate whether MSMs are robust against positivity. The focus of this thesis will be on violations of the positivity assumption. Positivity means that within every strata spanned by the confounders, there must be a positive probability of patients being exposed or unexposed to treatment. For example, in a medical context, if treatment protocols demand that treatment is initiated whenever a prognostic variable falls below a pre-defined threshold, there will only be exposed and no unexposed patients in this strata of the confounding prognostic variable.  make decisions based on protocols positivity can be. In the absence of structural positivity violations, there is always the threat that random zeroes arise in some strata of the confounder especially when the sample size is small or the number of confounding variables is large. In each case the sparsity of data within the strata of the confounder results in a high chance that positivity is violated. Positivity violations increase the bias and variance of estimates of the causal effect but the extent of the damage is not well known. The central aim of this thesis will be to investigate positivity violations when fitting MSMs to longitudinal data. To our knowledge positivity violations have not been systematically studied in the literature from a simulation point of view. We quantify the bias and variance introduced due to positivity violations and hope to provide practical advice to researchers tempted to fit MSMs to overcome confounding without realising the potential consequences of positivity violations in their data. \\linebreak\n",
    "\n",
    "Throughout this thesis we focus on clinical applications as examples. In the literature on marginal structural models the causal effect of Zidovudine on the survival of HIV positive men is often cited as an example. In this example a patients white blood cell (CD4) count is a prognostic variable that influences a doctor's decision to initiate treatment while at the same time being a predictor of survival. As a result CD4 count is a confounder. In the longitudinal setting previous treatments influence CD4 count. As such studies often depend on protocols which means that poistivity in some levels of the confounder make this a suitable example for our purposes. \\linebreak \n",
    "\n",
    "The structure of this thesis is as follows. In section 2 of part 1, the model considered in this thesis and its important aspects are explained. In part 2 simulating from this statistical model is discussed in detail. In part 3 the model under dynamic strategies is considered and comparisons are drawn with the static case. In part 4 we entertain violations of positivity in the data, this section represents the novelty in this thesis. Part 5 conducts a simulation study. Part 6 includes a discussion, conclusions and suggestions for future work. \\linebreak\n",
    "\n",
    "A second consequence is that simulating data from a specific marginal structural models is more challenging when the data is to exhibit time dependent confounding.\n",
    "\n",
    "Look through literature for applications of MSMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow for the joint determination of outcomes and treatment status or omitted variables related to both treatment status and outcomes (Angrist 2001).\n",
    "\n",
    "A covariate $L$ is a confounder if it predicts the event of interest and also predicts subsequent exposure. Explain how this actually happens, as U0 is a common ancestor of A through L and also Y, also that there is selection bias, and L is sufficient to adjust for confounding see Havercroft algorithm code page bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "In this thesis we assess the impact of violations of the positivity assumption on the performance of marginal structural models. \\linebreak\n",
    "\n",
    "The first chapter introduces marginal structural models and inverse probability of treatment weighting. Particular attention is given to the role of the positivity assumption in MSMs and the trade-off between finer confounding control and positivity. Chapter 2 explains the issues surrounding simulating data from a specific MSM with a longitudinal structure that captures the issues which arise in time dependent confounding. This chapter includes a literature review of algorithms that have been developed to simulate from a given marginal structural model. A particular simulation algorithm which is versatile enough that it can be used to introduce violations of positivity is then selected and explained in more detail. Chapter 3 presents simulation results and key findings. Chapter 4 uses real world data in which positivity violations arise as a result of treatment protocols in a chemotherapy trial. The last chapter concludes and provides limitations and directions for future work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "All simulations and analysis carried out in this thesis use the Python programming language and are provided with this thesis. Several modules were used to extend the base Python language and these are highlighted in the code where appropriate. The *survey* and *ipw* packages written in the R programming language were used to provide functionality not currently available as a Python module. These packages are freely avilable through the Comprehensive R Archive Network (CRAN). Combining R functionality in Python code is made possible through the *rpy2* Python module. \\linebreak\n",
    "\n",
    "All functions used for this thesis are provided in appendices. Appendix ? contains the code for generating data from the chosen marginal structural model and performing monte carlo simulations. Appendix ? contains the code used to generate the results and graphs in this thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginal structural models\n",
    "\n",
    "Marginal structural models (MSMs) are a class of models for the estimation of causal effects from observational data \\citet{Robins2000}. More specifically, marginal structural models allow us to make causal statements in the presence of time dependent confounding. Several key assumptions need to be met to achieve this. In this section we describe MSMs and focus on those assumtpions. This section explains the central concepts behind marginal structural models and introduces the notation that will be used in this thesis. Marginal structural models use only observed data and a set of assumptions to investigate causal effects. Positivity, the focus of this paper, is tied closely to confounder and, as we shall see, there is a trade-off between finer confounder control and positivity. The algorithm adapted in this paper simulates longitudinal data and hence we discuss time dependent confounding and the extra bits required for this. In particular, we draw attention to the core parts of MSMs needed for this study - linking to positivity.\n",
    "\n",
    "- describe other methods like g formula as alternative to MSMs\n",
    "- MSMs are models for some aspect (like the mean) of the distribution of counterfactuals.\n",
    "- many types like a marginal structural cox model (maybe let this follow on after weights part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactuals and causality\n",
    "\n",
    "In the counterfactual framework (\\citet{Neyman1923}, \\citet{Rubin1978}, \\citet{Robins1986}) the causal effect of treatment $X$ on outcome $Y$ for one subject can be defined as the difference between that subject's outcome had they been exposed and unexposed to X. In other words, one outcome is necessarily counterfactual because in reality the same subject cannot be both exposed and unexposed to $X$. If we denote the outcome when exposed as $Y_{x=1}$ and the outcome when not exposed as $Y_{x=0}$ then the causal effect for one subject can be expressed as $Y_{x=1} - Y_{x=0}$. For example, suppose a subject with a headache takes ibuprofen ($X = 1$), a popular treatment for headaches. After a suitable amount of time, say one hour, the headache either remains $Y_{x=1} = 1$ or has passed $Y_{x=1} = 0$. The outcome which is not observed is the counterfactual outcome that prevails had the subject, contrary to fact, not taken ibuprofen. In other words, we don't observe $Y_{x=0}$. \\linebreak\n",
    "\n",
    "Often we are interested in the average causal effect for a population rather than for one subject. Suppose sixty subjects are suffering from a headache and every subject was given ibuprofen. After one hour each subject will either have a headache ($Y_{x=1}=1$) or their headache will have passed ($Y_{x=1}=0$). The average outcome across all subjects is $\\mathbb{E}(Y_{x = 1})$ or equivalently when $Y$ is a dichotomous variable, $\\mathbb{P}(Y_{x = 1})$. The relevant causal comparison is now between $\\mathbb{P}(Y_{x = 1})$ and $\\mathbb{P}(Y_{x = 0})$, the latter being the counterfactual had none of sixty subjects been exposed. We do not observe the quantity $Y_{x=0}$ for any subject, and consequently we do not observe the quantity $\\mathbb{P}(Y_{x = 0})$. \\linebreak\n",
    "\n",
    "Dawid 1979 - we can vary the individual I and treatment X but this is largely a conceptual entity because only one treatment can in fact be applied to any unit.\n",
    "- Can also use the odds ratio as the comparison instead of the risk difference.\n",
    "- only have the data and observed outcomes, not the counterfactuals.\n",
    "- the collection of outcomes on a subject are called the potential outcomes and only one of these is observed\n",
    "\n",
    "- recently several studies have leveraged the missing data idea\n",
    "    - \\citet{Ding2017}\n",
    "    - \\citet{Howe2015}\n",
    "    - \\citet{Edwards2015}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounding\n",
    "\n",
    "Continuing the example, if group A consists of thirty of the headache sufferers who all took ibuprofen, we would ideally compare the quantity $\\mathbb{P}(Y_{x = 1}|X = 1) = \\mu_{A_{x = 1}}$ with the quantity $\\mathbb{P}(Y_{x = 0}|X = 1) = \\mu_{A_{x = 0}}$. As $\\mu_{A_{x = 0}}$ is not actually observed, we could instead compute the observable quantity $\\mathbb{P}(Y_{x = 0}|X = 0) = \\mu_{B_{x=0}}$ from the remaining thirty subjects who did not use ibuprofen and belong to group B. Replacing the comparison between $\\mu_{A_{x = 1}}$ and $\\mu_{A_{x = 0}}$ with the comparison between $\\mu_{A_{x = 1}}$ and $\\mu_{B_{x=0}}$ will have a causal interpretation if $\\mu_{A_{x = 0}} = \\mu_{B_{x=0}}$. In other words, if a subject from group B can be viewed as an analogue of a subject from group A had they, contrary to fact, not received ibuprofen. \\linebreak\n",
    "\n",
    "If $\\mu_{A_{x = 1}} \\neq \\mu_{B_{x=0}}$ then the comparison $\\mu_{A_{x = 1}} - \\mu_{B_{x=0}}$, a measure of association, is confounded for $\\mu_{A_{x = 1}} - \\mu_{A_{x = 0}}$, a measure of causal effect (\\citet{Greenland1999}). For example, if all the subjects in group A are male it would be reasonable to ask whether their sex influenced their decision to take ibuprofen. Suppose that males also tend to have headaches of a shorter duration so that at the end of one hour they are less likely to have a headache than females. The result is that both the decision to take ibuprofen and the probability of having a headache at the end of one hour are dependent on the sex of the subject. This obscures the causal effect of ibuprofen on headaches because there is a spurious association between $X$ and $Y$ through the subject's sex. We cannot establish whether the outcome is due to a causal relationship between ibuprofen and headache alleviation, a relationship between sex and headache alleviation or a mixture of the two.  \\linebreak\n",
    "\n",
    "One explanation for confounding is .... missing covariates (refer to greenland paper where confounding variables are one reason why there is confounding.)\n",
    "\n",
    "Closely related to confounding, exchangeability is the assumption that the distribution of the counterfactual outcomes $Y_{x}$ is independent of the actually observed treatment $X$. When exchangeability holds, subjects from group A and group B are exchangeable in the sense that were they all to remain untreated the distribution of the counterfactual outcomes $Y_{x}$ would be the same in the two groups \\citet{Daniel2013}. Imagine exchanging a subject from group A with a subject from group B where both recieve the treatment prevailing in their new group. Under exchangeability, the average outcome in the two groups is unchanged \\citet{HernanMA2018}. However, exchanging subjects between group A and group B introduces females into group A and males into group B. As males have a higher probability that $Y = 0$, exchanging subjects changes the distribution of the counterfactuals. The relationship between confounding and exchangeability is why the assumption of exchangeability is also called the assumption of \"no unmeasured confounding\". \\linebreak\n",
    "\n",
    "- set-up why comparisons are possible within strata and then averages across strata and why this means that naive methods for addresing time fixed confounding work\n",
    "- Hint that the finer the confounding control the more accurate the analysis but this has consequences for positivty.\n",
    "- from pearl 2001: namely, that if we compare treated vs. untreated subjects having the same\n",
    "- values of the selected factors, we get the correct treatment effect in that subpopulation of\n",
    "subjects.\n",
    "- Explain clearly why it is a bias.\n",
    "- Also explain why we want to be judicious in our choice of number confounders to control for. Can't include everything (Dawid 1979 on this)\n",
    "- Explain that structural parameters only coincide with associational parameters under exchangeability.\n",
    "- Define what the naive analysis is - analysis without adjustment.\n",
    "- knowing the value of Z gives us no more information about the distribution of the counterfactuals $Y_x$\n",
    "- Explain that in a randomized experiment exchangeability is guaranteed because $X$ is automatically noy related to any other variables.\n",
    "- Randomization ensures that missing values occur by chance. So the counterfactual values that we don't see for some observations are missing randomly and not due to confounding through a covariate.\n",
    "- Any residual confounding cannot be due to the variables that we have conditioned on.\n",
    "- hernan 2011 \"we say that positivity does not hold because for some confounder values there are no treated and untreated subjects to be compared\"\n",
    "- link to splines as a way of reducing residual confounding see cole 2008\n",
    "- When confounding is present we cannot simply substitute or exchange the exposed cohorts experience for the unexposed cohort.\n",
    "- confounders are simply covariates which explain why confounding is present (see Greenland 1996)\n",
    "- By conditioning on a variable (or a set of variables) C we will mean examining relations within levels of C (i.e. within strata defined by single values of C) (see Greenland 2011)\n",
    "- conditioning and adjustment not the same thing. Control is used as a synonym for asjutment.\n",
    "- in randomization, confounding is absent in expectation\n",
    "- explain how we standardize measures across strata but weighted combination and link to standardization itself\n",
    "- use conditional exchangeability to show why we can standardize across populations \n",
    "- Also allows a way in to positivity by looking at Technical Point 3.1 in hernan robins causality book.\n",
    "- explain residual confounding so that it can be reffered to later when estimating weights - for example explaining why splines help to remove any residual confounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graphs: graphical representations of causality\n",
    "\n",
    "Causal relationships, like those described in the previous section, can be represented using graphs. A graph consists of a finite set of vertices $\\nu$ and a set of edges $\\epsilon$. The vertices of a graph correspond to a collection of random variables which follow a joint probability distribution $P(\\nu)$. Edges in $\\epsilon$ consist of pairs of distinct vertices and denote a certain relationship that holds between the variables \\citet{Pearl2009}. The absence of an edge between two variables indicates that the variables are independent of one another. The direction of the causal relationship is denoted by an arrow and is acyclic because causal relationships between two variables only proceed in one direction. There are no feedback loops or mutual causation because in a causal framework a variable cannot be a cause of itself directly or indirectly \\citet{Hernan2004}.\\linebreak\n",
    "\n",
    "For example, figure ? represents the case where interest is in the causal relationship between treatment $X$ and outcome $Y$. Treatment is assigned according to conditional distributions $P(treatment|male)$ and $P(treatment|female)$. Once treatment has been assigned, the outcome $Y$ is determined by both $X$ and $Z$ by the conditional distribution P(Y|X, Z). \\citet(Pearl2001}, \\citet(Pearl2014}. Blocking or screening off $Z$ has the same inuition as explained in the section on confounding. The causal effect of X and Y cannot be different between two subjects because of $Z$ when everyone is that strata has the same value of $Z$. Blocking is the same as holding $Z$ constant. Intuition to drive forward is that difference in outcome cannot be due to strata when everyone shares that strata.\n",
    "\n",
    "Show the same graph without causal relationship between X and Y. There is a marginal dependence between X and Y through Z, but once we condition on Z this dependence dissapears as shown by the lack of an edge between X and Y. Once we condition on Z (i.e. we know that the subject was male of female.) then the margial dependence dissapears. Introduce idea of common cause here as well.\n",
    "\n",
    "Both treatment and outcome are determined by sex leading to a spurious association between $X$ and $Y$ through $Z$. This is called a \"back door\" path between $X$ and $Y$. Conditioning on $Z$ is reprsented graphically by blocking the back door and any spurious assocations to allow causal estimation\n",
    "\n",
    "- use example of cause by just removing an arrow from the DAG to illustrate the point that there must be a cause so adding back the causal arrow of interest does not change the fact that part of the cause comes through the confounder.\n",
    "- common cause and structural approach to selection bias paper.\n",
    "- the absense of an arrow means no direct effect between two variables cole 2009 illustrating bias paper.\n",
    "- Didelez 2010 and Pearl 2009 for connecting DAGs to probability distributions and factorizations.\n",
    "- explain how colliders block relationships so that variables are independent if there is a collider on their path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Simple causal structure with confounding.\n",
    "\n",
    "\\begin{figure}\n",
    "\\begin{tikzpicture}\n",
    "\n",
    "% nodes %\n",
    "\\node[text centered] (l0) {$L_0$};\n",
    "\\node[below = 3 of l0, text centered] (a0) {$A_0$};\n",
    "\\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};\n",
    "\n",
    "% edges %\n",
    "\\draw[->, line width= 1] (a0) --  (y);\n",
    "\\draw[->, line width= 1] (l0) --  (a0);\n",
    "\\draw[->, line width= 1] (l0) --  (y);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "\\caption{Causal graph}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time dependent confounding\n",
    "\n",
    "So far we have considered the time fixed context in which treatment and confounders take on a single value. It was sufficient to block the \"back door\" path between the treatment and outcome by conditioning on the confounding variable(s). In the headache example, the causal effect of ibuprofen on headache alleviation was confounded by sex. For most people, sex is a time-fixed covariate because it does not change value over time. To broaden the setting to a time dependent context, we adopt the canonical example of the causal effect of Zidovudine (AZT) on mortality amongst human immunodeficiency virus (HIV)-infected subjects \\citet{Hernan2000}. In this example, subjects are measured at baseline $t = 0$ and at subsequent visits. In each visit the patient's CD4 lymphocyte count is measured and a treatment decision  made. Survival at the end of follow-up is a binary outcome equal to 1 if the patient has died and 0 otherwise. \\linebreak\n",
    "\n",
    "The time-fixed notation can be extended to include subject histories for time varying variables. Treatment and covariate histories up to visit $k$ are can be represented by an overhead bar. For example, $\\bar X_{k} = \\{X_{0}, \\hdots, X_{k}\\}$ represented the vector of treatment decisions while $\\bar Z_{k} = \\{Z_{0}, \\hdots, Z_{k}\\}$ represents the vector of measurements on the time dependent-confounder $Z$. Time-fixed covariates like sex, or covariates which change linearly over time like age are tyically recorded at baseline ($t = 0$) and we denote the collection of baseline covariates as $V_{0}$.  The outcome of interest at the end of follow-up is mortality $Y$ which is a binary variable taking the value $1$ if the patient is dead and $0$ otherwise. \\linebreak \n",
    "\n",
    "Just as in the time-fixed case, time-dependent confounders lead to spurious associations between $X$ and $Y$ through a \"back door\" path between $X$ and $Y$ through $L$. To estimate a causal effect it is necessary to block this path by conditioning on the confounding variables. Figure ? gives an example of this in the time dependent case for two periods ($t = 0, 1$). In the first period a treatment decision is made based on the measured confounder $Z_0$. In the second period ($t = 1$) a new treatment decision is made based on both $Z_0$ and $Z_1$. Conditioning on $\\bar Z$ under this DAG leads to a consistent estimate of the causal effect because doing so blocks all paths between $X_0$ and $X_1$ and $Y$ except the causal path of interest. \\linebreak\n",
    "\n",
    "However, the time-dependent context also admits structures like the middle pane of figure ? with the addition of a causal relationship between $X_0$ and $Z_1$. It is now possible for current treatments to be a determinant of future confounders which are in turn determinants of future treatment \\citet{Robins2000a}. As a result the effect of $A_0$ on $Y$ is mediated through $L_1$ in the path $A_0 \\rightarrow L_1 \\rightarrow Y$. Blocking this path by conditioning on $Z$ also blocks some portion of the effect of $A_0$ on $Y$ and will lead to a biased estimate. \\linebreak\n",
    "\n",
    "A second danger in the time-dependent context arises when $Z$ is a common effect of treatment and an unmeasured variable $U$ which also influences the outcome $Y$. There is no direct association \n",
    "Figure ? shows the same two structures with the addition of a an unmeasured variable $U$ which influences $Z$ and $Y$. Conditioning on $Z$. Selection bias precludes unbiased estimation \\citet{Hernan2004}. There is a mediating relationship between $A$ and $Z$ in which case there is a spurious relationship between $A$ and $Y$ again? This is less inuitive and so examples are best according to \\citet{Cole2010}. We can say that Z is a common effect of A and U, once we condition on Z we create a dependence of A on U. U is a cause of Y and hence there is an association between A and Y. This association is present even when there is no direct causal path between A and Y. \\linebreak\n",
    "\n",
    "Hazard ratios and selection bias \\citet{Hernan2010}. Actual application will look at toxicity of treatment. Some people will be suceptible and drop out leaving more people in the untreated arm of the study. presumably because in any population some people are more suceptible than others.\n",
    "\n",
    "general point about selection bias is that the general population is not a valid control group. This is interesting because it links very closely with the counterfactual approach which defines the causal effect, not with references to a population but to an individual.\n",
    "\n",
    "Conclusion, 1) clearly a different technique is required for analysis 2) the nature of time dependent case needs to be described fully enough to explain why we choose the simulation algorithm that we choose and any holes in it. In subsequent sections our choice of simulation algorithm will be motivated by the structure of time dependent confounding as well as the viability of introducing positivity violations which are propogated through the time dependent structure. Explain meaning of a collider and that a collider that is conditioned on will not block confounding. Essentially with this kind of data we cannot use confoudning or stratification methods.\n",
    "\n",
    "- Intuition from Pearl 2009 book pp. 17 also on schools. More intuition in cole 2010\n",
    "- Simpson's paradox linked to making comparisons within strata - collapsibility\n",
    "- explain that we are often interested in parsimnonious models so cannot have all covariates $U$ that will create associations between $X$ and $Y$\n",
    "- explain why we do not need to worry about the path between A_0 and A_1\n",
    "- explain why mediation is likely to occur in example.\n",
    "- explain why saturated models cannot be used because they will have \n",
    "- inituitive examples of selection bias.\n",
    "- Saturated models are not an option because they would be computationally intensive and so we use parametric models which also links to positivity because we smooth over zeroes in certain strata.\n",
    "- Explain why hazard ratios have a built in selection bias after giving some examples of why selection bias arises. It is because it is selective on patients reaching the time period in question. Is this also the reason why summary methods create selection bias.\n",
    "- give an inuitive explanation for why CD4 count is a predictor of subsequent treatment and of death.\n",
    "- Because treatment is randomized (at baseline) in expectation the proportion of men and women in each group is the same. \n",
    "- The reason why summary values are a problem for selection bias is in \\citet{Robins1992} is similar/the same as hernan on hazards of hazard ratios.\n",
    "- explain that Z is a mediator variable and also explain colliders and why conditioning on a collider creates an association between A and U and hence A and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% time dependent causal structure with confounding.\n",
    "\n",
    "\\begin{figure}\n",
    "\n",
    "\\begin{minipage}{.2\\textwidth}\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\n",
    "% nodes %\n",
    "\\node[text centered] (l0) {$L_0$};\n",
    "\\node[below = 3 of l0, text centered] (a0) {$A_0$};\n",
    "\\node[right = 3 of l0, text centered] (l1) {$L_1$};\n",
    "\\node[right = 3 of a0, text centered] (a1) {$A_1$};\n",
    "\\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};\n",
    "\n",
    "% edges %\n",
    "\n",
    "%L0%\n",
    "\\draw[->, line width= 1] (l0) --  (l1);\n",
    "\\draw[->, line width= 1] (l0) --  (a0);\n",
    "\\draw[->, line width= 1] (l0) --  (a1);\n",
    "\\draw[->, line width= 1] (l0) --  (y);\n",
    "\n",
    "%A0%\n",
    "\\draw[->, line width= 1] (a0) --  (a1);\n",
    "\\draw[->, line width= 1] (a0) --  (y);\n",
    "\n",
    "%L1%\n",
    "\\draw[->, line width= 1] (l1) --  (a1);\n",
    "\\draw[->, line width= 1] (l1) --  (y);\n",
    "\n",
    "%A1%\n",
    "\\draw[->, line width= 1] (a1) --  (y);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "\n",
    "\\end{minipage}\n",
    "\\hspace{5cm}% NO SPACE BETWEEN \\end \\hspace and \\begin!\n",
    "\\begin{minipage}{.2\\textwidth}\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\n",
    "% nodes %\n",
    "\\node[text centered] (l0) {$L_0$};\n",
    "\\node[below = 3 of l0, text centered] (a0) {$A_0$};\n",
    "\\node[right = 3 of l0, text centered] (l1) {$L_1$};\n",
    "\\node[right = 3 of a0, text centered] (a1) {$A_1$};\n",
    "\\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};\n",
    "\n",
    "% edges %\n",
    "\n",
    "%L0%\n",
    "\\draw[->, line width= 1] (l0) --  (l1);\n",
    "\\draw[->, line width= 1] (l0) --  (a0);\n",
    "\\draw[->, line width= 1] (l0) --  (a1);\n",
    "\\draw[->, line width= 1] (l0) --  (y);\n",
    "\n",
    "%A0%\n",
    "\\draw[->, line width=1] (a0) --  (a1);\n",
    "\\draw[->, line width=1] (a0) --  (l1);\n",
    "\\draw[->, line width=1] (a0) --  (y);\n",
    "\n",
    "%L1%\n",
    "\\draw[->, line width= 1] (l1) --  (a1);\n",
    "\\draw[->, line width= 1] (l1) --  (y);\n",
    "\n",
    "%A1%\n",
    "\\draw[->, line width= 1] (a1) --  (y);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "\n",
    "\\end{minipage}\n",
    "\\caption{Figure 1 DAG} \\label{fig:fig2}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% time dependent causal structure with confounding.\n",
    "\n",
    "\\begin{figure}\n",
    "\n",
    "\\begin{minipage}{.2\\textwidth}\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\n",
    "% nodes %\n",
    "\\node[text centered] (l0) {$L_0$};\n",
    "\\node[below = 3 of l0, text centered] (a0) {$A_0$};\n",
    "\\node[right = 3 of l0, text centered] (l1) {$L_1$};\n",
    "\\node[right = 3 of a0, text centered] (a1) {$A_1$};\n",
    "\\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};\n",
    "\n",
    "% edges %\n",
    "\n",
    "%L0%\n",
    "\\draw[->, line width= 1] (l0) --  (l1);\n",
    "\\draw[->, line width= 1] (l0) --  (a0);\n",
    "\\draw[->, line width= 1] (l0) --  (a1);\n",
    "\\draw[->, line width= 1] (l0) --  (y);\n",
    "\n",
    "%A0%\n",
    "\\draw[->, line width=1] (a0) --  (a1);\n",
    "\\draw[->, line width=1] (a0) --  (l1);\n",
    "\\draw[->, line width=1] (a0) --  (y);\n",
    "\n",
    "%L1%\n",
    "\\draw[->, line width= 1] (l1) --  (a1);\n",
    "\\draw[->, line width= 1] (l1) --  (y);\n",
    "\n",
    "%A1%\n",
    "\\draw[->, line width= 1] (a1) --  (y);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "\n",
    "\\end{minipage}\n",
    "\\hspace{5cm}% NO SPACE BETWEEN \\end \\hspace and \\begin!\n",
    "\\begin{minipage}{.2\\textwidth}\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\n",
    "% nodes %\n",
    "\\node[text centered] (l0) {$L_0$};\n",
    "\\node[below = 3 of l0, text centered] (a0) {$A_0$};\n",
    "\\node[right = 3 of l0, text centered] (l1) {$L_1$};\n",
    "\\node[right = 3 of a0, text centered] (a1) {$A_1$};\n",
    "\\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};\n",
    "\n",
    "% edges %\n",
    "\n",
    "%L0%\n",
    "\\draw[->, line width= 1] (l0) --  (l1);\n",
    "\\draw[->, line width= 1] (l0) --  (a0);\n",
    "\\draw[->, line width= 1] (l0) --  (a1);\n",
    "\\draw[->, line width= 1] (l0) --  (y);\n",
    "\n",
    "%A0%\n",
    "\\draw[->, line width=1] (a0) --  (a1);\n",
    "\\draw[->, line width=1] (a0) --  (l1);\n",
    "\\draw[->, line width=1] (a0) --  (y);\n",
    "\n",
    "%L1%\n",
    "\\draw[->, line width= 1] (l1) --  (a1);\n",
    "\\draw[->, line width= 1] (l1) --  (y);\n",
    "\n",
    "%A1%\n",
    "\\draw[->, line width= 1] (a1) --  (y);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "\n",
    "\\end{minipage}\n",
    "\\caption{Figure 1 DAG} \\label{fig:fig2}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Probability of Treatment Weighting \n",
    "\n",
    "The previous section has highlighted how standard approaches for controlling for confounding in a time dependent context may lead to biased estimates. In this section we describe a technique called inverse probability of treatment weighting that can be used to obtain unbiased estimates of the causal effect of treatment on outcome in the presence of time dependent confounding.\n",
    "\n",
    "Inverse probability of treatment weighting is a technique that can be used to obtain unbiased estimates of the causal effect of treatment on outcome in the presence of time dependent confounding. The intuition behind the technique is that by re-weighting the data a pseudopopulation is created in which the treatment is independent of any measured confounders. Regression analysis on the pseudopopulation can be carried out without the need to control for confounders eliminating the problems which arose in the previous section due to conditioning on $Z$. Crucially, in the pseudopopulation, the causal effect of $X$ on $Y$ remains unchanged. As a result, it is possible to estimate the true causal effect of $X$ on $Y$.\n",
    "\n",
    "#### Construction of weights\n",
    "\n",
    "- why does it work and naive methods do not?\n",
    "- How does it break the link between X and Z?\n",
    "- Dangers associated with startification and controlling for methods highlighted already explain why we need other methods - explain a few of these like G-estimation, SNTM etc.\n",
    "- creates a pseudo-population in which we have something similar to an experimental setting\n",
    "- Because we need weights this means we need a model for the weights - model can be non parametric or parametric depending on data used.\n",
    "- Contrast IPTW methods with stratification methods.\n",
    "\n",
    "\\citet{Horvitz1952}\n",
    "\n",
    "Areas where IPTW has been used (time dependent confounding, comparing dynamic regimes, missing data)\n",
    "\n",
    "Inverse probability of treatment weighting is a technique that re-weights subject observations to a population where assignment of treatment is at random. An early example of this technique is the \\citet{Horovitz1952} weighted estimator of the mean. In the context of marginal structural models, a weight is calculated for each subject which can be thought of informally as the inverse of the probability that a subject receives their own treatment \\citet{Robins2000}. The result of applying these weights is to re-weight the data to create a pseudo-population in which treatment is independent of measured confounders \\citet{Cole2008}. Crucially, in the pseudo population the counterfactual probabilities are the same as in the true study population so that the causal RD, RR or OR are the same in both populations \\citet{Robins2000}.\n",
    "\n",
    "$$w_{t,i} = \\frac{1}{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "stabilized weights\n",
    "$$sw_{it} = \\frac{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i})} {\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "- why can weights be very unstable?\n",
    "- show why the stabilized weights have a mean of 1 by applying law of iterated expectations. \\citet{Hernan2006}\n",
    "- see appendix 1 of cole 2008 for good informal/intuitive explanation of stabilized weights.\n",
    "- describe no-parametric way of estimating numerator P(A=1) by cases/total subjects or saturated model with just an intercept.\n",
    "\n",
    "The use of IPTW is valid under the four assumptions of consistency, exchangeability, positivity and no misspecification of the model \\citet{Cole2008}. \n",
    "\n",
    "Informally a patients weight through visit k is proportional to the inverse of the probability of having her own exposure history through visit k (Cole and Hernan 2008)\n",
    "\n",
    "The weight is informally proportional to the participants probability of receiving her own exposure history\n",
    "\n",
    "As these weights have high instability we need to stabilize them. The unstabilized weights can be driven by only a small number of observations. Why are they unstable?\n",
    "\n",
    "- true weights are unknown but can be estimated from the data.\n",
    "- $A_t$ is no longer affected by $L_t$, and crucially the causal effect of $\\bar A$ on $Y$ remains unchanged\n",
    "\n",
    "Be more specific about what is contained in the weights. The denominator depends on the measured confounders $L$ the numerator does not.\n",
    "\n",
    "- weighted regression and MSM are equivalent.\n",
    "\n",
    "Point out that we need baseline variables in the conditional statments in the num and denom of the weights otherwise we break the relationship between outcome and baselines in the new pseudo-population. If the baseline variables are not confounders, then we do not want to break this relationship. Baseline covariates also help to stabalize the weights (how?)\n",
    "\n",
    "importantly, changing the relationship between L and A, won;t change the relationship between L and Y. This means that an intervention in $A$ does not affect the relationship between $L$ and $Y$. So we remove the link between $L$ and $A$ and assign to $A$ the value of treatment on or off. Once we place the patient on treatment, regardless of the relationship which had existed before hand between the covariate and treatment, a new relationship between $A$ and $Y$ exists in which the covariate has no say. \n",
    "\n",
    "- stabilized weights should have a mean of 1\n",
    "\n",
    "\n",
    "### Applications of IPTW\n",
    "\n",
    "Different from application of MSMs - IPW is a technique which has been applied to MSMs - i.e. MSMs are on example of how IPTW can be used.\n",
    "\n",
    "- Crucial that the relationship between Y and X remains the same in the new population. I.e. the marginal structural model is the same.\n",
    "\n",
    "Have been used for missing data problems. see pp.442 of Hernan, Brumback, Robins 2001 for a list of papers linked to this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "This section formalises the five assumptions under which inverse probability weighting can be used to correctly estimate MSMs. The first three assumptions of consistency, correct model specification and no measurement error are dealt with briefly. The exchangeability  . There is a trade off between finer confounder control and positovity. As a result, this section will mainly focus on exchangeability and positivity assumptions with only brief exposition of the consistency, correct model specification and measurement bias assumptions. Where necessary the reader is referred to further work on these assumptions. The assumption of no unmeasured confounders has received the most attention in the literature ().  Most attention will be given to positivity. Conditions under which IPTW work are largely untestable (westreich 2012)\n",
    "\n",
    "\n",
    "### No unmeasured confounders\n",
    "\n",
    "The assumption of no unmeasured confounders has already been discussed with respect to confounding\n",
    "\n",
    "$$Y_{x} \\perp\\!\\!\\!\\perp X$$\n",
    "\n",
    "- necessity of identifying the most important confounders \\citet{}, \\citet{}\n",
    "- conditional exchangeability.\n",
    "- this assumption is not empirically verifiable.\n",
    "\n",
    "### Consistency\n",
    "\n",
    "The consistency assumption states that the actual outcome $Y_{i}^{obs}$ for a subject $i$ is equal to the potential outcome $Y_{i}^{x}$ when the treatment received by subject $i$ is $x$, that is $X_i = x$ (\\citet{Cole2009}). The consistency assumption is required to make inferences about $y^{x}$ using observational data because it connects the observational data to the potential outcomes. For example, if it was known for a subject $i$ that ibuprofen did not alleviate headaches, then $Y_{i}^{x=1} = 0$ is the potential outcome associated with these events. However, if the same subject drank a glass of water along with the ibuprofen and this action did alleviate the headache, then $Y_{i}^{obs} = 1$ and the potential outcome is not equal to the observed outcome despite the fact that $X_i=1$. In other words, the consistency assumption rules out side effects of exposure and anchors the observational outcome to the potential outcome framework. As a result, expressions involving probabilities of counterfactuals can be cast in terms of ordinary conditional probabilities of measured variables \\citet{Pearl2010} and equated as follows.\n",
    "\n",
    "$$P(Y^x = y\\ |\\ Z = z, X = x) = P(Y = y\\ |\\ Z = z, X = x)$$\n",
    "\n",
    "In words, the probability that $Y^x = y$ when $Z = z$ and $X = x$ are observed is equal to the conditional probability that $Y = y$. The consistency assumption has generated discussion over whether it is really an assumption an axiom or definition. The interested reader is referred to \\citet{VanderWeele2009}, \\citet{Cole2009} and \\citet{Pearl2010}.\n",
    "\n",
    "### No model mispecification\n",
    "\n",
    "Estimating MSMs with a continuous confounder involves specifying a model for the weights and a structural model relating exposure to outcome. In both models, correct specification is required to obtain unbiased estimates. The structural model requires positing a relationship between exposure and outcome. For example, this relationship may be best captures through a linear relationship, threshold dose-response or a model accounting for long and short term effects of exposure \\citet{Cole2009}. The weight model also needs to be correctly specified in order to consistenly estimate the weights. \n",
    "\n",
    "\n",
    "For the weight model, the stabilized weights should be one. Parametric models are unlikely to be perfectly specified but should provide a good approximation of the true model. Deviations from one are an indication that the weight model is mispecified. \n",
    "\n",
    "Several studies have examined the effect of model mispecification in the estimatuion of MSMs including \\citet{Cole2009}. Some key findings. Broadly, the idea is to simulate data from a known MSM weight model and introduce realistic deviations from the model.\n",
    "\n",
    "Correct specifcation of the inverse probability weighting (IPW) model is necessary for consistent inference from\n",
    "a marginal structural Cox model (MSCM).\n",
    "\n",
    "- link to why simulation studies make it possible to isolate model specification as an error.\n",
    "- stabilized weights should have a mean of 1 - indicative of model mispecification if they do not\n",
    "- results in some confounding if the model is mispecified, think about using splines to mop up residual confounding\n",
    "- IPW cannot correctly adjust for all confounding under model miscpecification\n",
    "\n",
    "In this thesis we simulate data from a known weight and structural model. In other words, we simulate data in the absence of model mispecification elimiating this as a source of bias. Importantly, this makes it possible to isolate effects of interest without worrying about model miscpecification.\n",
    "\n",
    "### No measurement error\n",
    "\n",
    "Measurement error can affect the outcome, exposure or confounder and other covariates used to estimate MSMs. This can arise due to faulty equipment, poor recall by survey respondents or simply carlessnes and rounding. In each case the observed variable $X^*$ differs from the true underlying variable $X$. In general this will result in bias but the extent of that bias depends on the process through which the error is introduced and whether  with error is recorded. In this thesis we employ a simulation algorithm to generate data from a known marginal structural model and the simulated variables have no measurement error. Analgous to the case of no model miscpecification, this makes it possible to study the properties of MSMs in the absence of measurement error. More detail on the effect of measurement error in a causal context can be found in \\citet{Hernan2009}. \n",
    "\n",
    "### Positivity.\n",
    "\n",
    "To conclude this chapter ....\n",
    "\n",
    "- use intuition of making comparisons within strata as a means to introduce positivity and link to weight model and standardization as examples.\n",
    "\n",
    "The final assumption underlying MSMs, and the central topic of this thesis, is the positivity assumption. MSMs are used to estimate average causal effects in the study population, and one must therefore be able to estimate the average causal effect in every subset of the population defined by the confounders \\citet{Cole2008}. The positivity assumption requires that there be exposed and unexposed individuals in every strata of the confounding covariates. For example, when treatment is Zidovudine and CD4 count is the confounder, there must be a positive probability of some patients being exposed and unexposed at every level of CD4 count. Positivity can be expressed formally as $Pr(A=a\\ |\\ L) > 0$ for all $a \\in A$, which extends straightforwardly to the time dependent case where the positivity assumption must hold at every time step conditonal on previous treatment, time dependent confounders and any baseline covaraiates:\n",
    "\n",
    "$$Pr(A_{it}=a_{it}\\ |\\ L_{it}, A_{i, t-1}, V_{i0}) > 0$$\n",
    "\n",
    "Models for the risk $P(Y=1\\ |\\ A=a, L=l)$ are commonly studied in epidemiological applications. Applying basic probability rules reveals that the risk can be re-written with the term $Pr(A=a\\ |\\ L=l)$ in the denominator:\n",
    "\n",
    "$$P(Y=1\\ |\\ A=a, L=l) = \\frac{P(Y=1, A=a, L=l)}{Pr(A=a, L=l)} = \\frac{P(Y=1, A=a, L=l)}{Pr(A=a\\ |\\ L=l)Pr(L=l)}$$\n",
    "\n",
    "This model is only estimable when $Pr(A=a\\ |\\ L=l) \\neq 0$. Therefore, when positivity does not hold it is not possible to estimate the model. In the context of MSMs a similar problem emerges. Although weighting via IPTW allows naive estimation of (?) without including the confounders, the weights in (?) involved the term $Pr(A=a\\ |\\ L=l)$ in the denominator. This means that the weights are inestimable whenever positivity is violated. In order to estimate the causal effect of $A$ on $Y$, weights must be estimable in every subset of the population otherwise the average causal effect in the study population cannot be estimated. \\linebreak\n",
    "\n",
    "In practice, positivity can arise when random zeroes or structural zeroes are present in some levels of the confounding covariates. Random zeroes arise when, by chance, no individuals or all individuals, receive treatment within a certain strata as defined by the covariates. For example, \\citet{Cole2008} studies positivity violations in individuals in strata defined by CD4 count and viral load. By increasing the levels of CD4 count the chances of random zeroes also increases and \\citet{Cole2008} show that the IPT weights rapidly lose their stability with the consequence that causal effects are no longer estimable. Researchers applying IPTW methods must actively check that there are both treated and untreated individuals at every level of their covariates within cells defined by their covariates because parametric methods will smooth over positivity violations and not provide any indication of nonpositivity. Increasingly refined covariates are attractive because they provide better control of confounding, but the point that \\citet{Cole2008} make is that this control needs to be traded off against increased occurence of random zeroes and subsequent instability of IPT weights. \\linebreak\n",
    "\n",
    "More relevant to this thesis are violations of the positivity assumption due to structural zeroes. These occur when an individual cannot possibly be treated or if an individual is always treated within some levels of the confounding covariate, as is the case in the clinical protocol example motivating this thesis. Several studies give examples of structural violations of the positivity assumption in epidemiological contexts. In \\citet{Cole2008} structural zeroes arise  when the health effects due to exposure to a chemical are confounded by health status proxied by being at work. If individuals can only be exposed to the chemical at work then all individuals not at work will be unexposed. A second example is liver disease as a contraindication of treatment. If individuals with liver disease cannot be treated then all individuals in the \"liver disease = 1\" strata will be untreated. In \\citet{Messer2010} structural zeroes arise in the context of rates of preterm birth and racial segregation, whereas \\citet{Cheng2010} find structural zeroes in the context of fetal position and perinatal outcomes. Our motivating example is most closely related to liver disease as a contraindication, except that the clinical protocols require that patients with low CD4 count always be treated instead of never being treated, as in the case in the liver disease example. \\linebreak\n",
    "\n",
    "Although in many epidemiological settings the positivity assumption is guaranteed by experimental design, studying positivity violations is relevant because, as our own motivating example and the examples above suggest, structural violations do occur, and random zeroes are always possible especially at finer levels of confounding covariates. Studying the finite sample propoerties of MSMs under violations to positivity is therefore an important issue which is yet to be dealt with systematically in the literature. As \\citet{Westreich2010} points out, positivity violations, positivity violations by a time varying confounder pose an analytic challenge and they suggest g-estimation or g-computation may be a way forward. A good start to dealing with the time varying confounder case is to see how well MSMs work when positivity is violated. This is also a novelty of this thesis. \n",
    "\n",
    "6. estimated weights with a mean far from one, or very extreme values indicate either non-positivity or model mispecification of the weight model.\n",
    "7. It is not always true that we want more finely tuned covariates for confounder control because the bias and variance of the effect estimate may increase with the number of categories. This is similar to the positivity masking example.\n",
    "11. Our results are equally valid for other circumstances in which positivity may arise.\n",
    "12. Also think about how the number of categories of exposure increases the chance that one level of exposure will have a positivity.\n",
    "13. Westreich and Cole 2010 have suggested that methodological approaches are needed to weigh the resultant biases incurred when trading of confounding and positivity. The framework we use is flexible enough to allow this in a simulation setting.\n",
    "\n",
    "If the structural bias occurs within levels of a time-dependent confounder then restriction or censoring may lead to bias whether one uses weighting or other methods (Cole and Hernan 2008). In fact, weighted estimates are more sensitive to random zeroes (Cole, Hernan, 2008)\n",
    "Introducing violations of positivity can be achieved by censoring observations.\n",
    "\n",
    "But to give an intuitive example, think about how it links back to a situation where sicker patients receive treatment compared to others. So in the \"sick\" strata of the CD4 count **ALL** patients receive treatment which inflates the IPTW. This also affects how we think about the associational versus causal models. The causal effect might be 50/50 but because sicker patients get treatment the mortality ratio in the treated group is likely to be higher.\n",
    "\n",
    "The trade-off between positivity and confounding bias is emphasized in Cole2008\n",
    "\n",
    "Why is practicality important? Cole paper highlights practical advice to practictioners. positivity can be violated in a practical setting because of two few strata, it can be the result of protocols in a clinical setting and it can be seen as a trade-off between exchangeability (and we need more measured predictors to maintain exchangeability) and positivity where more predictors leads to more likely a zero problem.\n",
    "\n",
    "- Dynamic stratgies evaluated using MSMs will have rules like, start treatment if CD4 falls below a certain threshold. See Didelez presentation on this\n",
    "- Explain that there is positivity in estimation of the structural model and also in the weight model. The reason why positivity is more important in the weight model is because when the weights are unstable the estimates can be very wrong as a result.\n",
    "- read and add \\citet{Naimi2011} who have already done a simulation study of non-positivity\n",
    "    - in context of healthy worker effect\n",
    "    - explain why this doesn't get to the heart of the issue of non-positivity\n",
    "    - a relevant question highlighted by \\citet{Naimi2011} is whether the effect of non-positivity is amplified with more than two time periods.\n",
    "    - explain why healthy worker effect and positivty go neatly together because as a result of people dropping out there will be empty cells/strata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that parameterises $P(Y\\ |\\ do(A=a))$ is called a marginal structural model (MSM) as it is marginal over any covariates and structural in the sense that it represents an interventional rather than observational model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key concepts in survival analysis\n",
    "\n",
    "This chapter briefly reviews several important concepts in survival analysis which are pertinent to this thesis and specifically the simulation algorithm that will be used to generate data in later chapters. Survival analysis is the study of the distribution of life times. For example, in. Often we are interested in comparing survival in two or more groups. For example a group which is exposed to treatment and a group which is not. Confounding is also important. Survival analysis is by nature time dependent and hence time dependent confounding is particularly studies in a survival context. The basic concepts reviewed here are a condensed version of those presented in \\citet{JohnP.Klein2003}.\n",
    "\n",
    "- Explain why we do not look at censoring and truncation.\n",
    "- emphasis on discrete time which is the application.\n",
    "\n",
    "### Survival function\n",
    "\n",
    "The survival time $T$ is the time between a well defined start point and a well defined end point. For example the time between birth and death. The survival function is the probability that a certain individual survives until time $t$ or equivalently the probability that the survival time $T$ is greater than $t$, $$S(t) = P(T > t)$$. In the continuous case the survival function can be written\n",
    "\n",
    "$$S(t) = P(T > t) = \\int_{x}^{\\infty}f(t)dt$$\n",
    "\n",
    "In the discrete case the survival function \n",
    "\n",
    "$$S(t) = P(T > t) = \\sum_{x_j > x}p(x)$$\n",
    "\n",
    "### Hazard function\n",
    "\n",
    "The hazard function or hazard rate expresses *the approximate probability of an individual of age x experiencing the event in the next instant.*\n",
    "\n",
    "$$h(x) = \\lim\\limits_{\\Delta x \\to 0}\\frac{P(x \\le X \\le x + \\Delta x\\ |\\ X \\ge x)}{\\Delta x}$$\n",
    "\n",
    "In the discrete case, the probability that the event occurs between $j-1$ and $j$ is equal to the difference in survival of these times $p(x_j) = S(x_j) - S(x_{j-1})$. The hazard function is \n",
    "\n",
    "$$h(x) = Pr(X = x_j\\ |\\ X \\ge x_j) = \\frac{p(x_j)}{S(x_{j - 1})}$$\n",
    "\n",
    "$$h(x) = 1 - \\frac{S(x_j)}{S(x_{j - 1})}$$\n",
    "\n",
    "$$S(x) = \\prod \\frac{S(x_j)}{S(x_{j - 1})} = \\prod 1 - h(x_j)$$\n",
    "\n",
    "Showing that the survival function is determined by the hazard rates.\n",
    "\n",
    "In words, the discrete hazard function. There are two probabilities, the probability that the death occurs \n",
    "\n",
    "\n",
    "$$Pr(Y=1\\ |\\ X=x) = \\frac{e^{\\beta_0 + \\beta_1 \\times x}}{e^{\\beta_0 + \\beta_1 \\times x} + 1} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\times x)}}$$\n",
    "\n",
    "- show that hazard function is not collapsible or hazard ratio.\n",
    "- condition on survival up to a certain point?\n",
    "- Hazard ratios differ from relative risks and odds ratios in that RRs and ORs are cumulative over an entire study, using a defined endpoint, while HRs represent instantaneous risk over the study time period, or some subset thereof.\n",
    "- compare hazard rate and hazard function with the hazard ratio. The hazard ratio could be between men and women for example.\n",
    "- relate survival to hazard function in order to show that survival is completely determined by hazard rate.\n",
    "- Naturally survival is measured at each follow up point, but as the person is alive until their final follow up then we are always conditioning on Y = 0 until the final follow up.\n",
    "- a logistic model is one where the log-odds of the probability of an event is a linear combination of independent or predictor variables.\n",
    "- need to make clear that we could split groups into a control and treatment group if they are exchangeable.\n",
    "\n",
    "### Proportional hazards\n",
    "\n",
    "- explain multiplicative model\n",
    "- causal model\n",
    "- discrete time relationship between cox model and logistic model\n",
    "\n",
    "Often we are interested in how survival differs between subjects who have been exposed to a treatment versus those who remain unexposed.  In a randomised trial exchangeability is guaranteed and the survival function can be calculated. In observational trials the same confounding issues described in the previous chapter require a different approach. Adjusting for other variables is important to remove bias and provide a more accurate fit.\n",
    "\n",
    "In a typical study we want to adjust for confounders and other variables in order to get a better estimate than just looking at treated versus non-treated patients. \n",
    "\n",
    "- explain how we let the survival time depend on covariates.\n",
    "- link to causal framework, introduce marginal structural logistic regression or marginal structural cox proportional hazards model.\n",
    "- test for equality between (all) groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating from marginal structural models\n",
    "\n",
    "In order to assess the impact of violations of the positivity assumption on the performance of the IPTW estimator we simulate data from a specific marginal structural model in a series of monte carlo simulations. Several algorithms for simulating from marginal structural models have been suggested in the literature. To test the effect of positivity violations on the performance of marginal structural models we use the algorithm of \\citet{Havercroft2010}. This algorithm has several key features which are described in detail in this chapter. We start with an overview of the logic behind monte carlo simulations in general terms. \n",
    "\n",
    "In this section we first describe in general terms the logic behind monte carlo simulations. The specific  In this chapter we start by describing the logic behin monte carlo simulations in general terms. Next, we consider several important criteria that a simulation model must exhibit in the context of MSMs. In particular we require an algorithm that can simulate from a specific MSM, has the observational structure described earlier and we also define noncollapsibility. Several algorithms have been proposed in the literature and these are briefly discussed and compared. We then focus on the algorithm suggested in \\citet{} and explain why it satisfies our requirements. The most salient aspects of this algorithm for the purposes of this thesis are described. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Simulations\n",
    "\n",
    "In statistical research, interest often lies in the estimation of a population parameter $\\theta$. When only a sample $X_1$ from the population is available, statistical procedures are applied to obtain an estimate of the population parameter $\\hat \\theta_1$ based on that sample. That is, $\\hat \\theta_1$ is an estimate of $\\theta$ estimated from the sample $X_1$.  The same procedure applied to a second sample $X_2$ drawn from the same population will result in second estimate $\\hat \\theta_2$, and so on for more samples. Because only one sample is tyically available, the sampling distribution of $\\hat \\theta_i$ is used to make inferences. \\linebreak \n",
    "\n",
    "In methodological research, simulations can flip this process on its head. We start with a known true model and generate multiple samples according to this model. The properties of a statistical method Rather than being a process of dicoverey we can use simulations as a process of rediscovery and appraise a method by its ability to correctly rediscover the parameter. \\linebreak\n",
    "\n",
    "a known \"true\" model governing a population can be specified and a sample of data generated according to that model. For instance, the probability that a headache is alleviated after one hour could be expressed using a logistic model. \n",
    "\n",
    "$$log \\frac{P(Y = 1\\ |\\ X = 1)}{P(Y = 1\\ |\\ X = 0)} = \\alpha + \\beta \\times x$$\n",
    "\n",
    "\n",
    "\n",
    "- show that the logistic model is the log odds model or odds ratio model.\n",
    "- explain that the cox prop hazard model discrete time equivalent is the logistic model.\n",
    "\n",
    "So that the log odds ratio between those who received treatment and those who did not is given as\n",
    "\n",
    "$$log \\frac{P(Y = 1\\ |\\ X = 1)}{P(Y = 1\\ |\\ X = 0)} = \\beta_0 + \\beta_1 \\times x $$\n",
    "\n",
    "By specifying ${\\beta_0, \\beta_1}$ we can simulate outcome from this model by first generating values for $x$ and then using the values of $x$ to generate $Y$ using  calcualting y by rounding probabilities.  Each time we sample we get $(Y, X)$ and we can perform logistic regression to see whether the correct parameters are resolved. We could then change something like the model form and see what happens to the model under changes. Or we may just want to see how accurate the model is using finite a finite sample rather than asymptotics, say a sample of n = 1000 observations.\n",
    "\n",
    "More generally, the steps taken in a simulation study can be summarized:\n",
    "\n",
    "1. Specify the articifial population\n",
    "2. Sample from that population\n",
    "3. Calculate the parameter of interest\n",
    "4. repeat steps 2 and 3 a certain number of times\n",
    "5. draw conclusions.\n",
    "\n",
    "Naturally, generating data in this way requires knowing the parameters in advance in order to simulate from the model. In the case of MSMs this requires simulating from a specific MSM\n",
    "\n",
    "Explain why simulations still need to be treated critically. \\citet{Greenland1997}\n",
    "\n",
    "- advantage of no measurement error, no model misspecification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific MSM \n",
    "\n",
    "- specifying a specific MSM means postulating a model for the causal relationship between $Y$ and $X$\n",
    "\n",
    "The previous section described how data generated from a specific model can be used to assess statistical methodology and departures from key assumptions. In the case of MSM this entails specifying a specific form for the MSM from which data can be generated.\n",
    "\n",
    "For example, suppose that the chosen MSM was $P(Y\\ |\\ do(a)) = \\alpha + \\beta a$, then the observational data from which this is derived would be $Y$, $A$ and $L$. The simulation needs to allow $\\beta$ to be estimated from only that observational data. In order to simulate from a specific MSM we need to generate observational data in such a way that it conforms to a specific MSM. For instance, one specific MSM could be written as $P(Y\\ |\\ do(a)) = \\alpha + \\beta a$. However, we never see this so we need to generate data in such a way that we get this model from observational data and that we can then return to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noncollapsibility\n",
    "\n",
    "In the example of the effect of ibuprofen on headache alleviation it was reasonable to ask whether that effect differed between male and female subjects. That effect could be captured by effect measures such as the risk difference, risk ratio or odds ratio. If treatment affects the outcome in males and females differently then the effect in the male strata will be different from the effect in the female strata. Marginalizing over the two strata will lead to biased effect measure because comparisons will not be between exchangeable subjects which will propogate up to the effect measure of interest. \\linebreak\n",
    "\n",
    "On the other hand, if treatment affects males and females equally then it seems reasonable to expect that the marginal effect (after marginalizing over sex - pooling subjects) would also be equal to the male and female specific effects. In other words, if the effect in males is the same as in females then it is reasonable to think that the effect is the same in the total population, which is made up of males and females. In that case, the effect measure is collapsible because sex is unrelated to exposure to ibuprofen and pooling males and females together and collapsing over sex, correct analysis can be performed using a marginal or naive analysis. This extends to the case of many subgroups defined by, for example, age or race. Collapsibility is useful because it helps to reduce the dimensionality and computational effort which arises when many subgroups need to be taken account of in the analysis \\citet{Didelez2010}.  \\linebreak\n",
    "\n",
    "However, some effect measures are noncollapsible, in the sense that even when their strata specific effects are equal, collapsing over strata the marginal or naive effect measure does not equal the strata specific effect measures. Simpson's paradox (\\citet{}). Examples of non-collapsible effect measures include the odds ratio and rate difference. The odds ratio is estimated using logistic regression\n",
    "\n",
    "In a practical setting, the odds ratio may be used and there may be confounding and there may be noncollapsibility. In fact, the ability of the inverse probability to correctly estimate the causal effect of treatment on outcome in the presence of time dependent confounding make it possible to separate the confounding and collapsibility elements and quantfy the extend to which collapsibility affects results \\citet{Pang2016}. \\linebreak\n",
    "\n",
    "The relevance of noncollapsibility to this thesis is the need to simulate data from a known MSM model. The conditionals from which we draw data will not typically be collapsible.\n",
    "\n",
    "is the same it seems reasonable that the effect in the population would also be the same. Naturally the difference could be due to the differening effect in the population in which case confounding is present. the same we could conclude that sex made no difference and look at marginal associations instead. Reducing the number of variables required in analysis is often useful because it reduces dimensionality and computational effort allowing subgroups to be pooled together \n",
    "\n",
    "An effect measure like a rate difference, rate ratio or odds ratio is collapsible when the effect is equal in both strata and equal to marginal (over $C$) effect. Collapsibility is a useful property because it means that analysis can be carried out on a subset of variables after marginalizing over the others . In this case that could mean marginalizing over sex and looking at just the relationship between $Y$ and $X$. \\linebreak  \n",
    "\n",
    "- see greenland 1999 collapsibility regression formulation for an example in regression context.\n",
    "- mixed up with confounding and concluded that there is no confounding when the conditional odds ratio equals the conditional odds ratio.\n",
    "\n",
    "An effect measure for the association between $Y$ and $X$ such as an rate ratio or an odds ratio is noncollapsible when conditioning on a covariate a to $Y$ changes the size of the odds ratio even when . Collapsibility is useful when \n",
    "\n",
    "- Explain that the effect of interest depends on the question (also cite \\citet{Pearl2014} on this)\n",
    "\n",
    "An effect measure is non-collapsible across strata defined in the analysis if the constant effect measure does not equal the strata specific effect measure. greenland 1996\n",
    "\n",
    "example of randomized trial, there being no confounding because it is a randomized trial, same result in men and women, different result in whole population. Not confounding because we eliminate confounding by randomization.\n",
    "\n",
    "Greenland 1999 - if the model with Z is correct then the model without Z is unlikely to have the same coefficient\n",
    "\n",
    "Noncollapsibility arises when the marginal effect measure (marginal over any covariates, i.e. unstratified or with no confounder control, crude) is not equal to the strata specific effect measure.\n",
    "\n",
    "- explain why it is a probelm for marginals versus conditionals with (i.e. won;t be the same MSM)\n",
    "- use cox ph model as example.\n",
    "\n",
    "This is a problem when simulating from marginal structural models because the correct marginal structural model  \n",
    "\n",
    "$$correct marginal structural model$$\n",
    "$$model with covariates$$\n",
    "$$collapsed over covariates and not equal to marginal model$$\n",
    "\n",
    "\n",
    "\n",
    "Collapsibility starts with the notion of confounders. We assume that within strata of confounders that the effect of the confounder is homogenous. I.e. in the female strata, the effect of being female is homogenous.\n",
    "\n",
    "\\citet{Greenland1996}, \\citet{Greenland1999}, \\citet{Greenland2011}, \\citet{Sjoelander2016}\n",
    "\n",
    "The effect of treatment on diesease outcome may be unconfounded but noncollapsible\n",
    "\n",
    "Collapsibility is the same as simpson's paradox if we adopt the definition that without the conditional variable they can be equal.\n",
    "\n",
    "collapsibility depends on the measure used. Some are collapsible and some are not\n",
    "\n",
    "Could arise in two ways 1) within strata effect measures may not be the same 2) Even if they are the same they may not equal the marginal effect measure (marginal over any covariates Z)\n",
    "\n",
    "Collapsibility means there is no incompatibility between the marginal model and the conditional distributions used to simulate the data. Provide example of this. Explain how this affects the simulation algorithm. Especially hazard ratios which are non-collapsible.\n",
    "\n",
    "Models are noncollapsible when conditioning on a covariate **related to the outcome** changes the size of the estimate even when the covariate is unrelated to the exposure. Illustrate why this happens with survival models.\n",
    "\n",
    "Survival models are non-collapsible. Hence we cannot eaily simulate from them. Instead we use U as a sneaky trick. Explain why survival models are non-collapsible - through the hazard function.\n",
    "\n",
    "This is particularly important because collapsibility and confounding are often treated as identical concepts when in fact they are not. \\citet{Greenland1999}\n",
    "\n",
    "- relevance to the algorithm? How does this work with a specific MSM.\n",
    "- Show why hazard ratios are not collapsible.\n",
    "- Explain why models with product terms are clearly not collapsible\n",
    "- relation to a hazard function\n",
    "- link to lack of exchangeability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observational structure  \n",
    "\n",
    "The previous chapter included a discussion of the role of confounding covariates and time dependent confounding. In the time fixed context, blocking the path from a confounding covariate by conditioning was sufficient to consistently estimate the effect of treatment on outcome in the absence of unmeasured confounders. In the time dependent context, conditioning on confounding covariates may biases estimates in two ways. First, in a time dependent context, both treatment and covariates can change over time. Some effect of treatment may be transmitted through the covariate to the outcome or future treatment. Blocking this path also blocks the indirect effect of treatment on outcome through the confounding covariate. At the same time, selection bias arises due to conditioning on a colider. The hazard ratio, the ratio of the hazard function/rate at two levels of an explanatory variable, typically exhibits selection bias.  Failing to condition \\linebreak \n",
    "\n",
    "- why important for observational studies\n",
    "- examples of it in observational studies\n",
    "- explain why we need algorithm with this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Simulation algorithms literature review.\n",
    "\n",
    "Several algorithms for simulating data from a specific marginal structural model have been suggested in the data. Here we briefly summarise the discussion in \\citet{Havercroft2012} on competeing algorithms and then highlight a number of algorithms suggested subsequently to that paper. \n",
    "\n",
    "- briefly summarise the litrature in \\citet{Havercroft2012} \n",
    "- update with more recent algorithms like \\citet{Young2014}\n",
    "- discuss \\citet{Naimi2011} limitations of simulation algorithm. look into more detail for how this algoirthm works.\n",
    "\n",
    "Several algorithms for simulating from a specific MSM have ben suggested in the literature. \n",
    "\n",
    "- \\citet{Havercroft2012}\n",
    "- \\citet{Bryan2004}\n",
    "- \\citet{Westreich2012} not in review by \\citet{Havercroft2012}\n",
    "- \\citet{Young2014}\n",
    "\n",
    "- Bryan 2004: fixes the vector L at the beginning which means A never affects L which don't make no sense. In other words it doesn't have the observational structure we are looking for.\n",
    "- Do we introduce a form of selection bias into the data when we force positivity according to protocols? At baseline the proportion of people in any CD4 strata was unknown but randomized and hence in expectation it should be the same in treated and untreated. \n",
    "- Young 2014 - Law of the observed outcome conditional on the measured past. What this paper shows is that the regression results are not correct but the IPTW ones are fine. The issues is comparing IPTW to normal regression results. As we compare IPTW results to IPTW results under positivity it should be fine to use the Havercroft algorithm.\n",
    "\n",
    "\\citet{Havercroft2012} also make these comparisons in their paper in a few paragraphs.\n",
    "\n",
    "What we add here is how control over $L$ allows us to introduce positivity violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation algorithm\n",
    "\n",
    "The base algorithm used in this paper is drawn from \\citet{Havercroft2012}. In that paper, an algorithm for simulating data from a specific MSM is derived from the joint factorization of a DAG which exhibits time dependent confounding. The algorithm generates subject data for multiple visits. The central concepts of the algorithm can be explained using a simplifies two period version of the algorithm, see figure ?. \\linebreak\n",
    "\n",
    "- stress that we simulate from conditionals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{figure}\n",
    "\\centering\n",
    "\\begin{tikzpicture}[->,>=stealth',auto,node distance=3cm,\n",
    "  thick,main node/.style={circle,draw=none,font=\\sffamily\\Large\\bfseries}]\n",
    "\n",
    "  \\node[main node, circle, draw] (1) {$U_0$}; %unmeasured\n",
    "  \\node[main node] (2) [right of=1] {$A_0$};\n",
    "  \\node[main node] (3) [above of=2] {$L_0$};\n",
    "  \\node[main node] (4) [below of=2] {$Y_1$};\n",
    "  \\node[main node] (5) [right of=2] {$A_1$};\n",
    "  \\node[main node] (6) [above of=5] {$L_1$};\n",
    "  \\node[main node] (7) [below of=5] {$Y_2$};\n",
    "  \\node[main node, circle, draw] (8) [above of=6] {$U_1$}; %unmeasured\n",
    "\n",
    "  \\path[every node/.style={font=\\sffamily\\small}]\n",
    "    (1) edge node [right] {} (2)\n",
    "    (1) edge node [right] {} (3)\n",
    "    (1) edge node [right] {} (4)\n",
    "    (1) edge[bend right = 60] node [left] {} (7)\n",
    "    (1) edge[bend left] node [left] {} (8)\n",
    "    (2) edge node [right] {} (3)\n",
    "    (2) edge node [right] {} (4)\n",
    "    (2) edge node [right] {} (5)\n",
    "    (2) edge node [right] {} (6)\n",
    "    (2) edge node [right] {} (7)\n",
    "    (3) edge node [right] {} (6)\n",
    "    (4) edge node [right] {} (7)\n",
    "    (5) edge node [right] {} (6)\n",
    "    (5) edge node [right] {} (7)\n",
    "    (8) edge node [right] {} (6);\n",
    "\\end{tikzpicture}\n",
    "\\caption{Causal graph}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAG corresponding to their simulation algorithm is shown in figure ?. This DAG  who derive their algorithm according to a specific DAG. Here a sketch of the key points in this algorithm is presented, the full proof demonstrating that the algorithm simulates data from a specific MSM is presented in Appendix B of \\citet{Havercroft2012}.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{algorithm}[H]\n",
    "\\SetAlgoLined\n",
    "\\KwResult{Marginal Structural Model Under Time Dependent Confounding}\n",
    " \\For{i in 1, \\dots , n}{\n",
    "  $U_{0, i} \\sim U[0, 1]$\\\\\n",
    "  $\\epsilon_{0, i} \\sim N(\\mu, \\sigma^2)$\\\\\n",
    "  $L_{0, i} \\gets F^{-1}_{\\Gamma(k,\\theta)}(U_{i, 0}) + \\epsilon_{0, i}$\\\\\n",
    "  $A_{-1, i} \\gets 0$\\\\\n",
    "  $A_{0, i} \\gets Bern(expit(\\theta_0 + \\theta_2 (L_{0, i} - 500)))$\\\\\n",
    "  \\If{$A_{0, i}= 1$}{\n",
    "   $T^* \\gets 0$;\n",
    "  }\n",
    "  $\\lambda_{0, i} \\gets expit(\\gamma_0 + \\gamma_2 A_{0, i})$\\\\\n",
    "  \\eIf{$\\lambda_{0, i} \\ge U_{0, i}$}{\n",
    "   $Y_{1, i} \\gets 0$\\\\\n",
    "   }{\n",
    "   $Y_{1, i} \\gets 1$\\\\\n",
    "  }\n",
    "  \\For{k in 1, \\dots , T}{\n",
    "   \\If{$Y_{t, i} = 0$}{\n",
    "    $\\Delta_{t, i} \\sim N(\\mu_2, \\sigma^2_2)$\\\\\n",
    "    $U_{t, i} \\gets min(1, max(0, U_{t-1, i} + \\Delta_{t, i}))$\\\\\n",
    "    \\eIf{$t \\neq 0\\ (mod\\ k)$}{\n",
    "     $L_{t, i} \\gets L_{t-1, i}$\\\\\n",
    "     $A_{t, i} \\gets A_{t-1, i}$\\\\\n",
    "     }{\n",
    "     $\\epsilon_{t, i} \\sim N(100(U_{t, i}-2), \\sigma^2)$\\\\\n",
    "     $L_{t, i} \\gets max(0, L_{t-1, i} + 150A_{t-k,i}(1-A_{t-k-1,i}) + \\epsilon_{t, i})$\\\\\n",
    "     \\eIf{$A_{t-1, i} = 0$}{\n",
    "      $A_{t, i} \\sim Bern(expit(\\theta_0 + \\theta_1t + \\theta_2(L_{t, i}-500)))$\\\\\n",
    "      }{\n",
    "      $A_{t, i} \\gets 1$\\\\\n",
    "     }\n",
    "     \\If{$A_{t, i} = 1 \\and A_{t-k, i} = 0$}{\n",
    "      $T^* \\gets t$\\\\\n",
    "     }\n",
    "    }\n",
    "    $\\lambda_{t, i} \\gets expit()\\gamma_0 + \\gamma_1[(1 - A_{t, i})t + A_{t, i}T^*] + \\gamma_2 A_{t, i} + \\gamma_3 A_{t, i}(t-T^*))$\\\\\n",
    "    \\eIf{$1 - \\prod_{\\tau=0}^t(1 - \\lambda_{\\tau, i}) \\ge U_{0, i}$}{\n",
    "     $Y_{t+1, i} = 1$\\\\     \n",
    "    }{\n",
    "     $Y_{t+1, i} = 0$\\\\\n",
    "    }\n",
    "   }\n",
    "  }\n",
    " }\n",
    " \\caption{Simulation Algoirthm MSM}\n",
    "\\end{algorithm}\n",
    "\n",
    "- describe how the algorithm works, is derived and how it acheives a specific MSM, the observational structure and a specific MSM (collapsibility) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm with positivity violations.\n",
    "\n",
    "- show parts of algorithm which change \n",
    "- present full algorithm with all changes in appendix\n",
    "- interesting question is how the positivity violations are propogated through time. This can be checked by lengthening the number of visits sequentially. For example, do positivty violations with a time horizon of 5 visits have less of an effect than 10 visits.\n",
    "\n",
    "The parameter of interest could be expected survival or the five year survival probability\n",
    "\n",
    "Need to specify what the MSM is, give an example of it as a hazard function. Survival is completeley determined by the hazard function.\n",
    "\n",
    "U allows us to get any distribution we like for Y marginal over covariates, WOuld L itself allows this? probably not. We can somehow get from this the subjects counterfactual survival time.\n",
    "\n",
    "Importantly, this expression on the left hand side has unobserved counterfactuals, but the right hand side has only observed quantities which would be observed in an actual observational study.\n",
    "\n",
    "non-collapsibility is an unresolved issue here. So even if we investigate positivity we can still only do so for collapsible models?\n",
    "\n",
    "Equivalent way of motivating dividing the joint distribution by Pr(A|L) is through IPTW.\n",
    "\n",
    "1. derive relationship between MSM and DAG and the correct conditional distributions. Follows from truncated factorisation why we can et P(Y|do(a))\n",
    "\n",
    "5. think of this process as if we had fixed a treatment vector in advance. consistency assumption.\n",
    "9. HD 2012, with Pearl and truncated factorization formula, show that it is possible to link the counterfactual represented by $P(Y\\ |\\ do(a))$ to observattional data generated in an observational way. But the problem arises when the model is non-collapsible or non-linear.\n",
    "\n",
    "In the one shot case we set A = 1/0 because we are interested in the outcome under either of these treatment scenarios. In the time dependent case, A is a vector of 0s and 1s and we want to pretend that we decide in advance that the whole vector A is specified. But A and L have a complex interplay in an observational setting. So we want to pretend that A (a vector of 1s and 0s) is set in advance but at the same time have the observational structure for A and L. \n",
    "\n",
    "The relationship between Y and L is then dependent on A. There is no relationship between A and U because of the set-up in the DAG. The variable L blocks this relationship.\n",
    "\n",
    "Figure 1 represents the system under consideration. The DAG in figure 1 represents the one-shot non-longitudinal case. Factorising the joint distributions of the variables in figure 1 yields\n",
    "\n",
    "$$P(U,\\ L,\\ W,\\ A,\\ Y) = P(W)P(U)P(W)P(L\\ |\\ U)P(A\\ |\\ L,W)P(Y\\ |\\ U,A)$$\n",
    "\n",
    "Where, following definition 1.1 we delete $P(A\\ |\\ L,W)$, a probability function corresponding to $A$, and replace $A=a$ in all remaining functions\n",
    "\n",
    "$$ P(U, L, W, Y\\ |\\ do(A=a)) =\n",
    "  \\begin{cases}\n",
    "    P(U)P(L\\ |\\ U)P(Y\\ |\\ U,A = a) & \\quad \\text{if } A = a\\\\\n",
    "    0  & \\quad \\text{if } A \\neq a\\\\\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "The goal is to simulate from a particular MSM. This means parameterising $P(Y\\ |\\ do(A=a))$. Applying the law of total probability over $W$, $U$ and $L$ yields\n",
    "\n",
    "$$P(Y\\ |\\ do(A=a) = \\sum_{w, u, l} P(W)P(U)P(L\\ |\\ U)P(Y\\ |\\ U, L, A=a) = \\sum_{u, l} P(U)P(L\\ |\\ U)P(Y\\ |\\ U, L, A=a)$$\n",
    "\n",
    "Making use of the fact that $P(L, U) = P(L\\ |\\  U)P(U) = P(U\\ |\\ L)P(L)$ and summing over either W and U or W and L yields\n",
    "\n",
    "$$P(Y\\ |\\ do(A=a) = \\sum_{l}P(Y\\ |\\ L, A=a)P(L) = \\sum_{u} P(Y\\ |\\ U, A=a)P(U))$$\n",
    "\n",
    "If we can find suitable forms for either $P(Y\\ |\\ L, A=a)$ and $P(L)$ or $P(Y\\ |\\ U, A=a)$ and $P(U)$ that correspond to the MSM $P(Y\\ |\\ do(A=a)$, then, given suitable values for $A, L, U$ it will be possible to simulate from the chosen MSM.\n",
    "\n",
    "Choosing a functional form for $$P(Y\\ |\\ do(A=a)$$ depends on convenience. We need a functional form that can be easily represented by $P(Y\\ |\\ L, A=a)P(L)$. non-linear functions will be hard to work into the analysis.\n",
    "\n",
    "U ~ U[0, 1] is a good choice because we can usethe CDF of Y because U[0, 1] is always between 0 and 1\n",
    "\n",
    "General health is patient specific but comes from a clear distribution and has a nice medical interpretation. In contrast L would be more difficult to include. It is better as a function of U than a value in of itself.\n",
    "\n",
    "- Explain issue that survival models are not collapsible which is why most algorithms don't work. Big reason we choose HD2012 is because of this\n",
    "- no model mispecification in the HD2012 algorithm\n",
    "- stay on treatment after treatment starts\n",
    "- they motivate a logistic model for the haxard function, they use a discrete equivalent to the hazrd function (link to citation about farington study.)\n",
    "- treatment regime is determined by t* (starting point of treatment because it is a vector of {0, 0, 0, 1, 1, 1}\n",
    "- Explain why we can introduce positivity in this algorithm but not in others as easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violations of Positivity\n",
    "\n",
    "The motivation for using the algorith of \\citet{Havercroft2012} is that we have control over how $L$ affects $Y$ whereas in other algorithms $L$ would be fixed in advance., so we can itroduce positivity using a threshold. In other algorithms there would be a direct link between $L$ and $Y$, this would be a problem because altering treatment decisions based on $L$ would affect $Y$ directly.  \n",
    "- creating an artificial population in which positivity is violated in specific ways.\n",
    "\n",
    "## Extended discussion of algorithm linking to positivity\n",
    "\n",
    "As described in the introduction, one assumption of the model is that there is a non-zero probability of the event occuring at every startum of the covariate.\n",
    "\n",
    "- When previous covariates like CD4 count are strongly associated with treatment the probabilities in the denominator of the ustabilized weights may vary greatly. Because we are foricing positivity by using a treatment rule when L falls below a threshold and A is then eaual to one, we create a strong association between A and L -> hence the unstabilized weights would vary. (Robins et al 2000 pp. 553)\n",
    "- present the algorithm again with positivity violations.\n",
    "\n",
    "## Simulation scenarios\n",
    "\n",
    "#### thresholding\n",
    "\n",
    "#### percentage of compliant doctors. \n",
    "\n",
    "#### propogation through time, longer time periods\n",
    " \n",
    "- table of weights mean, min, max at T = 5, 10, 15 etc. plot with bias against time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation study\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "Include an example simulation graph plot showing the data colored to show where positivity would arise.\n",
    "\n",
    "## Number of positivity compliant doctors.\n",
    "\n",
    "## Varying levels of threshold.\n",
    "\n",
    "We wish to simulate survival data in discrete time $t = 0, \\dots, T$ for $n$ subjects. At baseline $t=0$ all subjects are assumed to be at risk of failure so that $Y_0 = 0$. For each time period $t = 0, \\dots, T$ a subject may either be on treatment,  $A_t = 1$, or not on treatment, $A_t = 0$. All patients are assumed to be not on treatment before the study begins. Once a patient commences treatment, they remain on treatment in all subsequent periods until failure or the end of follow-up. In each time period $L_t$ is the value of a covariate measured at time $t$. In the simulated data, $L_t$ behaves in a similar manner to CD4 counts such that a low value of $L_t$ represents a more severe illness and hence a higher probability of both tratemnt and failure in the following period. In addition to $L_t$, the variable $U_t$ represents subject specific general health at time $t$.\n",
    "\n",
    "Each time period is either a check up visit or is between two check up visits. If $t$ is a check-up visit and treatment has not yet commenced, $L_t$ is measured and a decision is made on whether to commence treatment. Between visits, treatment remains unchanged at the value recorded at the previous visit. Similarly, $L_t$ which is only measured when $t$ is a visit, alos remains unchanged.\n",
    "\n",
    "We represent the history of a random variable with an over bar. For example, the vector representing the treatment history of the variable A is represented by $\\bar A = [a_0, a_1, \\dots, a_m]$ where $m=T$ if the subject survives until the end of follow-up, or $m < T$ otherwise. Prior to basline both $A = 0$ for all subjects.\n",
    "\n",
    "- explain what $U$ is and how it relates to the simulation design/algorithm\n",
    "- Be more specific on $Y$\n",
    "- L_t is a measured confounder\n",
    "- U_t is an unmeasured confounder.\n",
    "\n",
    "## Simulation Algorithm\n",
    "\n",
    "### Algorithm \n",
    "\n",
    "Next, we describe the algorithm used to simulate data from our chosen marginal structural model under time dependent confounding. In the following section we discuss in detail how the algorithm works and the salient features for this thesis. The algorithm is taken from \\citet{Havercroft2012} who generate data on $n$ patients, for $k$ time periods. The outer loop in the following algorithm $i \\in {1, \\dots, n}$ , refers to the patients while the inner loop $t \\in {1, \\dots, T}$ refers to the subsject specific time periods from baseline to failure or the end of the study. There will be at least one, and at most $T$ records for each patient.\n",
    "\n",
    "Within the inner loop ($t \\in {1, \\dots, T}$) we see that the data is only updated at time $t \\neq 0\\ (mod\\ k)$, where $k$ refers to evenly spaced check-up visits. If $t$ is not a check-up visit the values of $A_t$ and $L_t$ are the same as in $t-1$. When $t$ is a visit $A_t$ and $L_t$ are updated.\n",
    "\n",
    "- if treatment has been commenced then a subject may feel extra benefit if more time has elapsed since treatment began\n",
    "- L_t affects A_t and also Y_t\n",
    "- explain starting values for A and Y are all zero (except L maybe)\n",
    "\n",
    "In order to operationalize the Algorithm 1 we need to choose parameters for $()$. In their paper \\citet{Havercroft2012} use values that simulate data with a close resemblance to the Swiss HIV Cohort Study. We postpone disussion of the patameters in Algorithm 1 to section 2.4. We just need to state that we follow their parameters because this is not the focus of this thesis.\t\n",
    "\n",
    "### Discussion of how algorithm works \n",
    "\n",
    "The algorithm of \\citet{Havercroft2012} works by factorizing the joint density of the histories of the four variables in the analysis.  \n",
    "\n",
    "- Important is that the form of the MSM is not specified intil the last stage\n",
    "- role of $U_{0, i}$\n",
    "- How does positivity enter the analysis?\n",
    "- Why this model is important in terms of positivity.\n",
    "\n",
    "\n",
    "## Constructing IPT weights\n",
    "\n",
    "Inverse Probability of Treatment weights can be used to adjust for measured confounding and selection bias in marginal structural models. Link back to pseudo population idea in previous section. This method relies on four assumptions consistency, exchangeability, positivity and no mispecification of the model used to estimate the weights \\citet{Cole2008}. Unstabilized weights are defined as:\n",
    "\n",
    "$$w_{t,i} = \\frac{1}{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "Where the denominator is the probability that the subject received the particular treatment history that they were observed to receive up to time $t$, given their prior observed treatment and covariate histories (Havercroft, Didelez, 2012). The probabilities $p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})$ may vary greatly between subjects when the covariate history is strongly asscoaited with treatment. In terms of the resulting pseudopopulation, very small values of the unstabilized weights for some subjects would result in a small number of observations dominating the weighted analysis. The result is that the IPTW estimator of the coefficients will have a large variance, and will fail to be normally distributed. This variability can be mitigated by using the following stabilized weights \n",
    "\n",
    "$$sw_{it} = \\frac{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i})} {\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "In the case that there is no confounding the denominator probabiliies in the stabilized weights reduce to $p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i})$ and $sw_{it}=1$ so that each subject contributes the same weight. In the case of confounding this will not be the case and the stabilized weight will vary around 1. \n",
    "\n",
    "In practice, we estimate the weights from the data using a pooled logistic model for the numerator and denominator probabilities. The histories of the treatment and covariates are included in the probabilities. In practice Specifically, following Havercroft and Didelez (2012), we estimate the model where the visit is only the visits every check up time. Between check ups both the treatment and covariate remain the same. Other ways of doing this include a spline function over the months to create a smooth function between the visits. Another difference might be to use a coxph function instead of logistic function\n",
    "\n",
    "$$logit\\ p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i}) = \\alpha_0 + \\alpha_1 k + \\alpha_2 a_{k-1} + \\dots + \\alpha_k a_0 + $$\n",
    "\n",
    "We have several options for estimating these weights. We could use a coxph model, or a logistic model.\n",
    "\n",
    "\n",
    "## Simulation Set-up\n",
    "\n",
    "We follow the simulation set-up of Havercroft, Didelez (2012) which is based on parameters that closely match the Swiss HIV Cohort Study (HAART). \n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "- check the distribution of the weights that come out of the model (see Cole 2008). This would allow us to see weight model mispecifications. Not a problem in the simuation case.\n",
    "- compare the bias, se, MSE, and 95% confidence interval\n",
    "- compare all of these in the positivity violation and non-positivty violation case.\n",
    "\n",
    "- explain to some extent monte-carlo standard error.\n",
    "- we don't confirm the results of the havercroft of Bryan papers, instead refer readers to these papers to see how IPTW outperforms the naive estimators.\n",
    "\n",
    "\n",
    "- Explain why we use MSE or other measures to assess simulation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Conclusion\n",
    "\n",
    "The focus of this thesis was the effect of positivity violations on \n",
    "\n",
    "## Limitations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
