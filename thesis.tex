
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    

    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    
\usepackage[round]{natbib}
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usepackage{algorithmic}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}


    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    
\title{Inverse Probability of Treatment Weighting Under Violations of Positivity}

    
    
\author{Tomas D. Morley}

    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \newpage

    \pagenumbering{roman} \textbf{Abstract}

    \newpage

    \textbf{Aknowledgements}

    \newpage

\tableofcontents{}

    \newpage

\pagenumbering{arabic} \# Introduction

\subsection{paragraph 1 - centrality of studying positivty to
inference.}\label{paragraph-1---centrality-of-studying-positivty-to-inference.}

\begin{itemize}
\tightlist
\item
  causality as central interest
\item
  intuitive definition of positivty
\end{itemize}

Does a drug reduce mortality? Does a headache reliever realy relieve
headaches? Does fertilizer make crops and does smoking really cause lung
cancer. Inherent in all of these questions is a causal comparison
between the group which smokes and the group which does not, the

Inuitively, comparisons . It is only a small leap to see that making
comparisons between treated and utreated within each strata. For
example, people with and without ibuprofen within gene types.

\begin{itemize}
\tightlist
\item
  formal(ish) definition of positivity
\item
  importance of studying positivty
\end{itemize}

The positivity assumption is essential for the study of causal infrence

Despite its central importance to inference, a systematic study of
positivty violations has received relatively little attention in the
literature, notable exceptions being , , ,

Marginal structural models are a class of models for the estimation of
causal effects from observational data in the presence of time dependent
confounding. Underlying this class of models are five assumptions:
exchangeability, consistency, no model mispecification, no measurement
error and positivty. The last of these, positivty, is violated w

In a protocol study, below a certain strata there may not exist any
patients who receive treatment

\begin{itemize}
\item
  set up problem and why we need to study positivty - give this
  motivation before anything else
\item
  exhchangeability/confounder control vs positivity
\item
  thresholding/positivty compliant doctors.
\item
  in a survival context, need to choose simulation algorithm carefull
  because survival models are noncollapsible.
\item
  G-formula can be used to circumvent nonpositivity, explain why we need
  MSMs anyway. (Robins 1987, Cole 2013)
\item
  look at pros and cons from Daniel paper to justify why we will still
  want to study MSMs
\item
  exchangeability, positivty and loss of efficiency due to over
  stratification. (something for conclusion, that we do not study? Or
  how do we study coverage in this case if
\end{itemize}

\subsection{exchangeability and
positivity}\label{exchangeability-and-positivity}

\subsection{thresholding, positivity compliant doctors and multiple
visits. Increasing number of confounders - allowing more opportunity for
positivity.}\label{thresholding-positivity-compliant-doctors-and-multiple-visits.-increasing-number-of-confounders---allowing-more-opportunity-for-positivity.}

Marginal structural models (MSMs) are a popular class of models for
performing causal inference in the presence of time dependent
confounders. These models have an important application in areas of
research such as epidemiology, social sciences and economics where
randomised trials are prohibited by ethical or financial considerations,
and hence confounding cannot be ruled out by randomization. Under these
circumstances confounding can obscure the causal effect of treatment on
outcome. An example of this, common in epidemiological studies, occurs
when prognostic variables inform treatment decisions while at same time
being predictors of the outcome of interest. In a longitudinal setting
this is further complicated when the confounder itself is determined by
earlier treatment. One consequence is that regression adjustment methods
do not control for confounding in the longitudinal case and other
techniques are required. \linebreak

The Inverse probability of treatment weighting (IPTW) estimator is a
technique which leads to consistent estimates in the presence of has
been applied to censoring, missing data and survey design problems. The
central idea is that by weighting the observed data in order to create a
pseudo population is constructed in which treatemnt is assigned at
random. Subsequent analysis where we ignore the confounder is then
possible. which inference on the target population can be achieved. For
example, when there is missing data weights can be used to create a
pseudo-population in which there is no missingness. In the context of
MSMs, the IPT weights relate to a pseudo-population in which there is no
longer any confounding between the confounder and treatment and causal
inferences can be made. \linebreak

Underlying the IPTW method for estimating MSMs are four assumptions: 1)
consistency 2) exchangeability 3) positivity 4) and correct model
specification. Exchangeability, also known as the no unmeasured
confounding assumption, is closely linked to causality?? Several studies
have considered violations of exchangeability and corrected model
specification. Positivity has received less attention because in typical
observational study positivity violations are not suspected explain why.
In the clinical context that we consider, protocols (give some examples,
like Platt 2012) threaten to violate the positivity assumption and we
investigate whether MSMs are robust against positivity. The focus of
this thesis will be on violations of the positivity assumption.
Positivity means that within every strata spanned by the confounders,
there must be a positive probability of patients being exposed or
unexposed to treatment. For example, in a medical context, if treatment
protocols demand that treatment is initiated whenever a prognostic
variable falls below a pre-defined threshold, there will only be exposed
and no unexposed patients in this strata of the confounding prognostic
variable. make decisions based on protocols positivity can be. In the
absence of structural positivity violations, there is always the threat
that random zeroes arise in some strata of the confounder especially
when the sample size is small or the number of confounding variables is
large. In each case the sparsity of data within the strata of the
confounder results in a high chance that positivity is violated.
Positivity violations increase the bias and variance of estimates of the
causal effect but the extent of the damage is not well known. The
central aim of this thesis will be to investigate positivity violations
when fitting MSMs to longitudinal data. To our knowledge positivity
violations have not been systematically studied in the literature from a
simulation point of view. We quantify the bias and variance introduced
due to positivity violations and hope to provide practical advice to
researchers tempted to fit MSMs to overcome confounding without
realising the potential consequences of positivity violations in their
data. \linebreak

Throughout this thesis we focus on clinical applications as examples. In
the literature on marginal structural models the causal effect of
Zidovudine on the survival of HIV positive men is often cited as an
example. In this example a patients white blood cell (CD4) count is a
prognostic variable that influences a doctor's decision to initiate
treatment while at the same time being a predictor of survival. As a
result CD4 count is a confounder. In the longitudinal setting previous
treatments influence CD4 count. As such studies often depend on
protocols which means that poistivity in some levels of the confounder
make this a suitable example for our purposes. \linebreak 

The structure of this thesis is as follows. In section 2 of part 1, the
model considered in this thesis and its important aspects are explained.
In part 2 simulating from this statistical model is discussed in detail.
In part 3 the model under dynamic strategies is considered and
comparisons are drawn with the static case. In part 4 we entertain
violations of positivity in the data, this section represents the
novelty in this thesis. Part 5 conducts a simulation study. Part 6
includes a discussion, conclusions and suggestions for future work.
\linebreak

A second consequence is that simulating data from a specific marginal
structural models is more challenging when the data is to exhibit time
dependent confounding.

Look through literature for applications of MSMs

\begin{itemize}
\tightlist
\item
  Greenland 1999 pp35 on protocols and how they can cause nonpositivity
  - motivation for this thesis.
\end{itemize}

    Allow for the joint determination of outcomes and treatment status or
omitted variables related to both treatment status and outcomes (Angrist
2001).

A covariate \(L\) is a confounder if it predicts the event of interest
and also predicts subsequent exposure. Explain how this actually
happens, as U0 is a common ancestor of A through L and also Y, also that
there is selection bias, and L is sufficient to adjust for confounding
see Havercroft algorithm code page bottom.

    \subsection{Outline}\label{outline}

In this thesis we assess the impact of violations of the positivity
assumption on the performance of marginal structural models. \linebreak

The first chapter introduces marginal structural models and inverse
probability of treatment weighting. Particular attention is given to the
role of the positivity assumption in MSMs and the trade-off between
finer confounding control and positivity. Chapter 2 explains the issues
surrounding simulating data from a specific MSM with a longitudinal
structure that captures the issues which arise in time dependent
confounding. This chapter includes a literature review of algorithms
that have been developed to simulate from a given marginal structural
model. A particular simulation algorithm which is versatile enough that
it can be used to introduce violations of positivity is then selected
and explained in more detail. Chapter 3 presents simulation results and
key findings. Chapter 4 uses real world data in which positivity
violations arise as a result of treatment protocols in a chemotherapy
trial. The last chapter concludes and provides limitations and
directions for future work.

    \subsection{Software}\label{software}

All simulations and analysis carried out in this thesis use the Python
programming language and are provided with this thesis. Several modules
were used to extend the base Python language and these are highlighted
in the code where appropriate. The \emph{survey} and \emph{ipw} packages
written in the R programming language were used to provide functionality
not currently available as a Python module. These packages are freely
avilable through the Comprehensive R Archive Network (CRAN). Combining R
functionality in Python code is made possible through the \emph{rpy2}
Python module. \linebreak

All functions used for this thesis are provided in appendices. Appendix
? contains the code for generating data from the chosen marginal
structural model and performing monte carlo simulations. Appendix ?
contains the code used to generate the results and graphs in this
thesis.

    \newpage

    \section{Marginal structural models}\label{marginal-structural-models}

Marginal structural models (MSMs) are one of several techniques
developed to estimate causal effects from observational data where time
dependent confounding precludes the use of standard approaches. Other
techniques that can also be used to estimate causal effects from
observational data include the g-computation formula and g-estimation of
structural nested models \citet{Daniel2013}.

A MSM is a model for some aspect of the distribution of counterfactual
outcomes, such as the mean. In this chapter we introduce several
concepts behind MSMs including the counterfactual framework for causal
inference, confounding and time dependent confounding. The parameters of
a MSM can be consistenly estimated using a class of estimators called
the inverse probability of treatment weighted estimators. Marginal
structural

This estimator allows for adjustment for time dependent confounding in
complex longitudinal settings where time dependent confounders are
predictors of future outcomes and are themselves affected by previous
treatment \citet{}

A necessary condition for estimating marginal structural models, and
causal inference more generally, is the positivity assumption. This
thesis focuses on violations of the positivity or experimental treatment
assumption, a necessary assumption for causal inference, in marginal
structural models.

Marginal structural models (MSMs) are a class of models for the
estimation of causal effects from observational data \citet{Robins2000}.
Several techniques This chapter begins by briefly describing the
counterfactual and potential outcomes framework which defines causal
effects. This is follwed by an explanation of confounding and directed
acyclic graphs (DAGs). MSMs can resolve the correct causal parameters in
a complex time dependent setting. Time dependent confounding is first
explained followed by a description of the inverse probability of
weighting (IPTW) technique which is used to account for time dependent
confounding.

Marginal structural models are valid under five assumptions. These are
the no unmeasured confounding, consistency, no model mispecification, no
measurement error and positivity assumptions. Each of these is dealt
with in turn and special emphasis is placed on positivity which is the
central topic of this thesis.

\begin{itemize}
\tightlist
\item
  MSMs are models for some aspect (like the mean) of the distribution of
  counterfactuals.
\item
  Marginal structural models use only observed data and a set of
  assumptions to investigate causal effects.
\item
  explain the name (marginal over covariates, structural in sense of
  causal)
\end{itemize}

    \subsection{Counterfactuals and
causality}\label{counterfactuals-and-causality}

In the counterfactual or potential outcomes framework
(\citet{Neyman1923}, \citet{Rubin1978}, \citet{Robins1986}) the causal
effect of treatment \(X\) on outcome \(Y\) is defined as the contrast
between the potential outcome when exposed to \(X\) and the potential
outcome when left unexposed to \(X\) for the same subject. For a binary
treatment, the subject's outcome when exposed is denoted by
\(Y_{x=1,i} = Y_{1i}\) and as \(Y_{x=0,i} = Y_{0i}\) when left
unexposed. The causal contrast for a single subject in this framework is
\(Y_{1i} - Y_{0i}\). As no subject can be both exposed and unexposed
either \(Y_{1i}\) or \(Y_{0i}\) is counterfactual and therefore
unobserved. For example, if the patient receives treatment, \(Y_{1, i}\)
is observed and \$Y\_\{0, i\} is the outcome that would have occured if
the patient had not received treatment.\linebreak

The canonical example in the literature on MSMs is the effect of
Zidovudine (AZT) on mortality amongst HIV positive subjects
\citet{Robins2000}. In this example, \(X\) is a binary variable equal to
one if the subject received AZT and zero otherwise. The outcome \(Y\) is
a binary variable equal to one if the subject is dead and zero if the
subject is alive, after a suitable time period. Before treatment is
assigned, there are four potential outcomes which could occur for a
given subject and these are enumerated in table 1.

\begin{longtable}[]{@{}llll@{}}
\toprule
X & Y & \(Y_{0}\) & \(Y_{1}\)\tabularnewline
\midrule
\endhead
0 & 0 & 0 & ?\tabularnewline
0 & 1 & 1 & ?\tabularnewline
1 & 0 & ? & 0\tabularnewline
1 & 1 & ? & 1\tabularnewline
\bottomrule
\end{longtable}

Once treatment has been decided one potential outcome is observed. In
the first row the outcome when the subject does not receive treatment is
\(Y = 0\) and so the potential outcome \(Y_{0}\) is observed. The
counterfactual outcome \(Y_{1}\) is not observed. The fact that only one
potential outcome is ever observed per subject has led several authors
to cast causal problems in terms of missing data problems where the
counterfactual outcome is viewed as missing (\citet{Ding2017},
\citet{Howe2015}, \citet{Edwards2015}). \linebreak

Often interest lies in the average causal effect for a population rather
than for one subject. The average outcome across all subjects who are
exposed to treatment is \(\mathbb{E}(Y_{1i})\). The equivalent average
causal comparison in the counterfactual framework is

\begin{equation}
\mathbb{E}(Y_{1i}\ |\ X = 1) - \mathbb{E}(Y_{0i}\ |\ X = 1)
\end{equation}

Or in other words, the causal contrast of interest is the difference
between the average outcome among the treated and the average outcome
among the treated had they, contrary to fact, remained untreated. When
\(Y\) is a dichotomous variable,
\(\mathbb{E}(Y_{0i}) = \mathbb{P}(Y_{xi})\), and the average causal
contrast is:

\begin{equation}
\mathbb{P}(Y_{1i}\ |\ X = 1) - \mathbb{P}(Y_{0i}\ |\ X = 1)
\end{equation}

Just as in the case of one subject the average counterfactual outcome is
not observed for those subjects who actually received treatment.

Among patients who received treatment \(Y_0\) is the outcome that would
have occured, had the patient received \(x=0\).

    \subsection{Confounding}\label{confounding}

As the counterfactual is never observed subjects who are exposed to
treatment are typically compared to those who are left unexposed. Among
the exposed, the contrast that would ideally be made is (2). Instead the
average outcome in the treated is typically compared to the average
outcome in the untreated.

\begin{equation}
\mathbb{P}(Y_{1i}\ |\ X = 1) - \mathbb{P}(Y_{0i}\ |\ X = 0)
\end{equation}

This carries a causal interpretation as long as
\(\mathbb{P}(Y_{0i}\ |\ X = 1) = \mathbb{P}(Y_{0i}\ |\ X = 0)\). In
other words, if unexposed subjects can be viewed as analogues of the
exposed subjects had they, contrary to fact, not been exposed. This
property is referred to as exchangeability and will be discussed in more
formal detail in subsequent sections\linebreak

On the other hand, if
\(\mathbb{P}(Y_{0i}\ |\ X = 1) \neq \mathbb{P}(Y_{0i}\ |\ X = 0)\) then
(3), a measure of association, is confounded for (2), a measure of
causal effect (\citet{Greenland1999}). One explanation for confounding
is the existence of confounders, covariates which are common causes of
both \(X\) and \(Y\). For example, in protocol doctor's base their
decision to initiate AZT treatment on a subject's white blood cell count
(WBC). But, WBC is also a risk factor for the outcome because subect's
with low WBC are at a higher risk of experiencing the event. Suppose
that \(Z\) is a binary variable equal to 1 if a subject's WBC count
falls below some threshold, say 200, at which point treatment is
initiated, and zero otherwise. Because treatment and death have a common
cause through \(Z\) there will be a spurious correlation and therefore
dependence between \(X\) and \(Y\). If a causal effect between \(X\) and
\(Y\) exists it will be mixed with this spurious association. Even when
no causal effect between \(X\) and \(Y\) exists we can still expect to
see an association between \(X\) and \(Y\) as a result of their common
cause \(Z\).

Randomization of exposure to subjects in randomized trials ensures that
exposure is not related to other variables like WBC. As a result,
confounding is absent in expectation or, more formally, the degree of
confounding has expectation zero and converges to zero under the
randomization distribution \citet{Greenland1999}.

In comparison, observational studies, or studies where violations of
study protocols or loss to follw up occur, often exhibit confounding.
Adjustment refers to ways in which the dependence of \(Y\) on \(X\) can
be adjusted to take account of the relationship of confounding variables
with \(X\) and \(Y\). The underlying intuition for adjustment is most
readily seen by considering contrasts within levels of a binary
confounder. Stated simply, the effect of \(X\) on \(Y\) is a
subpopulation cannot be due to a variable \(Z\) if all members of that
subpopulation share the same level of \(Z\). The effect of \(X\) on
\(Y\) cannot be confounded for sex if all members of the subpopulation
are female.

Inyuitively, adjustments come at a cost. On the one hand, comparisons
are made in smaller groups and averaged leading to a loss of efficiency.
More pertinent to this thesis, contrasts with levels of confounders
requires that there actually be both exposed and unexposed subjects
within every level of the confounders. This may not be a problem in the
case of one binary confounder but refined levels of confounders ...

Contrasts between outcomes do not c As a result, naive analysis As a
result of confounding variables, naive contrasts between exposed and
unexposed subjects will not carry a causal interpretation. Instead, the
relationship between \(X\) and \(Y\) can be examined within levels of
\(Z\). For example, the contrast in outcome amongst exposed and
unexposed subjects when \(z = 1\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Explain intuition of making comparisons within levels of confounders.
  Use a dichotomized version of CD4 count \textless{} 200 and
  \textgreater{}=200
\item
  Get positivity in somehow
\end{enumerate}

As \(Y_0\) is never observed for the exposed subjects it is typically
evaluated from subjects who did not receive treatment. If group A
contains subjects who receive treatment then we would ideally compare
the quantity \(\mathbb{P}(Y_{x = 1}|X = 1) = \mu_{A_{x = 1}}\) with the
quantity \(\mathbb{P}(Y_{x = 0}|X = 1) = \mu_{A_{x = 0}}\). Instead, we
can compute the observabed quantity
\(\mathbb{P}(Y_{x = 0}|X = 0) = \mu_{B_{x=0}}\) from group B, the
subjects who were left unexposed. Replacing the contrast
\(\mu_{A_{x = 1}} - \mu_{A_{x = 0}}\) with the contrast
\(\mu_{A_{x = 1}} - \mu_{B_{x=0}}\) will have a causal interpretation if
\(\mu_{A_{x = 0}} = \mu_{B_{x=0}}\). In other words, if subjects from
group B can be viewed as an analogues of subjects from group A had they,
contrary to fact, not received ibuprofen. \linebreak

On the other hand, if \(\mu_{A_{x = 1}} \neq \mu_{B_{x=0}}\) One
explanation for confounding is the existence of confounding covariates
which affect both treatment and outcome. For example, if AZT is more
often prescribed to subjects with low CD4 counts then unexposed subjects
are not analogues of exposed subjects because they will tend to. The
contrast in outcomes between exposed and unexposed subjects may be due
to treatment, CD4 count or a mixture of the two. The existence of a
confounding covariate prevents naive contrasts between
\(\mu_{A_{x = 1}}\) and \(\mu_{B_{x=0}}\). The counterfactual values are
therefore not missing by chance. From the perspetive of missing values,
the counterfactual values are not missing by chance.

Randomly assigning treatment ensures that treatment is not related to
any other variables. - Explain that in a randomized experiment
exchangeability is guaranteed because \(X\) is automatically not related
to any other variables through randomization so that missing values
occur by chance, or missing at random. So the counterfactual values that
we don't see for some observations are missing randomly and not due to
confounding through a covariate. In randomization, confounding is absent
in expectation

For example, if a certain geotype \(G\) the subjects in group A turn out
to be male and all those in group B turn out to be female it would be
reasonable to ask whether their sex influenced their decision to take
ibuprofen. Suppose that males also tend to have headaches of a shorter
duration so that at the end of one hour they are less likely to have a
headache than females. The result is that both the decision to take
ibuprofen and the probability of having a headache at the end of one
hour are dependent on the sex of the subject. This obscures the causal
effect of ibuprofen on headaches because there is a spurious association
between \(X\) and \(Y\) through the subject's sex. We cannot establish
whether the outcome is due to a causal relationship between ibuprofen
and headache alleviation, a relationship between sex and headache
alleviation or a mixture of the two. \linebreak

One explanation for why confounding exists is that In other words, a
confounding covariate limits our ability to draw a causal comparison.
One explanation for confounding is .... missing covariates (refer to
greenland paper where confounding variables are one reason why there is
confounding.)

Closely related to confounding, exchangeability is the assumption that
the distribution of the counterfactual outcomes \(Y_{x}\) is independent
of the actually observed treatment \(X\). When exchangeability holds,
subjects from group A and group B are exchangeable in the sense that
were they all to remain untreated the distribution of the counterfactual
outcomes \(Y_{x}\) would be the same in the two groups
\citet{Daniel2013}. Imagine exchanging a subject from group A with a
subject from group B where both recieve the treatment prevailing in
their new group. Under exchangeability, the average outcome in the two
groups is unchanged \citet{HernanMA2018}. However, exchanging subjects
between group A and group B introduces females into group A and males
into group B. As males have a higher probability that \(Y = 0\),
exchanging subjects changes the distribution of the counterfactuals. The
relationship between confounding and exchangeability is why the
assumption of exchangeability is also called the assumption of "no
unmeasured confounding". \linebreak

\begin{itemize}
\tightlist
\item
  set-up why comparisons are possible within strata and then averages
  across strata and why this means that naive methods for addresing time
  fixed confounding work
\item
  Hint that the finer the confounding control the more accurate the
  analysis but this has consequences for positivty and efficiency. On
  the one hand it is well known that finer covariate control leads to
  less efficiency? On the other hand it is less well known how this
  affects positivity.
\item
  from pearl 2001: namely, that if we compare treated vs. untreated
  subjects having the same values of the selected factors, we get the
  correct treatment effect in that subpopulation of subjects. The
  inuition is simple, the effect of X on Y cannot be due to sex if all
  subjects are female.
\item
  Also explain why we want to be judicious in our choice of number
  confounders to control for. Can't include everything (Dawid 1979 on
  this)
\item
  Explain that structural parameters only coincide with associational
  parameters under exchangeability.
\item
  Define what the naive analysis is - analysis without adjustment.
\item
  knowing the value of Z gives us no more information about the
  distribution of the counterfactuals \(Y_x\)
\item
  hernan 2011 "we say that positivity does not hold because for some
  confounder values there are no treated and untreated subjects to be
  compared"
\item
  link to splines as a way of reducing residual confounding see cole
  2008
\item
  By conditioning on a variable (or a set of variables) C we will mean
  examining relations within levels of C (i.e. within strata defined by
  single values of C) (see Greenland 2011)
\item
  conditioning and adjustment not the same thing. Control is used as a
  synonym for adjustment.
\item
  explain how we standardize measures across strata but weighted
  combination and link to standardization itself
\item
  use conditional exchangeability to show why we can standardize across
  populations
\item
  Also allows a way in to positivity by looking at Technical Point 3.1
  in hernan robins causality book.
\item
  explain residual confounding so that it can be reffered to later when
  estimating weights - for example explaining why splines help to remove
  any residual confounding.
\item
  dichotomize Z and then link to residual confounding and bescher paper.
\end{itemize}

    \subsection{Directed Acyclic Graphs: graphical representations of
causality}\label{directed-acyclic-graphs-graphical-representations-of-causality}

Causal relationships, like those described in the previous section, can
be represented using graphs. A graph consists of a finite set of
vertices \(\nu\) and a set of edges \(\epsilon\). The vertices of a
graph correspond to a collection of random variables which follow a
joint probability distribution \(P(\nu)\). Edges in \(\epsilon\) consist
of pairs of distinct vertices and denote a certain relationship that
holds between the variables \citet{Pearl2009}. The absence of an edge
between two variables indicates that the variables are independent of
one another and there is therefore no direct effect of the one variable
on the other \citet{Cole2009}. The direction of the causal relationship
is denoted by an arrow and is acyclic because causal relationships
between two variables only proceed in one direction. There are no
feedback loops or mutual causation because in a causal framework a
variable cannot be a cause of itself either directly or indirectly
\citet{Hernan2004}.\linebreak

For example, figure 1 (left) represents a causal relationship between
treatment \(X\) and outcome \(Y\). Continuing the AZT example with a
dichotomized confounder \(Z\), there is a third variable which is a
common cause of both \(X\) and \(Y\) \citet{Hernan2004}. Treatment is
assigned according to conditional distributions \(P(X\ |\ Z = z_1)\) and
\(P(X\ |\ Z = z_2)\). Once treatment has been assigned, the outcome
\(Y\) is determined by both \(X\) and \(Z\) by the conditional
distribution \(P(Y\ |\ X, Z)\). \linebreak 

The graph captures both the spurious correlation between \(X\) and \(Y\)
through the "back door" \(X <- Z -> Y\) as well as a graphical
representation of adjustment thrpugh conditioning on \(Z\) Because
treatment decisions are made on the basis of \(Z\) which is also a risk
factor for death, there is a spurious relationship between \(X\) and
\(Y\). When \(z = 1\) is low, treatment is more likely - point here is
that the causal structure of common causes can be seen in graphs. When
\(z = 1\) CD4 count is below the threshold and treatment is initiated.
But, a low CD4 count is also a risk factor for death. Thus, even when
there is no edge between \(X\) and \(Y\) as in the right hand pane of
figure 1, there remains an association between \(X\) and \(Y\) through
\(Z\) because patients with low CD4 counts are at higher risk of
receiving treatment and death. In other words, there is a spurious
marginal dependence between \(X\) and \(Y\) through \(Z\). Adjusting for
\(Z\) by conditioning is represented graphically in figure 1 by blocking
the "back door" path between \(X\) and \(Y\)
\citet{Pearl2009, Pearl2001}.

DAG's also encode the factorization of the joint densities of the
variables in the graph. In the graph above the joint density
\(p(y, x, z)\) can be factorized as

\[p(y, x, z) = p(y\ |\ x, z)p(x\ |\ z)p(z)\]

Whereas the DAG on the

\begin{itemize}
\tightlist
\item
  Didelez 2010 and Pearl 2009 for connecting DAGs to probability
  distributions and factorizations.
\item
  explain how colliders block relationships so that variables are
  independent if there is a collider on their path
\end{itemize}

    \begin{figure}
\begin{tikzpicture}

% nodes %
\node[text centered] (l0) {$L_0$};
\node[below = 3 of l0, text centered] (a0) {$A_0$};
\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};

% edges %
\draw[->, line width= 1] (a0) --  (y);
\draw[->, line width= 1] (l0) --  (a0);
\draw[->, line width= 1] (l0) --  (y);

\end{tikzpicture}
\caption{Causal graph}
\end{figure}

    \subsection{Time dependent
confounding}\label{time-dependent-confounding}

So far we have considered the time fixed context in which treatment and
confounders take on a single value. It was sufficient to block the "back
door" path between the treatment and outcome by conditioning on the
confounding variable(s). In the headache example, the causal effect of
ibuprofen on headache alleviation was confounded by sex. For most
people, sex is a time-fixed covariate because it does not change value
over time. To broaden the setting to a time dependent context, we adopt
the canonical example of the causal effect of Zidovudine (AZT) on
mortality amongst human immunodeficiency virus (HIV)-infected subjects
\citet{Hernan2000}. In this example, subjects are measured at baseline
\(t = 0\) and at subsequent visits. In each visit the patient's CD4
lymphocyte count is measured and a treatment decision made. Survival at
the end of follow-up is a binary outcome equal to 1 if the patient has
died and 0 otherwise. \linebreak

The time-fixed notation can be extended to include subject histories for
time varying variables. Treatment and covariate histories up to visit
\(k\) are can be represented by an overhead bar. For example,
\(\bar X_{k} = \{X_{0}, \hdots, X_{k}\}\) represented the vector of
treatment decisions while \(\bar Z_{k} = \{Z_{0}, \hdots, Z_{k}\}\)
represents the vector of measurements on the time dependent-confounder
\(Z\). Time-fixed covariates like sex, or covariates which change
linearly over time like age are tyically recorded at baseline
(\(t = 0\)) and we denote the collection of baseline covariates as
\(V_{0}\). The outcome of interest at the end of follow-up is mortality
\(Y\) which is a binary variable taking the value \(1\) if the patient
is dead and \(0\) otherwise. \linebreak 

Just as in the time-fixed case, time-dependent confounders lead to
spurious associations between \(X\) and \(Y\) through a "back door" path
between \(X\) and \(Y\) through \(L\). To estimate a causal effect it is
necessary to block this path by conditioning on the confounding
variables. Figure ? gives an example of this in the time dependent case
for two periods (\(t = 0, 1\)). In the first period a treatment decision
is made based on the measured confounder \(Z_0\). In the second period
(\(t = 1\)) a new treatment decision is made based on both \(Z_0\) and
\(Z_1\). Conditioning on \(\bar Z\) under this DAG leads to a consistent
estimate of the causal effect because doing so blocks all paths between
\(X_0\) and \(X_1\) and \(Y\) except the causal path of interest.
\linebreak

However, the time-dependent context also admits structures like the
middle pane of figure ? with the addition of a causal relationship
between \(X_0\) and \(Z_1\). It is now possible for current treatments
to be a determinant of future confounders which are in turn determinants
of future treatment \citet{Robins2000a}. As a result the effect of
\(A_0\) on \(Y\) is mediated through \(L_1\) in the path
\(A_0 \rightarrow L_1 \rightarrow Y\). Blocking this path by
conditioning on \(Z\) also blocks some portion of the effect of \(A_0\)
on \(Y\) and will lead to a biased estimate. \linebreak

A second danger in the time-dependent context arises when \(Z\) is a
common effect of treatment and an unmeasured variable \(U\) which also
influences the outcome \(Y\). There is no direct association Figure ?
shows the same two structures with the addition of a an unmeasured
variable \(U\) which influences \(Z\) and \(Y\). Conditioning on \(Z\).
Selection bias precludes unbiased estimation \citet{Hernan2004}. There
is a mediating relationship between \(A\) and \(Z\) in which case there
is a spurious relationship between \(A\) and \(Y\) again? This is less
inuitive and so examples are best according to \citet{Cole2010}. We can
say that Z is a common effect of A and U, once we condition on Z we
create a dependence of A on U. U is a cause of Y and hence there is an
association between A and Y. This association is present even when there
is no direct causal path between A and Y. \linebreak

Hazard ratios and selection bias \citet{Hernan2010}. Actual application
will look at toxicity of treatment. Some people will be suceptible and
drop out leaving more people in the untreated arm of the study.
presumably because in any population some people are more suceptible
than others.

general point about selection bias is that the general population is not
a valid control group. This is interesting because it links very closely
with the counterfactual approach which defines the causal effect, not
with references to a population but to an individual.

Conclusion, 1) clearly a different technique is required for analysis 2)
the nature of time dependent case needs to be described fully enough to
explain why we choose the simulation algorithm that we choose and any
holes in it. In subsequent sections our choice of simulation algorithm
will be motivated by the structure of time dependent confounding as well
as the viability of introducing positivity violations which are
propogated through the time dependent structure. Explain meaning of a
collider and that a collider that is conditioned on will not block
confounding. Essentially with this kind of data we cannot use
confoudning or stratification methods.

\begin{itemize}
\tightlist
\item
  Intuition from Pearl 2009 book pp. 17 also on schools. More intuition
  in cole 2010
\item
  Simpson's paradox linked to making comparisons within strata -
  collapsibility
\item
  explain that we are often interested in parsimnonious models so cannot
  have all covariates \(U\) that will create associations between \(X\)
  and \(Y\)
\item
  explain why we do not need to worry about the path between A\_0 and
  A\_1
\item
  explain why mediation is likely to occur in example.
\item
  explain why saturated models cannot be used because they will have
\item
  inituitive examples of selection bias.
\item
  Saturated models are not an option because they would be
  computationally intensive and so we use parametric models which also
  links to positivity because we smooth over zeroes in certain strata.
\item
  Explain why hazard ratios have a built in selection bias after giving
  some examples of why selection bias arises. It is because it is
  selective on patients reaching the time period in question. Is this
  also the reason why summary methods create selection bias.
\item
  give an inuitive explanation for why CD4 count is a predictor of
  subsequent treatment and of death.
\item
  Because treatment is randomized (at baseline) in expectation the
  proportion of men and women in each group is the same.
\item
  The reason why summary values are a problem for selection bias is in
  \citet{Robins1992} is similar/the same as hernan on hazards of hazard
  ratios.
\item
  explain that Z is a mediator variable and also explain colliders and
  why conditioning on a collider creates an association between A and U
  and hence A and Y
\item
  The question is whether treatment as a whole, the entire history of
  treatments, has an effect on the outcome \citet{Gill2001}
\end{itemize}

    \begin{figure}

\begin{minipage}{.2\textwidth}

\begin{tikzpicture}

% nodes %
\node[text centered] (l0) {$L_0$};
\node[below = 3 of l0, text centered] (a0) {$A_0$};
\node[right = 3 of l0, text centered] (l1) {$L_1$};
\node[right = 3 of a0, text centered] (a1) {$A_1$};
\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};

% edges %

%L0%
\draw[->, line width= 1] (l0) --  (l1);
\draw[->, line width= 1] (l0) --  (a0);
\draw[->, line width= 1] (l0) --  (a1);
\draw[->, line width= 1] (l0) --  (y);

%A0%
\draw[->, line width= 1] (a0) --  (a1);
\draw[->, line width= 1] (a0) --  (y);

%L1%
\draw[->, line width= 1] (l1) --  (a1);
\draw[->, line width= 1] (l1) --  (y);

%A1%
\draw[->, line width= 1] (a1) --  (y);

\end{tikzpicture}

\end{minipage}
\hspace{5cm}% NO SPACE BETWEEN \end \hspace and \begin!
\begin{minipage}{.2\textwidth}

\begin{tikzpicture}

% nodes %
\node[text centered] (l0) {$L_0$};
\node[below = 3 of l0, text centered] (a0) {$A_0$};
\node[right = 3 of l0, text centered] (l1) {$L_1$};
\node[right = 3 of a0, text centered] (a1) {$A_1$};
\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};

% edges %

%L0%
\draw[->, line width= 1] (l0) --  (l1);
\draw[->, line width= 1] (l0) --  (a0);
\draw[->, line width= 1] (l0) --  (a1);
\draw[->, line width= 1] (l0) --  (y);

%A0%
\draw[->, line width=1] (a0) --  (a1);
\draw[->, line width=1] (a0) --  (l1);
\draw[->, line width=1] (a0) --  (y);

%L1%
\draw[->, line width= 1] (l1) --  (a1);
\draw[->, line width= 1] (l1) --  (y);

%A1%
\draw[->, line width= 1] (a1) --  (y);

\end{tikzpicture}

\end{minipage}
\caption{Figure 1 DAG} \label{fig:fig2}
\end{figure}

    \begin{figure}

\begin{minipage}{.2\textwidth}

\begin{tikzpicture}

% nodes %
\node[text centered] (l0) {$L_0$};
\node[below = 3 of l0, text centered] (a0) {$A_0$};
\node[right = 3 of l0, text centered] (l1) {$L_1$};
\node[right = 3 of a0, text centered] (a1) {$A_1$};
\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};

% edges %

%L0%
\draw[->, line width= 1] (l0) --  (l1);
\draw[->, line width= 1] (l0) --  (a0);
\draw[->, line width= 1] (l0) --  (a1);
\draw[->, line width= 1] (l0) --  (y);

%A0%
\draw[->, line width=1] (a0) --  (a1);
\draw[->, line width=1] (a0) --  (l1);
\draw[->, line width=1] (a0) --  (y);

%L1%
\draw[->, line width= 1] (l1) --  (a1);
\draw[->, line width= 1] (l1) --  (y);

%A1%
\draw[->, line width= 1] (a1) --  (y);

\end{tikzpicture}

\end{minipage}
\hspace{5cm}% NO SPACE BETWEEN \end \hspace and \begin!
\begin{minipage}{.2\textwidth}

\begin{tikzpicture}

% nodes %
\node[text centered] (l0) {$L_0$};
\node[below = 3 of l0, text centered] (a0) {$A_0$};
\node[right = 3 of l0, text centered] (l1) {$L_1$};
\node[right = 3 of a0, text centered] (a1) {$A_1$};
\node[below right = 1.5 and 5 of l0, text centered] (y) {$Y$};

% edges %

%L0%
\draw[->, line width= 1] (l0) --  (l1);
\draw[->, line width= 1] (l0) --  (a0);
\draw[->, line width= 1] (l0) --  (a1);
\draw[->, line width= 1] (l0) --  (y);

%A0%
\draw[->, line width=1] (a0) --  (a1);
\draw[->, line width=1] (a0) --  (l1);
\draw[->, line width=1] (a0) --  (y);

%L1%
\draw[->, line width= 1] (l1) --  (a1);
\draw[->, line width= 1] (l1) --  (y);

%A1%
\draw[->, line width= 1] (a1) --  (y);

\end{tikzpicture}

\end{minipage}
\caption{Figure 1 DAG} \label{fig:fig2}
\end{figure}

    \subsection{Inverse Probability of Treatment
Weighting}\label{inverse-probability-of-treatment-weighting}

In the previous sections the issue of has highlighted how standard
approaches for controlling for confounding in a time dependent context
may lead to biased estimates. In this section we describe a technique
called inverse probability of treatment weighting that can be used to
obtain unbiased estimates of the causal effect of treatment on outcome
in the presence of time dependent confounding.

Inverse probability of treatment weighting is a technique that can be
used to obtain unbiased estimates of the causal effect of treatment on
outcome in the presence of time dependent confounding. The intuition
behind the technique is that by re-weighting the data a pseudopopulation
is created in which the treatment is independent of any measured
confounders. Regression analysis on the pseudopopulation can be carried
out without the need to control for confounders eliminating the problems
which arose in the previous section due to conditioning on \(Z\).
Crucially, in the pseudopopulation, the causal effect of \(X\) on \(Y\)
remains unchanged. As a result, it is possible to estimate the true
causal effect of \(X\) on \(Y\).

\paragraph{Construction of weights}\label{construction-of-weights}

\begin{itemize}
\tightlist
\item
  why does it work and naive methods do not?
\item
  How does it break the link between X and Z?
\item
  Dangers associated with startification and controlling for methods
  highlighted already explain why we need other methods - explain a few
  of these like G-estimation, SNTM etc.
\item
  creates a pseudo-population in which we have something similar to an
  experimental setting
\item
  Because we need weights this means we need a model for the weights -
  model can be non parametric or parametric depending on data used.
\item
  Contrast IPTW methods with stratification methods.
\end{itemize}

\citet{Horvitz1952}

Areas where IPTW has been used (time dependent confounding, comparing
dynamic regimes, missing data)

Inverse probability of treatment weighting is a technique that
re-weights subject observations to a population where assignment of
treatment is at random. An early example of this technique is the
\citet{Horovitz1952} weighted estimator of the mean. In the context of
marginal structural models, a weight is calculated for each subject
which can be thought of informally as the inverse of the probability
that a subject receives their own treatment \citet{Robins2000}. The
result of applying these weights is to re-weight the data to create a
pseudo-population in which treatment is independent of measured
confounders \citet{Cole2008}. Crucially, in the pseudo population the
counterfactual probabilities are the same as in the true study
population so that the causal RD, RR or OR are the same in both
populations \citet{Robins2000}.

\[w_{t,i} = \frac{1}{\prod_{\tau=0} ^ t p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i}, \bar L_{\tau, i})}\]

stabilized weights
\[sw_{it} = \frac{\prod_{\tau=0} ^ t p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i})} {\prod_{\tau=0} ^ t p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i}, \bar L_{\tau, i})}\]

\begin{itemize}
\tightlist
\item
  why can weights be very unstable?
\item
  show why the stabilized weights have a mean of 1 by applying law of
  iterated expectations. \citet{Hernan2006}
\item
  see appendix 1 of cole 2008 for good informal/intuitive explanation of
  stabilized weights.
\item
  describe no-parametric way of estimating numerator P(A=1) by
  cases/total subjects or saturated model with just an intercept.
\end{itemize}

The use of IPTW is valid under the four assumptions of consistency,
exchangeability, positivity and no misspecification of the model
\citet{Cole2008}.

Informally a patients weight through visit k is proportional to the
inverse of the probability of having her own exposure history through
visit k (Cole and Hernan 2008)

The weight is informally proportional to the participants probability of
receiving her own exposure history

As these weights have high instability we need to stabilize them. The
unstabilized weights can be driven by only a small number of
observations. Why are they unstable?

\begin{itemize}
\tightlist
\item
  true weights are unknown but can be estimated from the data.
\item
  \(A_t\) is no longer affected by \(L_t\), and crucially the causal
  effect of \(\bar A\) on \(Y\) remains unchanged
\end{itemize}

Be more specific about what is contained in the weights. The denominator
depends on the measured confounders \(L\) the numerator does not.

\begin{itemize}
\tightlist
\item
  weighted regression and MSM are equivalent.
\end{itemize}

Point out that we need baseline variables in the conditional statments
in the num and denom of the weights otherwise we break the relationship
between outcome and baselines in the new pseudo-population. If the
baseline variables are not confounders, then we do not want to break
this relationship. Baseline covariates also help to stabalize the
weights (how?)

importantly, changing the relationship between L and A, won;t change the
relationship between L and Y. This means that an intervention in \(A\)
does not affect the relationship between \(L\) and \(Y\). So we remove
the link between \(L\) and \(A\) and assign to \(A\) the value of
treatment on or off. Once we place the patient on treatment, regardless
of the relationship which had existed before hand between the covariate
and treatment, a new relationship between \(A\) and \(Y\) exists in
which the covariate has no say.

\begin{itemize}
\tightlist
\item
  stabilized weights should have a mean of 1
\end{itemize}

\subsubsection{Applications of IPTW}\label{applications-of-iptw}

Different from application of MSMs - IPW is a technique which has been
applied to MSMs - i.e. MSMs are on example of how IPTW can be used.

\begin{itemize}
\tightlist
\item
  Crucial that the relationship between Y and X remains the same in the
  new population. I.e. the marginal structural model is the same.
\end{itemize}

Have been used for missing data problems. see pp.442 of Hernan,
Brumback, Robins 2001 for a list of papers linked to this

\begin{itemize}
\tightlist
\item
  many types like a marginal structural cox model (maybe let this follow
  on after weights part.
\end{itemize}

    \subsection{Assumptions}\label{assumptions}

This section formalises the five assumptions under which inverse
probability weighting can be used to correctly estimate MSMs. The first
three assumptions of consistency, correct model specification and no
measurement error are dealt with briefly. The exchangeability . There is
a trade off between finer confounder control and positovity. As a
result, this section will mainly focus on exchangeability and positivity
assumptions with only brief exposition of the consistency, correct model
specification and measurement bias assumptions. Where necessary the
reader is referred to further work on these assumptions. The assumption
of no unmeasured confounders has received the most attention in the
literature (). Most attention will be given to positivity. Conditions
under which IPTW work are largely untestable (westreich 2012)

\subsubsection{No unmeasured
confounders}\label{no-unmeasured-confounders}

The assumption of no unmeasured confounders has already been discussed
with respect to confounding

\[Y_{x} \perp\!\!\!\perp X\]

\begin{itemize}
\tightlist
\item
  necessity of identifying the most important confounders \citet{},
  \citet{}
\item
  conditional exchangeability.
\item
  this assumption is not empirically verifiable.
\item
  no unmeasured confounding in the simulation
\end{itemize}

\subsubsection{Consistency}\label{consistency}

The consistency assumption states that the actual outcome
\(Y_{i}^{obs}\) for a subject \(i\) is equal to the potential outcome
\(Y_{i}^{x}\) when the treatment received by subject \(i\) is \(x\),
that is \(X_i = x\) (\citet{Cole2009}). The consistency assumption is
required to make inferences about \(y^{x}\) using observational data
because it connects the observational data to the potential outcomes.
For example, if it was known for a subject \(i\) that ibuprofen did not
alleviate headaches, then \(Y_{i}^{x=1} = 0\) is the potential outcome
associated with these events. However, if the same subject drank a glass
of water along with the ibuprofen and this action did alleviate the
headache, then \(Y_{i}^{obs} = 1\) and the potential outcome is not
equal to the observed outcome despite the fact that \(X_i=1\). In other
words, the consistency assumption rules out side effects of exposure and
anchors the observational outcome to the potential outcome framework. As
a result, expressions involving probabilities of counterfactuals can be
cast in terms of ordinary conditional probabilities of measured
variables \citet{Pearl2010} and equated as follows.

\[P(Y^x = y\ |\ Z = z, X = x) = P(Y = y\ |\ Z = z, X = x)\]

In words, the probability that \(Y^x = y\) when \(Z = z\) and \(X = x\)
are observed is equal to the conditional probability that \(Y = y\). The
consistency assumption has generated discussion over whether it is
really an assumption an axiom or definition. The interested reader is
referred to \citet{VanderWeele2009}, \citet{Cole2009} and
\citet{Pearl2010}.

\begin{itemize}
\tightlist
\item
  may consider linking no interference assumption here too. Potential
  outcomes are no longer clearly defined if the treatment received by
  one individual can affect the treatment received by another.
\end{itemize}

\subsubsection{No model mispecification}\label{no-model-mispecification}

Estimating MSMs with a continuous confounder involves specifying a model
for the weights and a structural model relating exposure to outcome. In
both models, correct specification is required to obtain unbiased
estimates. The structural model requires positing a relationship between
exposure and outcome. For example, this relationship may be best
captures through a linear relationship, threshold dose-response or a
model accounting for long and short term effects of exposure
\citet{Cole2009}. The weight model also needs to be correctly specified
in order to consistenly estimate the weights.

For the weight model, the stabilized weights should be one. Parametric
models are unlikely to be perfectly specified but should provide a good
approximation of the true model. Deviations from one are an indication
that the weight model is mispecified.

Several studies have examined the effect of model mispecification in the
estimatuion of MSMs including \citet{Cole2009}. Some key findings.
Broadly, the idea is to simulate data from a known MSM weight model and
introduce realistic deviations from the model.

Correct specifcation of the inverse probability weighting (IPW) model is
necessary for consistent inference from a marginal structural Cox model
(MSCM).

\begin{itemize}
\tightlist
\item
  link to why simulation studies make it possible to isolate model
  specification as an error.
\item
  stabilized weights should have a mean of 1 - indicative of model
  mispecification if they do not
\item
  results in some confounding if the model is mispecified, think about
  using splines to mop up residual confounding
\item
  IPW cannot correctly adjust for all confounding under model
  miscpecification
\item
  could use cohran paper to give example of how model bias enters by
  showing a true model and an incorrect model.
\end{itemize}

In this thesis we simulate data from a known weight and structural
model. In other words, we simulate data in the absence of model
mispecification elimiating this as a source of bias. Importantly, this
makes it possible to isolate effects of interest without worrying about
model miscpecification.

\subsubsection{No measurement error}\label{no-measurement-error}

Measurement error can affect the outcome, exposure or confounder and
other covariates used to estimate MSMs. This can arise due to faulty
equipment, poor recall by survey respondents or simply carlessnes and
rounding. In each case the observed variable \(X^*\) differs from the
true underlying variable \(X\). In general this will result in bias but
the extent of that bias depends on the process through which the error
is introduced and whether with error is recorded. In this thesis we
employ a simulation algorithm to generate data from a known marginal
structural model and the simulated variables have no measurement error.
Analgous to the case of no model miscpecification, this makes it
possible to study the properties of MSMs in the absence of measurement
error. More detail on the effect of measurement error in a causal
context can be found in \citet{Hernan2009}.

\subsubsection{Positivity.}\label{positivity.}

To conclude this chapter ....

\begin{itemize}
\tightlist
\item
  use intuition of making comparisons within strata as a means to
  introduce positivity and link to weight model and standardization as
  examples.
\end{itemize}

The final assumption underlying MSMs, and the central topic of this
thesis, is the positivity assumption. MSMs are used to estimate average
causal effects in the study population, and one must therefore be able
to estimate the average causal effect in every subset of the population
defined by the confounders \citet{Cole2008}. The positivity assumption
requires that there be exposed and unexposed individuals in every strata
of the confounding covariates. For example, when treatment is Zidovudine
and CD4 count is the confounder, there must be a positive probability of
some patients being exposed and unexposed at every level of CD4 count.
Positivity can be expressed formally as \(Pr(A=a\ |\ L) > 0\) for all
\(a \in A\), which extends straightforwardly to the time dependent case
where the positivity assumption must hold at every time step conditonal
on previous treatment, time dependent confounders and any baseline
covaraiates:

\[Pr(A_{it}=a_{it}\ |\ L_{it}, A_{i, t-1}, V_{i0}) > 0\]

Models for the risk \(P(Y=1\ |\ A=a, L=l)\) are commonly studied in
epidemiological applications. Applying basic probability rules reveals
that the risk can be re-written with the term \(Pr(A=a\ |\ L=l)\) in the
denominator:

\[P(Y=1\ |\ A=a, L=l) = \frac{P(Y=1, A=a, L=l)}{Pr(A=a, L=l)} = \frac{P(Y=1, A=a, L=l)}{Pr(A=a\ |\ L=l)Pr(L=l)}\]

This model is only estimable when \(Pr(A=a\ |\ L=l) \neq 0\). Therefore,
when positivity does not hold it is not possible to estimate the model.
In the context of MSMs a similar problem emerges. Although weighting via
IPTW allows naive estimation of (?) without including the confounders,
the weights in (?) involved the term \(Pr(A=a\ |\ L=l)\) in the
denominator. This means that the weights are inestimable whenever
positivity is violated. In order to estimate the causal effect of \(A\)
on \(Y\), weights must be estimable in every subset of the population
otherwise the average causal effect in the study population cannot be
estimated. \linebreak

In practice, positivity can arise when random zeroes or structural
zeroes are present in some levels of the confounding covariates. Random
zeroes arise when, by chance, no individuals or all individuals, receive
treatment within a certain strata as defined by the covariates. For
example, \citet{Cole2008} studies positivity violations in individuals
in strata defined by CD4 count and viral load. By increasing the levels
of CD4 count the chances of random zeroes also increases and
\citet{Cole2008} show that the IPT weights rapidly lose their stability
with the consequence that causal effects are no longer estimable.
Researchers applying IPTW methods must actively check that there are
both treated and untreated individuals at every level of their
covariates within cells defined by their covariates because parametric
methods will smooth over positivity violations and not provide any
indication of nonpositivity. Increasingly refined covariates are
attractive because they provide better control of confounding, but the
point that \citet{Cole2008} make is that this control needs to be traded
off against increased occurence of random zeroes and subsequent
instability of IPT weights. \linebreak

More relevant to this thesis are violations of the positivity assumption
due to structural zeroes. These occur when an individual cannot possibly
be treated or if an individual is always treated within some levels of
the confounding covariate, as is the case in the clinical protocol
example motivating this thesis. Several studies give examples of
structural violations of the positivity assumption in epidemiological
contexts. In \citet{Cole2008} structural zeroes arise when the health
effects due to exposure to a chemical are confounded by health status
proxied by being at work. If individuals can only be exposed to the
chemical at work then all individuals not at work will be unexposed. A
second example is liver disease as a contraindication of treatment. If
individuals with liver disease cannot be treated then all individuals in
the "liver disease = 1" strata will be untreated. In \citet{Messer2010}
structural zeroes arise in the context of rates of preterm birth and
racial segregation, whereas \citet{Cheng2010} find structural zeroes in
the context of fetal position and perinatal outcomes. Our motivating
example is most closely related to liver disease as a contraindication,
except that the clinical protocols require that patients with low CD4
count always be treated instead of never being treated, as in the case
in the liver disease example. \linebreak

Although in many epidemiological settings the positivity assumption is
guaranteed by experimental design, studying positivity violations is
relevant because, as our own motivating example and the examples above
suggest, structural violations do occur, and random zeroes are always
possible especially at finer levels of confounding covariates. Studying
the finite sample propoerties of MSMs under violations to positivity is
therefore an important issue which is yet to be dealt with
systematically in the literature. As \citet{Westreich2010} points out,
positivity violations, positivity violations by a time varying
confounder pose an analytic challenge and they suggest g-estimation or
g-computation may be a way forward. A good start to dealing with the
time varying confounder case is to see how well MSMs work when
positivity is violated. This is also a novelty of this thesis.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  estimated weights with a mean far from one, or very extreme values
  indicate either non-positivity or model mispecification of the weight
  model.
\item
  It is not always true that we want more finely tuned covariates for
  confounder control because the bias and variance of the effect
  estimate may increase with the number of categories. This is similar
  to the positivity masking example.
\item
  Our results are equally valid for other circumstances in which
  positivity may arise.
\item
  Also think about how the number of categories of exposure increases
  the chance that one level of exposure will have a positivity.
\item
  Westreich and Cole 2010 have suggested that methodological approaches
  are needed to weigh the resultant biases incurred when trading of
  confounding and positivity. The framework we use is flexible enough to
  allow this in a simulation setting.
\end{enumerate}

If the structural bias occurs within levels of a time-dependent
confounder then restriction or censoring may lead to bias whether one
uses weighting or other methods (Cole and Hernan 2008). In fact,
weighted estimates are more sensitive to random zeroes (Cole, Hernan,
2008) Introducing violations of positivity can be achieved by censoring
observations.

But to give an intuitive example, think about how it links back to a
situation where sicker patients receive treatment compared to others. So
in the "sick" strata of the CD4 count \textbf{ALL} patients receive
treatment which inflates the IPTW. This also affects how we think about
the associational versus causal models. The causal effect might be 50/50
but because sicker patients get treatment the mortality ratio in the
treated group is likely to be higher.

The trade-off between positivity and confounding bias is emphasized in
Cole2008

Why is practicality important? Cole paper highlights practical advice to
practictioners. positivity can be violated in a practical setting
because of two few strata, it can be the result of protocols in a
clinical setting and it can be seen as a trade-off between
exchangeability (and we need more measured predictors to maintain
exchangeability) and positivity where more predictors leads to more
likely a zero problem.

\begin{itemize}
\tightlist
\item
  Dynamic stratgies evaluated using MSMs will have rules like, start
  treatment if CD4 falls below a certain threshold. See Didelez
  presentation on this
\item
  Explain that there is positivity in estimation of the structural model
  and also in the weight model. The reason why positivity is more
  important in the weight model is because when the weights are unstable
  the estimates can be very wrong as a result.
\item
  read and add \citet{Naimi2011} who have already done a simulation
  study of non-positivity

  \begin{itemize}
  \tightlist
  \item
    in context of healthy worker effect
  \item
    explain why this doesn't get to the heart of the issue of
    non-positivity
  \item
    a relevant question highlighted by \citet{Naimi2011} is whether the
    effect of non-positivity is amplified with more than two time
    periods.
  \item
    explain why healthy worker effect and positivty go neatly together
    because as a result of people dropping out there will be empty
    cells/strata.
  \end{itemize}
\item
  Mortimer 2005 has also done a study on positivty (also called the
  experimental treatment assignment (ETA))

  \begin{itemize}
  \tightlist
  \item
    they use real data, not simulation and
  \end{itemize}
\end{itemize}

    A model that parameterises \(P(Y\ |\ do(A=a))\) is called a marginal
structural model (MSM) as it is marginal over any covariates and
structural in the sense that it represents an interventional rather than
observational model.

    \newpage

    \section{Key concepts in survival
analysis}\label{key-concepts-in-survival-analysis}

This chapter briefly reviews several important concepts in survival
analysis which are pertinent to this thesis and specifically the
simulation algorithm that will be used to generate data in later
chapters. Survival analysis is the study of the distribution of life
times. For example, in. Often we are interested in comparing survival in
two or more groups. For example a group which is exposed to treatment
and a group which is not. Confounding is also important. Survival
analysis is by nature time dependent and hence time dependent
confounding is particularly studies in a survival context. The basic
concepts reviewed here are a condensed version of those presented in
\citet{JohnP.Klein2003}.

\begin{itemize}
\tightlist
\item
  Explain why we do not look at censoring and truncation.
\item
  emphasis on discrete time which is the application.
\end{itemize}

\subsubsection{Survival function}\label{survival-function}

The survival time \(T\) is the time between a well defined start point
and a well defined end point. For example the time between birth and
death. The survival function is the probability that a certain
individual survives until time \(t\) or equivalently the probability
that the survival time \(T\) is greater than \(t\), \[S(t) = P(T > t)\].
In the continuous case the survival function can be written

\[S(t) = P(T > t) = \int_{x}^{\infty}f(t)dt\]

In the discrete case the survival function

\[S(t) = P(T > t) = \sum_{x_j > x}p(x)\]

\subsubsection{Hazard function}\label{hazard-function}

The hazard function or hazard rate expresses \emph{the ``approximate''
probability of an individual of age x experiencing the event in the next
instant.}

\[h(x) = \lim\limits_{\Delta x \to 0}\frac{P(x \le X \le x + \Delta x\ |\ X \ge x)}{\Delta x}\]

In the discrete case, the probability that the event occurs between
\(j-1\) and \(j\) is equal to the difference in survival of these times
\(p(x_j) = S(x_j) - S(x_{j-1})\). The hazard function is

\[h(x) = Pr(X = x_j\ |\ X \ge x_j) = \frac{p(x_j)}{S(x_{j - 1})}\]

\[h(x) = 1 - \frac{S(x_j)}{S(x_{j - 1})}\]

\[S(x) = \prod \frac{S(x_j)}{S(x_{j - 1})} = \prod 1 - h(x_j)\]

Showing that the survival function is determined by the hazard rates.

In words, the discrete hazard function. There are two probabilities, the
probability that the death occurs

\[Pr(Y=1\ |\ X=x) = \frac{e^{\beta_0 + \beta_1 \times x}}{e^{\beta_0 + \beta_1 \times x} + 1} = \frac{1}{1 + e^{-(\beta_0 + \beta_1 \times x)}}\]

\begin{itemize}
\tightlist
\item
  show that hazard function is not collapsible or hazard ratio.
\item
  condition on survival up to a certain point?
\item
  Hazard ratios differ from relative risks and odds ratios in that RRs
  and ORs are cumulative over an entire study, using a defined endpoint,
  while HRs represent instantaneous risk over the study time period, or
  some subset thereof.
\item
  compare hazard rate and hazard function with the hazard ratio. The
  hazard ratio could be between men and women for example.
\item
  relate survival to hazard function in order to show that survival is
  completely determined by hazard rate.
\item
  Naturally survival is measured at each follow up point, but as the
  person is alive until their final follow up then we are always
  conditioning on Y = 0 until the final follow up.
\item
  a logistic model is one where the log-odds of the probability of an
  event is a linear combination of independent or predictor variables.
\item
  need to make clear that we could split groups into a control and
  treatment group if they are exchangeable.
\end{itemize}

\subsubsection{Proportional hazards}\label{proportional-hazards}

\begin{itemize}
\tightlist
\item
  explain multiplicative model
\item
  causal model
\item
  discrete time relationship between cox model and logistic model
\end{itemize}

Often we are interested in how survival differs between subjects who
have been exposed to a treatment versus those who remain unexposed. In a
randomised trial exchangeability is guaranteed and the survival function
can be calculated. In observational trials the same confounding issues
described in the previous chapter require a different approach.
Adjusting for other variables is important to remove bias and provide a
more accurate fit.

In a typical study we want to adjust for confounders and other variables
in order to get a better estimate than just looking at treated versus
non-treated patients.

\begin{itemize}
\tightlist
\item
  explain how we let the survival time depend on covariates.
\item
  link to causal framework, introduce marginal structural logistic
  regression or marginal structural cox proportional hazards model.
\item
  test for equality between (all) groups.
\end{itemize}

    \newpage

    \section{Simulating from marginal structural
models}\label{simulating-from-marginal-structural-models}

In order to assess the impact of violations of the positivity assumption
on the performance of the IPTW estimator we simulate data from a
specific marginal structural model in a series of monte carlo
simulations. Several algorithms for simulating from marginal structural
models have been suggested in the literature. To test the effect of
positivity violations on the performance of marginal structural models
we use the algorithm of \citet{Havercroft2010}. This algorithm has
several key features which are described in detail in this chapter. We
start with an overview of the logic behind monte carlo simulations in
general terms.

In this section we first describe in general terms the logic behind
monte carlo simulations. The specific In this chapter we start by
describing the logic behin monte carlo simulations in general terms.
Next, we consider several important criteria that a simulation model
must exhibit in the context of MSMs. In particular we require an
algorithm that can simulate from a specific MSM, has the observational
structure described earlier and we also define noncollapsibility.
Several algorithms have been proposed in the literature and these are
briefly discussed and compared. We then focus on the algorithm suggested
in \citet{} and explain why it satisfies our requirements. The most
salient aspects of this algorithm for the purposes of this thesis are
described.

    \paragraph{Monte Carlo Simulations}\label{monte-carlo-simulations}

In statistical research, interest often lies in the estimation of a
population parameter \(\theta\). When only a sample \(X_1\) from the
population is available, statistical procedures are applied to estimate
the population parameter \(\hat \theta_1\) based on that sample. The
same procedure applied to a second sample \(X_2\) drawn from the same
population will result in second estimate \(\hat \theta_2\), and so on
for more samples. We rely on the sampling distribution of the
\(\hat \theta_i\) to draw statistical inferences. \linebreak 

Monte Carlo simulations flip this process on its head. We start with a
known true model governed by parameters \(\theta\) and generate a sample
of data according to this model. We then apply a statistical procedure
to rediscover the true parameters governing the data generating process.
Most often we are interested in the finite sample properties of
techniques. In other words, we are interested in how the technique
behaves with, say, only one thousand observations rather than one
million or one billion. A single sample of one thousand observations we
never truly simulated data and check how closely the to Where
statistical inference is a process of discovering The properties of a
statistical method Rather than being a process of dicoverey we can use
simulations as a process of rediscovery and appraise a method by its
ability to correctly rediscover the parameter. For example, the effect
of a ibuprofe on headaches can be expressed in by an effect measure such
as the odds ratio \linebreak

\[OR = \frac{P(Y = 1\ |\ X = 1)/(P(Y = 1\ |\ X = 0))}{P(Y = 0\ |\ X = 1)/(P(Y = 0\ |\ X = 0))}\]

Or the log odds ratio

\[log(OR) = log(P(Y = 1\ |\ X = 1)/(P(Y = 1\ |\ X = 0)) - log(P(Y = 0\ |\ X = 1)/(P(Y = 0\ |\ X = 0))\]

When \$P(Y = y~\textbar{}~X = x) = \$

\[log \frac{P(Y = 1\ |\ X = 1)}{P(Y = 1\ |\ X = 0)} = \alpha + \beta \times x\]

These steps can be summarised

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Specify the articifial population
\item
  Sample from that population
\item
  Calculate the parameter of interest
\item
  repeat steps 2 and 3 a certain number of times
\item
  draw conclusions.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  show that the logistic model is the log odds model or odds ratio
  model.
\item
  explain that the cox prop hazard model discrete time equivalent is the
  logistic model.
\end{itemize}

So that the log odds ratio between those who received treatment and
those who did not is given as

\[log \frac{P(Y = 1\ |\ X = 1)}{P(Y = 1\ |\ X = 0)} = \beta_0 + \beta_1 \times x \]

By specifying \({\beta_0, \beta_1}\) we can simulate outcome from this
model by first generating values for \(x\) and then using the values of
\(x\) to generate \(Y\) using calcualting y by rounding probabilities.
Each time we sample we get \((Y, X)\) and we can perform logistic
regression to see whether the correct parameters are resolved. We could
then change something like the model form and see what happens to the
model under changes. Or we may just want to see how accurate the model
is using finite a finite sample rather than asymptotics, say a sample of
n = 1000 observations.

More generally, the steps taken in a simulation study can be summarized:

Explain why simulations still need to be treated critically.
\citet{Greenland1997}

\begin{itemize}
\tightlist
\item
  advantage of no measurement error, no model misspecification
\item
  Add in confidence intervals.
\item
  end with why we need to sample from a specific MSM
\end{itemize}

    \paragraph{Noncollapsibility}\label{noncollapsibility}

In the example of the effect of ibuprofen on headache alleviation it was
reasonable to ask whether that effect differed between male and female
subjects. That effect could be captured by effect measures such as the
risk difference, risk ratio or odds ratio. If treatment affects the
outcome in males and females differently then the effect in the male
strata will be different from the effect in the female strata.
Marginalizing over the two strata will lead to biased effect measure
because comparisons will not be between exchangeable subjects which will
propogate up to the effect measure of interest. \linebreak

On the other hand, if treatment affects males and females equally then
it seems reasonable to expect that the marginal effect (after
marginalizing over sex - pooling subjects) would also be equal to the
male and female specific effects. In other words, if the effect in males
is the same as in females then it is reasonable to think that the effect
is the same in the total population, which is made up of males and
females. In that case, the effect measure is collapsible because sex is
unrelated to exposure to ibuprofen and pooling males and females
together and collapsing over sex, correct analysis can be performed
using a marginal or naive analysis. This extends to the case of many
subgroups defined by, for example, age or race. Collapsibility is useful
because it helps to reduce the dimensionality and computational effort
which arises when many subgroups need to be taken account of in the
analysis \citet{Didelez2010}. \linebreak

However, some effect measures are noncollapsible, in the sense that even
when their strata specific effects are equal, collapsing over strata the
marginal or naive effect measure does not equal the strata specific
effect measures. Simpson's paradox (\citet{}). Examples of
non-collapsible effect measures include the odds ratio and rate
difference. The odds ratio is estimated using logistic regression

In a practical setting, the odds ratio may be used and there may be
confounding and there may be noncollapsibility. In fact, the ability of
the inverse probability to correctly estimate the causal effect of
treatment on outcome in the presence of time dependent confounding make
it possible to separate the confounding and collapsibility elements and
quantfy the extend to which collapsibility affects results
\citet{Pang2016}. \linebreak

The relevance of noncollapsibility to this thesis is the need to
simulate data from a known MSM model. The conditionals from which we
draw data will not typically be collapsible.

is the same it seems reasonable that the effect in the population would
also be the same. Naturally the difference could be due to the
differening effect in the population in which case confounding is
present. the same we could conclude that sex made no difference and look
at marginal associations instead. Reducing the number of variables
required in analysis is often useful because it reduces dimensionality
and computational effort allowing subgroups to be pooled together

An effect measure like a rate difference, rate ratio or odds ratio is
collapsible when the effect is equal in both strata and equal to
marginal (over \(C\)) effect. Collapsibility is a useful property
because it means that analysis can be carried out on a subset of
variables after marginalizing over the others . In this case that could
mean marginalizing over sex and looking at just the relationship between
\(Y\) and \(X\). \linebreak  

\begin{itemize}
\tightlist
\item
  see greenland 1999 collapsibility regression formulation for an
  example in regression context.
\item
  mixed up with confounding and concluded that there is no confounding
  when the conditional odds ratio equals the conditional odds ratio.
\end{itemize}

An effect measure for the association between \(Y\) and \(X\) such as an
rate ratio or an odds ratio is noncollapsible when conditioning on a
covariate a to \(Y\) changes the size of the odds ratio even when .
Collapsibility is useful when

\begin{itemize}
\tightlist
\item
  Explain that the effect of interest depends on the question (also cite
  \citet{Pearl2014} on this)
\end{itemize}

An effect measure is non-collapsible across strata defined in the
analysis if the constant effect measure does not equal the strata
specific effect measure. greenland 1996

example of randomized trial, there being no confounding because it is a
randomized trial, same result in men and women, different result in
whole population. Not confounding because we eliminate confounding by
randomization.

Greenland 1999 - if the model with Z is correct then the model without Z
is unlikely to have the same coefficient

Noncollapsibility arises when the marginal effect measure (marginal over
any covariates, i.e. unstratified or with no confounder control, crude)
is not equal to the strata specific effect measure.

\begin{itemize}
\tightlist
\item
  explain why it is a probelm for marginals versus conditionals with
  (i.e. won;t be the same MSM)
\item
  use cox ph model as example.
\end{itemize}

This is a problem when simulating from marginal structural models
because the correct marginal structural model

\[correct marginal structural model\] \[model with covariates\]
\[collapsed over covariates and not equal to marginal model\]

Collapsibility starts with the notion of confounders. We assume that
within strata of confounders that the effect of the confounder is
homogenous. I.e. in the female strata, the effect of being female is
homogenous.

\citet{Greenland1996}, \citet{Greenland1999}, \citet{Greenland2011},
\citet{Sjoelander2016}

The effect of treatment on diesease outcome may be unconfounded but
noncollapsible

Collapsibility is the same as simpson's paradox if we adopt the
definition that without the conditional variable they can be equal.

collapsibility depends on the measure used. Some are collapsible and
some are not

Could arise in two ways 1) within strata effect measures may not be the
same 2) Even if they are the same they may not equal the marginal effect
measure (marginal over any covariates Z)

Collapsibility means there is no incompatibility between the marginal
model and the conditional distributions used to simulate the data.
Provide example of this. Explain how this affects the simulation
algorithm. Especially hazard ratios which are non-collapsible.

Models are noncollapsible when conditioning on a covariate
\textbf{related to the outcome} changes the size of the estimate even
when the covariate is unrelated to the exposure. Illustrate why this
happens with survival models.

Survival models are non-collapsible. Hence we cannot eaily simulate from
them. Instead we use U as a sneaky trick. Explain why survival models
are non-collapsible - through the hazard function.

This is particularly important because collapsibility and confounding
are often treated as identical concepts when in fact they are not.
\citet{Greenland1999}

\begin{itemize}
\tightlist
\item
  relevance to the algorithm? How does this work with a specific MSM.
\item
  Show why hazard ratios are not collapsible.
\item
  Explain why models with product terms are clearly not collapsible
\item
  relation to a hazard function
\item
  link to lack of exchangeability
\end{itemize}

    \paragraph{Observational structure}\label{observational-structure}

The previous chapter included a discussion of the role of confounding
covariates and time dependent confounding. In the time fixed context,
blocking the path from a confounding covariate by conditioning was
sufficient to consistently estimate the effect of treatment on outcome
in the absence of unmeasured confounders. In the time dependent context,
conditioning on confounding covariates may biases estimates in two ways.
First, in a time dependent context, both treatment and covariates can
change over time. Some effect of treatment may be transmitted through
the covariate to the outcome or future treatment. Blocking this path
also blocks the indirect effect of treatment on outcome through the
confounding covariate. At the same time, selection bias arises due to
conditioning on a colider. The hazard ratio, the ratio of the hazard
function/rate at two levels of an explanatory variable, typically
exhibits selection bias. Failing to condition \linebreak 

\begin{itemize}
\tightlist
\item
  why important for observational studies
\item
  examples of it in observational studies
\item
  explain why we need algorithm with this structure.
\end{itemize}

    \paragraph{Simulation algorithms literature
review.}\label{simulation-algorithms-literature-review.}

Several algorithms for simulating data from a specific marginal
structural model have been suggested in the data. Here we briefly
summarise the discussion in \citet{Havercroft2012} on competeing
algorithms and then highlight a number of algorithms suggested
subsequently to that paper.

\begin{itemize}
\tightlist
\item
  briefly summarise the litrature in \citet{Havercroft2012}
\item
  update with more recent algorithms like \citet{Young2014}
\item
  discuss \citet{Naimi2011} limitations of simulation algorithm. look
  into more detail for how this algoirthm works.
\end{itemize}

Several algorithms for simulating from a specific MSM have ben suggested
in the literature.

\begin{itemize}
\item
  \citet{Havercroft2012}
\item
  \citet{Bryan2004}
\item
  \citet{Westreich2012} not in review by \citet{Havercroft2012}
\item
  \citet{Young2014}
\item
  Bryan 2004: fixes the vector L at the beginning which means A never
  affects L which don't make no sense. In other words it doesn't have
  the observational structure we are looking for.
\item
  Do we introduce a form of selection bias into the data when we force
  positivity according to protocols? At baseline the proportion of
  people in any CD4 strata was unknown but randomized and hence in
  expectation it should be the same in treated and untreated.
\item
  Young 2014 - Law of the observed outcome conditional on the measured
  past. What this paper shows is that the regression results are not
  correct but the IPTW ones are fine. The issues is comparing IPTW to
  normal regression results. As we compare IPTW results to IPTW results
  under positivity it should be fine to use the Havercroft algorithm.
\end{itemize}

\citet{Havercroft2012} also make these comparisons in their paper in a
few paragraphs.

What we add here is how control over \(L\) allows us to introduce
positivity violations.

\begin{itemize}
\tightlist
\item
  should explain all the reasons why we choose Havercroft algorithm over
  others.
\end{itemize}

    \subsection{Simulation algorithm}\label{simulation-algorithm}

The base algorithm used in this paper is drawn from
\citet{Havercroft2012}. In that paper, an algorithm for simulating data
from a specific MSM is derived from the joint factorization of a DAG
which exhibits time dependent confounding. The algorithm generates
subject data for multiple visits. The central concepts of the algorithm
can be explained using a simplifies two period version of the algorithm,
see figure ?. \linebreak

\begin{itemize}
\tightlist
\item
  stress that we simulate from conditionals.
\end{itemize}

    \begin{figure}
\centering
\begin{tikzpicture}[->,>=stealth',auto,node distance=3cm,
  thick,main node/.style={circle,draw=none,font=\sffamily\Large\bfseries}]

  \node[main node, circle, draw] (1) {$U_0$}; %unmeasured
  \node[main node] (2) [right of=1] {$A_0$};
  \node[main node] (3) [above of=2] {$L_0$};
  \node[main node] (4) [below of=2] {$Y_1$};
  \node[main node] (5) [right of=2] {$A_1$};
  \node[main node] (6) [above of=5] {$L_1$};
  \node[main node] (7) [below of=5] {$Y_2$};
  \node[main node, circle, draw] (8) [above of=6] {$U_1$}; %unmeasured

  \path[every node/.style={font=\sffamily\small}]
    (1) edge node [right] {} (2)
    (1) edge node [right] {} (3)
    (1) edge node [right] {} (4)
    (1) edge[bend right = 60] node [left] {} (7)
    (1) edge[bend left] node [left] {} (8)
    (2) edge node [right] {} (3)
    (2) edge node [right] {} (4)
    (2) edge node [right] {} (5)
    (2) edge node [right] {} (6)
    (2) edge node [right] {} (7)
    (3) edge node [right] {} (6)
    (4) edge node [right] {} (7)
    (5) edge node [right] {} (6)
    (5) edge node [right] {} (7)
    (8) edge node [right] {} (6);
\end{tikzpicture}
\caption{Causal graph}
\end{figure}

    The DAG corresponding to their simulation algorithm is shown in figure
?. This DAG who derive their algorithm according to a specific DAG. Here
a sketch of the key points in this algorithm is presented, the full
proof demonstrating that the algorithm simulates data from a specific
MSM is presented in Appendix B of \citet{Havercroft2012}.

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Marginal Structural Model Under Time Dependent Confounding}
 \For{i in 1, \dots , n}{
  $U_{0, i} \sim U[0, 1]$\\
  $\epsilon_{0, i} \sim N(\mu, \sigma^2)$\\
  $L_{0, i} \gets F^{-1}_{\Gamma(k,\theta)}(U_{i, 0}) + \epsilon_{0, i}$\\
  $A_{-1, i} \gets 0$\\
  $A_{0, i} \gets Bern(expit(\theta_0 + \theta_2 (L_{0, i} - 500)))$\\
  \If{$A_{0, i}= 1$}{
   $T^* \gets 0$;
  }
  $\lambda_{0, i} \gets expit(\gamma_0 + \gamma_2 A_{0, i})$\\
  \eIf{$\lambda_{0, i} \ge U_{0, i}$}{
   $Y_{1, i} \gets 0$\\
   }{
   $Y_{1, i} \gets 1$\\
  }
  \For{k in 1, \dots , T}{
   \If{$Y_{t, i} = 0$}{
    $\Delta_{t, i} \sim N(\mu_2, \sigma^2_2)$\\
    $U_{t, i} \gets min(1, max(0, U_{t-1, i} + \Delta_{t, i}))$\\
    \eIf{$t \neq 0\ (mod\ k)$}{
     $L_{t, i} \gets L_{t-1, i}$\\
     $A_{t, i} \gets A_{t-1, i}$\\
     }{
     $\epsilon_{t, i} \sim N(100(U_{t, i}-2), \sigma^2)$\\
     $L_{t, i} \gets max(0, L_{t-1, i} + 150A_{t-k,i}(1-A_{t-k-1,i}) + \epsilon_{t, i})$\\
     \eIf{$A_{t-1, i} = 0$}{
      $A_{t, i} \sim Bern(expit(\theta_0 + \theta_1t + \theta_2(L_{t, i}-500)))$\\
      }{
      $A_{t, i} \gets 1$\\
     }
     \If{$A_{t, i} = 1 \and A_{t-k, i} = 0$}{
      $T^* \gets t$\\
     }
    }
    $\lambda_{t, i} \gets expit()\gamma_0 + \gamma_1[(1 - A_{t, i})t + A_{t, i}T^*] + \gamma_2 A_{t, i} + \gamma_3 A_{t, i}(t-T^*))$\\
    \eIf{$1 - \prod_{\tau=0}^t(1 - \lambda_{\tau, i}) \ge U_{0, i}$}{
     $Y_{t+1, i} = 1$\\     
    }{
     $Y_{t+1, i} = 0$\\
    }
   }
  }
 }
 \caption{Simulation Algoirthm MSM}
\end{algorithm}

\begin{itemize}
\tightlist
\item
  describe how the algorithm works, is derived and how it acheives a
  specific MSM, the observational structure and a specific MSM
  (collapsibility)
\end{itemize}

    \subsection{Algorithm with positivity
violations.}\label{algorithm-with-positivity-violations.}

\begin{itemize}
\tightlist
\item
  show parts of algorithm which change
\item
  present full algorithm with all changes in appendix
\item
  interesting question is how the positivity violations are propogated
  through time. This can be checked by lengthening the number of visits
  sequentially. For example, do positivty violations with a time horizon
  of 5 visits have less of an effect than 10 visits.
\end{itemize}

The parameter of interest could be expected survival or the five year
survival probability

Need to specify what the MSM is, give an example of it as a hazard
function. Survival is completeley determined by the hazard function.

U allows us to get any distribution we like for Y marginal over
covariates, WOuld L itself allows this? probably not. We can somehow get
from this the subjects counterfactual survival time.

Importantly, this expression on the left hand side has unobserved
counterfactuals, but the right hand side has only observed quantities
which would be observed in an actual observational study.

non-collapsibility is an unresolved issue here. So even if we
investigate positivity we can still only do so for collapsible models?

Equivalent way of motivating dividing the joint distribution by
Pr(A\textbar{}L) is through IPTW.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  derive relationship between MSM and DAG and the correct conditional
  distributions. Follows from truncated factorisation why we can et
  P(Y\textbar{}do(a))
\item
  think of this process as if we had fixed a treatment vector in
  advance. consistency assumption.
\item
  HD 2012, with Pearl and truncated factorization formula, show that it
  is possible to link the counterfactual represented by
  \(P(Y\ |\ do(a))\) to observattional data generated in an
  observational way. But the problem arises when the model is
  non-collapsible or non-linear.
\end{enumerate}

In the one shot case we set A = 1/0 because we are interested in the
outcome under either of these treatment scenarios. In the time dependent
case, A is a vector of 0s and 1s and we want to pretend that we decide
in advance that the whole vector A is specified. But A and L have a
complex interplay in an observational setting. So we want to pretend
that A (a vector of 1s and 0s) is set in advance but at the same time
have the observational structure for A and L.

The relationship between Y and L is then dependent on A. There is no
relationship between A and U because of the set-up in the DAG. The
variable L blocks this relationship.

Figure 1 represents the system under consideration. The DAG in figure 1
represents the one-shot non-longitudinal case. Factorising the joint
distributions of the variables in figure 1 yields

\[P(U,\ L,\ W,\ A,\ Y) = P(W)P(U)P(W)P(L\ |\ U)P(A\ |\ L,W)P(Y\ |\ U,A)\]

Where, following definition 1.1 we delete \(P(A\ |\ L,W)\), a
probability function corresponding to \(A\), and replace \(A=a\) in all
remaining functions

\[ P(U, L, W, Y\ |\ do(A=a)) =
  \begin{cases}
    P(U)P(L\ |\ U)P(Y\ |\ U,A = a) & \quad \text{if } A = a\\
    0  & \quad \text{if } A \neq a\\
  \end{cases}
\]

The goal is to simulate from a particular MSM. This means parameterising
\(P(Y\ |\ do(A=a))\). Applying the law of total probability over \(W\),
\(U\) and \(L\) yields

\[P(Y\ |\ do(A=a) = \sum_{w, u, l} P(W)P(U)P(L\ |\ U)P(Y\ |\ U, L, A=a) = \sum_{u, l} P(U)P(L\ |\ U)P(Y\ |\ U, L, A=a)\]

Making use of the fact that
\(P(L, U) = P(L\ |\  U)P(U) = P(U\ |\ L)P(L)\) and summing over either W
and U or W and L yields

\[P(Y\ |\ do(A=a) = \sum_{l}P(Y\ |\ L, A=a)P(L) = \sum_{u} P(Y\ |\ U, A=a)P(U))\]

If we can find suitable forms for either \(P(Y\ |\ L, A=a)\) and
\(P(L)\) or \(P(Y\ |\ U, A=a)\) and \(P(U)\) that correspond to the MSM
\(P(Y\ |\ do(A=a)\), then, given suitable values for \(A, L, U\) it will
be possible to simulate from the chosen MSM.

Choosing a functional form for \[P(Y\ |\ do(A=a)\] depends on
convenience. We need a functional form that can be easily represented by
\(P(Y\ |\ L, A=a)P(L)\). non-linear functions will be hard to work into
the analysis.

U \textasciitilde{} U{[}0, 1{]} is a good choice because we can usethe
CDF of Y because U{[}0, 1{]} is always between 0 and 1

General health is patient specific but comes from a clear distribution
and has a nice medical interpretation. In contrast L would be more
difficult to include. It is better as a function of U than a value in of
itself.

\begin{itemize}
\tightlist
\item
  Explain issue that survival models are not collapsible which is why
  most algorithms don't work. Big reason we choose HD2012 is because of
  this
\item
  no model mispecification in the HD2012 algorithm
\item
  stay on treatment after treatment starts
\item
  they motivate a logistic model for the haxard function, they use a
  discrete equivalent to the hazrd function (link to citation about
  farington study.)
\item
  treatment regime is determined by t* (starting point of treatment
  because it is a vector of \{0, 0, 0, 1, 1, 1\}
\item
  Explain why we can introduce positivity in this algorithm but not in
  others as easily.
\end{itemize}

    \section{Violations of Positivity}\label{violations-of-positivity}

The motivation for using the algorith of \citet{Havercroft2012} is that
we have control over how \(L\) affects \(Y\) whereas in other algorithms
\(L\) would be fixed in advance., so we can itroduce positivity using a
threshold. In other algorithms there would be a direct link between
\(L\) and \(Y\), this would be a problem because altering treatment
decisions based on \(L\) would affect \(Y\) directly.\\
- creating an artificial population in which positivity is violated in
specific ways.

\subsection{Extended discussion of algorithm linking to
positivity}\label{extended-discussion-of-algorithm-linking-to-positivity}

As described in the introduction, one assumption of the model is that
there is a non-zero probability of the event occuring at every startum
of the covariate.

\begin{itemize}
\tightlist
\item
  When previous covariates like CD4 count are strongly associated with
  treatment the probabilities in the denominator of the ustabilized
  weights may vary greatly. Because we are foricing positivity by using
  a treatment rule when L falls below a threshold and A is then eaual to
  one, we create a strong association between A and L -\textgreater{}
  hence the unstabilized weights would vary. (Robins et al 2000 pp. 553)
\item
  present the algorithm again with positivity violations.
\end{itemize}

\subsection{Simulation scenarios}\label{simulation-scenarios}

\paragraph{thresholding}\label{thresholding}

\paragraph{percentage of compliant
doctors.}\label{percentage-of-compliant-doctors.}

\paragraph{propogation through time, longer time
periods}\label{propogation-through-time-longer-time-periods}

\begin{itemize}
\tightlist
\item
  table of weights mean, min, max at T = 5, 10, 15 etc. plot with bias
  against time.
\end{itemize}

    \section{Simulation study}\label{simulation-study}

\subsection{Data Structure}\label{data-structure}

Include an example simulation graph plot showing the data colored to
show where positivity would arise.

\subsection{Number of positivity compliant
doctors.}\label{number-of-positivity-compliant-doctors.}

\subsection{Varying levels of
threshold.}\label{varying-levels-of-threshold.}

We wish to simulate survival data in discrete time \(t = 0, \dots, T\)
for \(n\) subjects. At baseline \(t=0\) all subjects are assumed to be
at risk of failure so that \(Y_0 = 0\). For each time period
\(t = 0, \dots, T\) a subject may either be on treatment, \(A_t = 1\),
or not on treatment, \(A_t = 0\). All patients are assumed to be not on
treatment before the study begins. Once a patient commences treatment,
they remain on treatment in all subsequent periods until failure or the
end of follow-up. In each time period \(L_t\) is the value of a
covariate measured at time \(t\). In the simulated data, \(L_t\) behaves
in a similar manner to CD4 counts such that a low value of \(L_t\)
represents a more severe illness and hence a higher probability of both
tratemnt and failure in the following period. In addition to \(L_t\),
the variable \(U_t\) represents subject specific general health at time
\(t\).

Each time period is either a check up visit or is between two check up
visits. If \(t\) is a check-up visit and treatment has not yet
commenced, \(L_t\) is measured and a decision is made on whether to
commence treatment. Between visits, treatment remains unchanged at the
value recorded at the previous visit. Similarly, \(L_t\) which is only
measured when \(t\) is a visit, alos remains unchanged.

We represent the history of a random variable with an over bar. For
example, the vector representing the treatment history of the variable A
is represented by \(\bar A = [a_0, a_1, \dots, a_m]\) where \(m=T\) if
the subject survives until the end of follow-up, or \(m < T\) otherwise.
Prior to basline both \(A = 0\) for all subjects.

\begin{itemize}
\tightlist
\item
  explain what \(U\) is and how it relates to the simulation
  design/algorithm
\item
  Be more specific on \(Y\)
\item
  L\_t is a measured confounder
\item
  U\_t is an unmeasured confounder.
\end{itemize}

\subsection{Simulation Algorithm}\label{simulation-algorithm}

\subsubsection{Algorithm}\label{algorithm}

Next, we describe the algorithm used to simulate data from our chosen
marginal structural model under time dependent confounding. In the
following section we discuss in detail how the algorithm works and the
salient features for this thesis. The algorithm is taken from
\citet{Havercroft2012} who generate data on \(n\) patients, for \(k\)
time periods. The outer loop in the following algorithm
\(i \in {1, \dots, n}\) , refers to the patients while the inner loop
\(t \in {1, \dots, T}\) refers to the subsject specific time periods
from baseline to failure or the end of the study. There will be at least
one, and at most \(T\) records for each patient.

Within the inner loop (\(t \in {1, \dots, T}\)) we see that the data is
only updated at time \(t \neq 0\ (mod\ k)\), where \(k\) refers to
evenly spaced check-up visits. If \(t\) is not a check-up visit the
values of \(A_t\) and \(L_t\) are the same as in \(t-1\). When \(t\) is
a visit \(A_t\) and \(L_t\) are updated.

\begin{itemize}
\tightlist
\item
  if treatment has been commenced then a subject may feel extra benefit
  if more time has elapsed since treatment began
\item
  L\_t affects A\_t and also Y\_t
\item
  explain starting values for A and Y are all zero (except L maybe)
\end{itemize}

In order to operationalize the Algorithm 1 we need to choose parameters
for \(()\). In their paper \citet{Havercroft2012} use values that
simulate data with a close resemblance to the Swiss HIV Cohort Study. We
postpone disussion of the patameters in Algorithm 1 to section 2.4. We
just need to state that we follow their parameters because this is not
the focus of this thesis.

\subsubsection{Discussion of how algorithm
works}\label{discussion-of-how-algorithm-works}

The algorithm of \citet{Havercroft2012} works by factorizing the joint
density of the histories of the four variables in the analysis.

\begin{itemize}
\tightlist
\item
  Important is that the form of the MSM is not specified intil the last
  stage
\item
  role of \(U_{0, i}\)
\item
  How does positivity enter the analysis?
\item
  Why this model is important in terms of positivity.
\end{itemize}

\subsection{Constructing IPT weights}\label{constructing-ipt-weights}

Inverse Probability of Treatment weights can be used to adjust for
measured confounding and selection bias in marginal structural models.
Link back to pseudo population idea in previous section. This method
relies on four assumptions consistency, exchangeability, positivity and
no mispecification of the model used to estimate the weights
\citet{Cole2008}. Unstabilized weights are defined as:

\[w_{t,i} = \frac{1}{\prod_{\tau=0} ^ t p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i}, \bar L_{\tau, i})}\]

Where the denominator is the probability that the subject received the
particular treatment history that they were observed to receive up to
time \(t\), given their prior observed treatment and covariate histories
(Havercroft, Didelez, 2012). The probabilities
\(p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i}, \bar L_{\tau, i})\) may
vary greatly between subjects when the covariate history is strongly
asscoaited with treatment. In terms of the resulting pseudopopulation,
very small values of the unstabilized weights for some subjects would
result in a small number of observations dominating the weighted
analysis. The result is that the IPTW estimator of the coefficients will
have a large variance, and will fail to be normally distributed. This
variability can be mitigated by using the following stabilized weights

\[sw_{it} = \frac{\prod_{\tau=0} ^ t p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i})} {\prod_{\tau=0} ^ t p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i}, \bar L_{\tau, i})}\]

In the case that there is no confounding the denominator probabiliies in
the stabilized weights reduce to
\(p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i})\) and \(sw_{it}=1\) so
that each subject contributes the same weight. In the case of
confounding this will not be the case and the stabilized weight will
vary around 1.

In practice, we estimate the weights from the data using a pooled
logistic model for the numerator and denominator probabilities. The
histories of the treatment and covariates are included in the
probabilities. In practice Specifically, following Havercroft and
Didelez (2012), we estimate the model where the visit is only the visits
every check up time. Between check ups both the treatment and covariate
remain the same. Other ways of doing this include a spline function over
the months to create a smooth function between the visits. Another
difference might be to use a coxph function instead of logistic function

\[logit\ p_{\tau} (A_{\tau, i}\ |\ \bar A_{\tau-1, i}, \bar L_{\tau, i}) = \alpha_0 + \alpha_1 k + \alpha_2 a_{k-1} + \dots + \alpha_k a_0 + \]

We have several options for estimating these weights. We could use a
coxph model, or a logistic model.

\subsection{Simulation Set-up}\label{simulation-set-up}

We follow the simulation set-up of Havercroft, Didelez (2012) which is
based on parameters that closely match the Swiss HIV Cohort Study
(HAART).

\subsection{Results}\label{results}

\begin{itemize}
\item
  check the distribution of the weights that come out of the model (see
  Cole 2008). This would allow us to see weight model mispecifications.
  Not a problem in the simuation case.
\item
  compare the bias, se, MSE, and 95\% confidence interval
\item
  compare all of these in the positivity violation and non-positivty
  violation case.
\item
  explain to some extent monte-carlo standard error.
\item
  we don't confirm the results of the havercroft of Bryan papers,
  instead refer readers to these papers to see how IPTW outperforms the
  naive estimators.
\item
  Explain why we use MSE or other measures to assess simulation results.
\end{itemize}

    \section{Discussion and Conclusion}\label{discussion-and-conclusion}

The focus of this thesis was the effect of positivity violations on

\subsection{Limitations}\label{limitations}


    % Add a bibliography block to the postdoc
    
    
\bibliographystyle{plainnat}
\bibliography{references/thesis}

    
    \end{document}
