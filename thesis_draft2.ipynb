{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Abstract\n",
    "\n",
    "In this thesis we study the performance of the inverse probability of treatment weighted (IPTW) for marginal structural models under violations of the positivity assumption. To our knowledge the performance of MSMs under violations of this assumption have not been systematically studied in the litrature. We employ the simulation algorithm of \\citet{Havercroft2012} to generate data from a typical longitudinal setting. By exploiting the design of this algorithm it is possible to introduce positivity violations that are propagated through patient histories and study the effect of these violations on the ability of a marginal structural model to recover the true parameters. Our results suggest that even small violations of the positivity assumption can have a large impact on the performance of the estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "\\tableofcontents{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# Section 1: Introduction\n",
    "\n",
    "Marginal structural models (MSMs) are a popular class of models for performing causal inference in the presence of time dependent confounders. These models have an important application in areas of research such as epidemiology, social sciences and economics where randomised trials are prohibited by ethical or financial considerations, and hence confounding cannot be ruled out by randomization. Under these circumstances confounding can obscure the causal effect of treatment on outcome. An example of this, common in epidemiological studies, occurs when prognostic variables inform treatment decisions while at same time being predictors of the outcome of interest. In a longitudinal setting this is further complicated when the confounder itself is determined by earlier treatment. One consequence is that regression adjustment methods do not control for confounding in the longitudinal case and other techniques are required. A second consequence is that simulating data from a specific marginal structural models is more challenging when the data is to exhibit time dependent confounding.\n",
    "\n",
    "The Inverse probability of treatment weighting (IPTW) estimator is a technique that has been applied to censoring, missing data and survey design problems. The central idea is that by weighting the observed data, a pseudo population is constructed in which tratemnt is assigned at random. Subsequent analysis where we ignore the confounder is then possible. which inference on the target population can be achieved. For example, when there is missing data weights can be used to create a pseudo-population in which there is no missingness. In the context of MSMs, the IPT weights relate to a pseudo-population in which there is no longer any confounding between the confounder and treatment and causal inferences can be made. \n",
    "\n",
    "Underlying IPTW technique for estimating causal effects are four assumptions: 1) consistency 2) exchangeability 3) positivity 4) and correct model specification. Exchangeability, also known as the no unmeasured confounding assumption,  is closely linked to causality??##  Several studies have considered violations of exchangeability and corrected model specification. Positivity has received less attention because in typical observational study positivity violations are not suspected. In the clinical context that we consider, protocols threaten to violate the positivity assumption and we investigate whether MSMs are robust against positivity. The focus of this thesis will be on violations of the positivity assumption. Positivity means that within every strata spanned by the confounders, there must be a positive probability of patients being exposed or unexposed to treatment. For example, in a medical context, if treatment protocols demand that treatment is initiated whenever a prognostic variable falls below a pre-defined threshold, there will only be exposed and no unexposed patients in this strata of the confounding prognostic variable.  make decisions based on protocols positivity can be. In the absence of structural positivuty violations, there is always the threat that random zeroes arise in some strata of the confounder especially when the sample size is small or the number of confounding variables is large. In each case the sparsity of data within the strata of the confounder results in a high chance that positivity is violated. Positivity violations increase the bias and variance of estimates of the causal effect but the extent of the damage is not well known. The central aim of this thesis will be to investigate positivity violations when fitting MSMs to longitudinal data. To our knowledge positivity violations have not been systematically studied in the literature from a simulation point of view. We quantify the bias and variance introduced due to positivity violations and hope to provide practical advice to researchers tempted to fit MSMs to overcome confounding without realising the potential consequences of positivity violations in their data.\n",
    "\n",
    "Throughout this thesis we focus on clinical applications as examples. In the literature on marginal structural models the causal effect of Zidovudine on the survival of HIV positive men is often cited as an example. In this example a patients white blood cell (CD4) count is a prognostic variable that influences a doctor's decision to initiate treatment while at the same time being a predictor of survival. As a result CD4 count is a confounder. In the longitudinal setting previous treatments influence CD4 count. As such studies often depend on protocols which means that poistivity in some levels of the confounder make this a suitable example for our purposes. \n",
    "\n",
    "The structure of this thesis is as follows. In section 2 of part 1, the model considered in this thesis and its important aspects are explained. In part 2 simulating from this statistical model is discussed in detail. In part 3 the model under dynamic strategies is considered and comparisons are drawn with the static case. In part 4 we entertain violations of positivity in the data, this section represents the novelty in this thesis. Part 5 conducts a simulation study. Part 6 includes a discussion, conclusions and suggestions for future work.\n",
    "\n",
    "Cole et al have discussed the role of positivity underlying the IPTW method but a limitation of these paper is that they lack a formal exploration through simulation of the effects this can have. Here we explore formally different levels of the violation and offer practical advice to researchers who suspect this as an issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed Acyclic Graphs: graphical representations of causality\n",
    "\n",
    "In this thesis we consider simulating from marginal structural models (MSMs), a class of models that can be used to estimate causal effects from observational data under time dependent confounding. Specifically, we consider the problem, common in medical research, of investigating the relationship between a binary treatment variable $A$ and an outcome of interest $Y$ in the presence of a covariate $L$. One way of expressing the relationship between these three variables is through a graph such as the one shown in the left hand size of figure 1.\n",
    "\n",
    "A graph consists of a finite set of vertices $\\nu$ and a set of edges $\\epsilon$. The vertices of a graph correspond to a collection of random variables which follow a joint probability distribution $P(\\nu)$. Edges in $\\epsilon$ consist of pairs of distinct vertices and denote a certain relationship that holds between the variables \\citet{Pearl2009}. The direction of the relationship is denoted by an arrow and in this thesis we consider only acyclic graphs which means that the relationship between two variables only proceeds in one direction, there are no feedback loops or mutual causation. The resulting directed acyclic graph (DAG) $G = (\\nu, \\epsilon)$ shown in figure 1 can be represeted as a set of connections ${(A, Y), (L, Y)}$ where direction is from the first element of each pair to the second element. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Dependent Confounding\n",
    "\n",
    "The single time case of confounding is represented in figure 1 where $L$ affects both $A$ and $Y$. A model of $Y$ in terms of $A$ such as a regression model where $Y$ is regressed against $A$ will not resolve the correct causal parameters because $A$ is confounded by $L$. If $A$ and $L$ are independent then this is not an issuethat ignores $L$ will notIf we ignore $L$ from the analysis and model $Y$ in terms of $A$ we will not get the true c\n",
    "The right hand side of figure 1 represents the confounding case where the DAG $G$ has been ammended to include a relationship from $L$ to $A$ $G = {(A, Y), (L, Y), (L, A)}$. This confounding relationship precludes causal inference because we cannot discern the relationship between treatment $A$ on outcome $Y$ when $L$ is a common cause of both. If $L$ is the only confounder and there are no unmeasured confounders, then it is possible to correctly estimate the causal relationship between $A$ and $Y$ by adjusting for the measured confounders. \n",
    "\n",
    "time dependent confounding in the set-up we consider, will always introduce  bias. In the time dependent case, we will therefore always have bias. And IPT weighting a means of avoiding this.\n",
    "\n",
    "\n",
    "The time dependent case adds another complication which is a further relationship between previous treatment $A_{t-1}$ and $L_t$ as shown in figure 2.\n",
    "\n",
    "Talk about the minimum required to deal with the time dependent case. Data on all time independent and time dependent variables. correct model specification. Explain why time dependent as opposed to time independent (baseline - do not change over time) variables are more of a problem. \n",
    "\n",
    "By blocking the pathway between A -> L -> Y at L, we get distorted estimates of the effect of treatment on outcome.\n",
    "\n",
    "In the one-shot case the outcome $Y$ depends on the treatment decision, measured covariates and unmeasured covariates. In the longitudinal case, the outcome depends on the histories of the treatment and covariates. One complication is that current values of the covariate may depend on previous treatments, and previous treatments may, in turn, depend on previous covariates. If treatment is succesful this will inform future treatments and also affect the outcome of interest $Y$. While in the one-shot case in figure 1 regression adjustment using \n",
    "\n",
    "Allow for the joint determination of outcomes and treatment status or omitted variables related to both treatment status and outcomes (Angrist 2001).\n",
    "\n",
    "Similarily, if treatment is succesful this will affect both the outcome and subsequent treatments. Analogous to the one-shot case, $P(y\\ |\\ \\bar A= \\bar a) \\ne P(y\\ |\\ do(\\bar A = \\bar a))$. Regression adjustment will always introduce other sources of bias in the time dependent case. As a result regression adjustment does not work. Only the IPTW method adjust for selection bias and confounding. \n",
    "\n",
    "A covariate $L$ is a confounder if it predicts the event of interest and also predicts subsequent exposure. Explain how this actually happens, as U0 is a common ancestor of A through L and also Y, also that there is selection bias, and L is sufficient to adjust for confounding see Havercroft algorithm code page bottom.\n",
    "\n",
    "The reason why U is important is because with U, we can obtain any P(Y\\ |\\ do(a))$ using the inverse of U?? We can do this under any counterfactual survival path.\n",
    "\n",
    "Explain why there will always be selection bias, and hence there will always be a form of confounding in a longitudinal model. Talk about conditioning on Y in a collider setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal structural models\n",
    "\n",
    "This section should have more on general idea, what they are and examples highlighting what they are.\n",
    "\n",
    "Specify explictly that marginal structural models are used for estimating average causal effects.\n",
    "\n",
    "define what a causal effect is and how we go from the individual to the population and that we cannot in general identify individual causal effects due to a missing data problem. Let the assumptions behind this extend from the meaning of causal effects, for example the exchangeability assumption.\n",
    "\n",
    "Marginal structural models are a class of models which can be used to estimate causal effects in the presence of time dependent confounding. Specifically we are interested in the causal effect of a treatment A on an outcome Y. A causal model differs from an associative model and associative models like regression models will not have a causal interpretation when time dependent confounding is present, nor will naive methods of adjustment result in a causal interpretation. When time dependent confounding is present, associative models such as regression models do not have a causal effect.\n",
    "\n",
    "- independence between treatment and prognostic factors/covariates.\n",
    "- comparison with the random treatment paradigm.\n",
    "- If treatment at each time $t$ is assigned completely at random then treatment will be independent of both measured and unmeasured confounding \n",
    "- At the individual level we cannot establish causal effects because of a missing data problem. Namely that we do not see the counterfactual outcomes. Link in literature about all problems being missing data problems.\n",
    "- The missing data idea could  be shown with an example table of results of people under treatment and not under treatment.\n",
    "\n",
    "They model the marginal distribution of counterfactual variables over any covariates and are referred to as structural because in econometric and social sciences literature the term structural is often used in place of causal \\citet(hernan, brumback and robins). \n",
    "\n",
    "\n",
    "An oft cited observational outcome in statistics without a causal relationship is umbrella sales and the probability of rain. A counterfactual outcome is broader, it encompasses not only the outcome when umbrella sales are purchased in high volumes but also when they are not. Intuitvely the connection between counterfactuals and     A counterfactual variable for $Y$ is the random variable consisting of a subjects outcome under a $A$ regardless of whether the subject actually received this treatment or not. Juxtaposed to an observational outcome, a counterfactual outcome ... correspondingly, the probability    recorded for a subject. In the observational case, given the treatment and . Within the context of a MSM, the focus is the causal relationship that exists between treatment and outcome. The following from \\citet{Pearl2009} defines a causal relationship.    \n",
    "\n",
    "#### Definition 1\n",
    "\n",
    "(Def 3.2.1 from \\citet{Pearl2009}, abridged) Given two disjoint sets of variables, A and Y, the causal effect of $A$ on $Y$, denoted as $P(y\\ |\\ do(A=a))$, is a function from X to the space of probability distributions on $Y$. For each realization $a$ of $A$, $P(y\\ |\\ do(A=a))$ gives the probability that $Y=y$ induced by deleting from the model all equations corresponding to variables in $A$ and substituting $a$ into the remaining equations.\n",
    "\n",
    "Associative models like regression models model the conditional distribution $P(Y\\ |\\ A=a, L=l)$\n",
    "\n",
    "Need a better description of do notation and how it differs to conditional case and how it relates to counterfactuals\n",
    "\n",
    "See Robins etal 2000 for explanantion of why they are called marginal\n",
    "\n",
    "explain counterfactual and how we model it (this is what is missing and need to connect to graph)\n",
    "\n",
    "marginal because they model the marginal distribution of the counterfactual variables. Not marginal over covariates??\n",
    "\n",
    "$E(Y\\ |\\ do(a)) = \\beta_0 + \\beta_1a$ contains no $l$ term.\n",
    "\n",
    "Non collapsible models and simulation\n",
    "\n",
    "Have been used for missing data problems. see pp.442 of Hernan, Brumback, Robins 2001 for a list of papers linked to this \n",
    "\n",
    "structural is only in the MSM name because structural is used in econometrics and social science. Structural here only means that it is a causal model.\n",
    "\n",
    "Counterfactuals, breeze through this ...\n",
    "\n",
    "A MSM  When a relationship exists between $L$ and $A$ as in the right hand side of figure 1 then there is a confounding relationship and associative models will not have a causual interpretation. The following definition from \\citet{Pearl2009} defines what is meant by a causal effect:\n",
    "\n",
    "An example of this definition involves deleting the link between L and A in the diagram.\n",
    "\n",
    "An associative model like a regression model.\n",
    "\n",
    "When treatments are randomised this is not a problem This might arise, for example, when the decision to instigate treatment is based on a measured covariate $L$. Contrast this to randomized controlled trial, where due to the randomization process treatment will be independent of any measured or unmeasured covariates. The randomized controlled trial does not often arise in epidemiology, social sciences or econometrics due to prohibitive ethical or financial concerns. As a result other methods are sought in order to uncover the causal effects.  Marginal structural models are a class of models which make it possible to estimate the causal effect of $A$ on $Y$ in the presence of confounders. \n",
    "\n",
    "- still have unmeasured confounders of course. Or model mispecification. Either of these mean that results no longer hold.\n",
    "\n",
    "A model that parameterises $P(Y\\ |\\ do(A=a))$ is called a marginal structural model (MSM) as it is marginal over any covariates and structural in the sense that it represents an interventional rather than observational model. The distinction between an interventional and observational model is expressed in definition 1.1 through the notation $do(a)$ in $P(y\\ |\\ do(x))$, as opposed to the observational case $P(y\\ |\\ A=a)$. \n",
    "\n",
    "marginal structural models are usually for counterfactuals -> example -> straight through to the IPW not too much on counterfactuals.\n",
    "\n",
    "Randomization ensures that missing values occur by chance. So the counterfactual values that we don't see for some observations are missing randomly and not due to confounding through a covariate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% treatment and outcome with one covariate %\n",
    "\n",
    "\\begin{figure}\n",
    "\\centering\n",
    "\n",
    "\\begin{minipage}{.2\\textwidth}\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\n",
    "\\tikzstyle{every node}=[font=\\Huge]\n",
    "\n",
    "% nodes %\n",
    "\\node[text centered] (a) {$A$};\n",
    "\\node[right = 2.5 of a, text centered] (y) {$Y$};\n",
    "\\node[above= 2.5 of a, text centered] (l) {$L$};\n",
    "\n",
    "% edges %\n",
    "\\draw[->, line width= 1] (a) --  (y);\n",
    "\\draw [->, line width= 1] (l) -- (y);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "\n",
    "\\end{minipage}\n",
    "\\hspace{3cm}% NO SPACE!\n",
    "\\begin{minipage}{.2\\textwidth}\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\n",
    "\\tikzstyle{every node}=[font=\\Huge]\n",
    "\n",
    "% nodes %\n",
    "\\node[text centered] (a) {$A$};\n",
    "\\node[right = 2.5 of a, text centered] (y) {$Y$};\n",
    "\\node[above= 2.5 of a, text centered] (l) {$L$};\n",
    "\n",
    "% edges %\n",
    "\\draw[->, line width= 1] (a) --  (y);\n",
    "\\draw [->, line width= 1] (l) -- (y);\n",
    "\\draw [->, line width= 1, dashed] (l) -- (a);\n",
    "\n",
    "\\end{tikzpicture}\n",
    "\n",
    "\\end{minipage}\n",
    "\\caption{Figure 1 DAG} \\label{fig:fig2}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of this distinction is  the effect of Zidovudine on the survival of HIV-positive men. Of interest is the effect that Zidovudine has on survival. This is what we want to estimate and it is captured in the DAG in figure 1 as the edge connecting A and Y.\n",
    "\n",
    "importantly, changing the relationship between L and A, won;t change the relationship between L and Y. This means that an intervention in $A$ does not affect the relationship between $L$ and $Y$. So we remove the link between $L$ and $A$ and assign to $A$ the value of treatment on or off. Once we place the patient on treatment, regardless of the relationship which had existed before hand between the covariate and treatment, a new relationship between $A$ and $Y$ exists in which the covariate has no say. \n",
    "\n",
    "This distinction can be understood more clearly when comparing randomized controlled trials in medical resarch and observational epidemiological research. In randomised trials the    In the presence of confounding $P(y\\ |\\ A=a)$ will not represent the true causal effect of $A$ on $Y$ why?. The interventional case  $P(y\\ |\\ do(A=a))$, on the otherhand,  $\\ne P(y\\ |\\ do(x))$ with the result that a model which is conditional on A will not represent the true causal effect of A on Y. In the former case the system is changed such that variable A takes on a particular value or history, the result represents the system under the intervention $do(A=a)$ (pearl 2010). On the other hand the first method represents the situation where the model is unchanged and $A$ is observed to be $a$. This distinction will be particularly important when considering static and dynamic stratgeies in subsequent sections. \n",
    "\n",
    "\n",
    "From the point of view of simulating from a marginal structural model, In a nutshell, we need a simulation algorithm that allows us to simulate discrete time longitudinal data in which A affects Y, and so does L and L affects A. The most direct way of doing this would be the red line in the DAG. But this won't do in our case. The Havercroft and Didelez algorithm is useful because it allows us to break the relationship between L and A. We need to do this in such a way that we do not change the relationship between Y and L, although we are not so interested in this relationship because it is the least interesting of the relationships in the diagram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Probability Weighting \n",
    "\n",
    "Inverse probability of treatment weighting is a technique that re-weights subject observations to a population where assignment of treatment is at random. An early example of this technique is the \\citet{Horovitz1952} weighted estimator of the mean. In the context of marginal structural models, a weight is calculated for each subject which can be thought of informally as the inverse of the probability that a subject receives their own treatment \\citet{Robins2000}. The result of applying these weights is to re-weight the data to create a pseudo-population in which treatment is independent of measured confounders \\citet{Cole2008}. The resulting pseudo population no longer has a time dependent confounding problem and the causal relationship between A and Y can be estimated using naive methods like regression without adjusting for confounding because it is no longer present. Crucially, in the pseudo population the counterfactual probabilities are the same as in the true study population so that the causal RD, RR or OR are the same in both populations \\citet{Robins2000}.\n",
    "\n",
    "$$w_{t,i} = \\frac{1}{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "stabilized weights\n",
    "$$sw_{it} = \\frac{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i})} {\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "The use of IPTW is valid under the four assumptions of consistency, exchangeability, positivity and no misspecification of the model \\citet{cole2008}. \n",
    "\n",
    "Informally a patients weight through visit k is proportional to the inverse of the probability of having her own exposure history through visit k (Cole and Hernan 2008)\n",
    "\n",
    "In previous simulation studies unstabilized weights show substantial increase in SEs\n",
    "\n",
    "Explain why this method does appropriately adjust for **measured time varying** confounders affected by prioir exposure.\n",
    "\n",
    "The weight is informally proportional to the participants probability of receiving her own exposure history\n",
    "\n",
    "As these weights have high instability we need to stabilize them. The unstabilized weights can be driven by only a small number of observations.\n",
    "Under time dependent confounding it may still be possible to recover the causal effect of $A$ on $Y$ by the method of Inverse Probability of Treatment (IPT) weihting. How does this work?\n",
    "\n",
    "- true weights are unknown but can be estimated from the data.\n",
    "- Robins(2000) - when there are no unmeasured confounders, we can control for confounding using weights\n",
    "- weighting adjusts for confounding and selection bias due to mesured time varying covariates affected by prior exposure.\n",
    "- $A_t$ is no longer affected by $L_t$, and crucially the causal effect of $\\bar A$ on $Y$ remains unchanged\n",
    "- quantify the degree to which treatment is statistically non-exogenous through month $t$\n",
    "\n",
    "\n",
    "lack of adjustment for L precludes unbiased estimation. This is because it introduces selection bias.\n",
    "\n",
    "Problem is that the weights will have very high variability, so we need to stabilize them otherwise they will affect the estimates. Explain all the little details about why it becomes more variable etc.\n",
    "\n",
    "No matter whether we use stabilized or unstabilized weights, under positivity violations the weights will be undefined.\n",
    "\n",
    "Conditions under which IPTW work are largely untestable (westreich 2012)\n",
    "\n",
    "Be more specific about what is contained in the weights. The denominator depends on the measured confounders $L$ the numerator does not, but could contain baseline coefficients in the numerator to help stabilize the weights.\n",
    "\n",
    "weighted regression and MSM are equivalent.\n",
    "\n",
    "L0 and baseline values - make sure that B = baseline and L0 are either together, or have them clearly split.\n",
    "V is a variable predictive of A but not of Y - hence not a confounder\n",
    "No rebalancing the baseline covariates like gender or age (B), so if we want to see the effect of groups of basline \n",
    "If we do not condition on baselines like age then we break the link between age and treatment. So baselines must be in the condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating from a MSM\n",
    "\n",
    "In this section we discuss the simulation algorithm of \\citet{Havercroft2012}. which is later used to explore the affects of positivity violations on the IPTW method. In particular we describe how this algorithm relates to the time dependent confoudning described above, and how it allows us to capture the observational nature of the data while still permitting simulating from a given MSM. \n",
    "\n",
    "How does selection bias relate to $U$?\n",
    "\n",
    "Explain why the algoritm allows positivity to be propagated throught the patients history.\n",
    "\n",
    "The simulating procedure needs to allow us to specifiy a particular MSM, while at the same time\n",
    "capturing the characteristics of observational data. \n",
    "\n",
    "Problem statement: Simulating from a given marginal structural model is challenging because a direct relationship between L and Y would ...\n",
    "\n",
    "Problem is that conditional distributions one might use to draw the simulated data are not compatible with the desired properties of the marginal model. \n",
    "\n",
    "1. derive relationship between MSM and DAG and the correct conditional distributions. Follows from truncated factorisation why we can et P(Y|do(a))\n",
    "\n",
    "1. simulating from a chosen MSM\n",
    "2. creating a confounding relationship\n",
    "3. only have the data and observed outcomes, not the counterfactuals.\n",
    "4. Fixing relationships.\n",
    "5. think of this process as if we had fixed a treatment vector in advance. consistency assumption.\n",
    "6. Relationship between A and L should be observational and a confounding relationship\n",
    "7. U is breaking the relationship between L and Y, important for positvity.\n",
    "8. Don't care about the relationship between L and Y\n",
    "9. HD 2012, with Pearl and truncated factorization formula, show that it is possible to link the counterfactual represented by $P(Y\\ |\\ do(a))$ to observattional data generated in an observational way. But the problem arises when the model is non-collapsible or non-linear.\n",
    "\n",
    "In the one shot case we set A = 1/0 because we are interested in the outcome under either of these treatment scenarios. In the time dependent case, A is a vector of 0s and 1s and we want to pretend that we decide in advance that the whole vector A is specified. But A and L have a complex interplay in an observational setting. So we want to pretend that A (a vector of 1s and 0s) is set in advance but at the same time have the observational structure for A and L. \n",
    "\n",
    "The relationship between Y and L is then dependent on A. There is no relationship between A and U because of the set-up in the DAG. The variable L blocks this relationship.\n",
    "\n",
    "In their paper \\citet{Havercroft2012} develop an algorithm that allows simulating data that corresponds to a particular parameterisation of an MSM. This algorithm provides the bedrock of the simulation structure considered in this thesis. Figure 1 represents the system under consideration. The DAG in figure 1 represents the one-shot non-longitudinal case. Factorising the joint distributions of the variables in figure 1 yields\n",
    "\n",
    "$$P(U,\\ L,\\ W,\\ A,\\ Y) = P(W)P(U)P(W)P(L\\ |\\ U)P(A\\ |\\ L,W)P(Y\\ |\\ U,A)$$\n",
    "\n",
    "Where, following definition 1.1 we delete $P(A\\ |\\ L,W)$, a probability function corresponding to $A$, and replace $A=a$ in all remaining functions\n",
    "\n",
    "$$ P(U, L, W, Y\\ |\\ do(A=a)) =\n",
    "  \\begin{cases}\n",
    "    P(U)P(L\\ |\\ U)P(Y\\ |\\ U,A = a) & \\quad \\text{if } A = a\\\\\n",
    "    0  & \\quad \\text{if } A \\neq a\\\\\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "The goal is to simulate from a particular MSM. This means parameterising $P(Y\\ |\\ do(A=a))$. Applying the law of total probability over $W$, $U$ and $L$ yields\n",
    "\n",
    "$$P(Y\\ |\\ do(A=a) = \\sum_{w, u, l} P(W)P(U)P(L\\ |\\ U)P(Y\\ |\\ U, L, A=a) = \\sum_{u, l} P(U)P(L\\ |\\ U)P(Y\\ |\\ U, L, A=a)$$\n",
    "\n",
    "Making use of the fact that $P(L, U) = P(L\\ |\\  U)P(U) = P(U\\ |\\ L)P(L)$ and summing over either W and U or W and L yields\n",
    "\n",
    "$$P(Y\\ |\\ do(A=a) = \\sum_{l}P(Y\\ |\\ L, A=a)P(L) = \\sum_{u} P(Y\\ |\\ U, A=a)P(U))$$\n",
    "\n",
    "If we can find suitable forms for either $P(Y\\ |\\ L, A=a)$ and $P(L)$ or $P(Y\\ |\\ U, A=a)$ and $P(U)$ that correspond to the MSM $P(Y\\ |\\ do(A=a)$, then, given suitable values for $A, L, U$ it will be possible to simulate from the chosen MSM.\n",
    "\n",
    "use the phrase, \"in words ....\" to make something more understandable.\n",
    "\n",
    "Choosing a functional form for $$P(Y\\ |\\ do(A=a)$$ depends on convenience. We need a functional form that can be easily represented by $P(Y\\ |\\ L, A=a)P(L)$. non-linear functions will be hard to work into the analysis.\n",
    "\n",
    "U ~ U[0, 1] is a good choice because we can usethe CDF of Y because U[0, 1] is always between 0 and 1\n",
    "\n",
    "General health is patient specific but comes from a clear distribution and has a nice medical interpretation. In contrast L would be more difficult to include. It is better as a function of U than a value in of itself.\n",
    "\n",
    "### collapsibility\n",
    "\n",
    "Collapsibility means there is no incompatibility between the marginal model and the conditional distributions used to simulate the data. Provide example of this. Explain how this affects the simulation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positivity\n",
    "\n",
    "3. explain that it has not been thoroughly studied. Cite the examples where it has been. The cole papers but also two empirical applications.\n",
    "3. highlight times and examples in which it may arise\n",
    "4. explain its importance to the weights in IPTW\n",
    "5. explain why you must trade-off positivity with more precise confounder control\n",
    "6. What happens (intuitivley) when the average causal effet cannot be estimated in every level of the strata?\n",
    "7. deterministic violations of positivity like protocols versus random violations. What is more dangerous.\n",
    "8. The positivity assumption must hold in general, but there is an extra problem for the weights because the denominator will be zero.\n",
    "\n",
    "The final assumption underlying MSMs, and the central topic of this thesis, is the positivity assumption. MSMs are used to estimate average causal effects in the study population, and one must therefore be able to estimate the average causal effect in every subset of the population defined by the confounders \\citet{Cole2008}. The positivity assumption requires that there be exposed and unexposed subjects in every strata of the confounding covariates. For example, when treatment is Zidovudine, there must be a positive probability of some patients being exposed and unexposed at every level of CD4 count. Positivity can be expressed formally in the one time period case as $Pr(A=a\\ |\\ L) > 0$ for all $a \\in A$, which extends straightforwardly to the time dependent case where the positivity assumption must hold at every time step conditonal on previous treatment, time dependent confounders and any baseline covaraiates:\n",
    "\n",
    "$$Pr(A_{it}=a_{it}\\ |\\ L_{it}, A_{i, t-1}, V_{i0}) > 0$$\n",
    "\n",
    "Models for risk are commonly used in epidemiology and for a dichotomous outcome, binary treatment and covariate L often take the form\n",
    "\n",
    "$$P(Y=1\\ |\\ A=a, L=l) = \\frac{P(Y=1, A=a, L=l)}{Pr(A=a, L=l)} = \\frac{P(Y=1, A=a, L=l)}{Pr(A=a\\ |\\ L=l)Pr(L=l)} = \\alpha + \\beta_1 a + \\beta_2 l$$\n",
    "\n",
    "Which can only be properly defined when $Pr(A=a\\ |\\ L=l) \\neq 0$. In the MSM case, naive methods can be applied without confounding control as long as we correctly estimate the weights. However, the denominator of the weights in (?) above is Pr(A=a\\ |\\ L=l) so that although the MSM does not require L it still requires the positivity assumption in order to estimate causal effects in each strata and the population. In practice, positivity can arise structural zeroes or random zeroes. In the first case positivity arises because  (?) is often estimated using a parametric model which smoothes over zeroes by borrowing information from other individuals with similar histories to those that, by chance, resulted in random zeroes. Researchers need to check for violations of positivity in their data \n",
    "\n",
    "but parametric models will still work because they smooth over the positivity.\n",
    "Formally, positivity holds when \n",
    "\n",
    "The expression for the IPTW involves division by this term. If, for an individual patient the probability of exposure is zero then the weights are not estimable.\n",
    "\n",
    "- Cole2008 points out that non-positivity by a time varying parameter poses an analytic challenge - we can quantify this challenge more than other papers have done.\n",
    "- although marginal structural models can be used to deal with time varying confounding, there does not seem to be much/any work on positivity in the time varying case.\n",
    "\n",
    "\n",
    "\n",
    "Positivity may arise by chance. For example, aspirin is a common treatment to prevent blood clots in patients suffering from coronary artery disease (CAD). People of all ages with CAD are at risk, but elderly people are disproportionately affected. In any given study younger patients \n",
    "\n",
    "The positivity assumption has received less attention than exchangeability in the literature on MSMs.  Two notable exceptions are \\citet{Cole009} and \\citet{Westereich2009} who have studied positivity in detail. Both papers aim to give some \n",
    "\n",
    "Both papers lack a formal simulation study, and neither have looked into detail on the time dependent situation.\n",
    "\n",
    "The trade-off between positivity and confounding bias is emphasized in Cole2008\n",
    "\n",
    "because it is always possible to apply a parametric model, researchers need to be careful when performing analysis and should investigate their data thoroughly. The two papers cited above offer practical advice but do not give an indication of large the effect of positivity is in practical settings. They focus more on the interaction with confounders for example.\n",
    "\n",
    "3. we can always estimate weights parametrically (why?) even when there are some strata where no individuals are exposed. Hence we need check for positivity. Otherwise we will not notice that these assumption is not met. The reason we can always estimate parametric models is that we smooth over the random zeroes by borrowing information from other individuals in the sample with histories similar to those that, by chance resulted in random zeroes.\n",
    "4. structural versus random zeroes\n",
    "5. explain how the zeroes appear in the clinical case and why this is more likely to occur due to protocols in the clinical case. This is touched on in Cole 2008. He points out that in studies where there is a contraindication there is a structural zero within those who have a contraindication. So they cannot possibly be treated because they have a contraindication. \n",
    "6. estimated weights with a mean far from one, or very extreme values indicate either non-positivity or model mispecification of the weight model.\n",
    "7. It is not always true that we want more finely tuned covariates for confounder control because the bias and variance of the effect estimate may increase with the number of categories. This is similar to the positivity masking example.\n",
    "8. Both the structural and exposure models must be correctly specified. \n",
    "9. Baseline versus time varying CD4 count.\n",
    "10. In most epidemiological settings this is guaranteed by experimental design.\n",
    "11. Although this paper uses a clinical setting with protocols as its motivation positivity can arise in practice. Our results are equally valid for other circumstances in which positivity may arise.\n",
    "12. Also think about how the number of categories of exposure increases the chance that one level of exposure will have a positivity.\n",
    "13. Westreich and Cole 2009 have suggested that methodological approaches are needed to weigh the resultant biases incurred when trading of confounding and positivity. The framework we use is flexible enough to allow this in a simulation setting.\n",
    "14. As a last point in Westreich and Cole 2009 they point out that in the time varying cnfounder case more research is needed but that g estimation or g cmputation might work.\n",
    "\n",
    "\n",
    "How can positivity arise?\n",
    "How does it arise when there are random zeroes?\n",
    "How does it arise in the clinical case that is the motivation for this thesis?\n",
    "\n",
    "Estimating the weights described in the previous section relies on four assumptions of consistency, exchangeability, positivity and no mispecification of the model \\citet{Cole2008}. In this thesis we examine the effect of positivity violations on our ability to recover the true model parameters from a given MSM.\n",
    "\n",
    "Another way that positivity can enter the analysis is if many confounders are identified and this splits the data into many small strata and as a result there is a higher chance of seeing random zeroes, or structural zeroes \\citet{cole2008}\n",
    "\n",
    "Consistency is the assumption that the actual outcome observed in the data is equal to the potential or counterfactual outcome under the observed treatment. \n",
    "\n",
    "The positivity problem is less well known \\citet{Cole2009_consistency paper} He points out that consistency and positivity are less well known because there is little written about them and the problems they create. But there is now a lot written on consistency, but still relatively little on positivity.\n",
    "\n",
    "Exchangeability is untestable - but has been studied\n",
    "Consistency is assumed to hold so cannot be studied\n",
    "positivity has not been widely studied.\n",
    "correct model specification has been studied quite a lot.\n",
    "put all of these in a simulation context, have they been systematically studied in a simulation context? If so under what algorithm have they been studied?\n",
    "\n",
    "\n",
    "Positivity is the condition that in every strata of the covariates there is a non-zero probability of receiving each level of treatment. In the one shot case represented by the DAG in figure 1 this would correspond to the formal statement $P(A\\ |\\ A_0, L) > 0$ while in the longitudinal case, incorporating the histories of the treatment and covariates, the positivity assumption can be stated \n",
    "\n",
    "$$P_\\tau(A_\\tau\\ |\\ \\bar A_{\\tau-1}, \\bar L_{\\tau}) > 0$$\n",
    "\n",
    "Also need to condition on survival up to the present time.\n",
    "\n",
    "When this is not met, the weights will be undefined, and the resulting estimates will be biased.\n",
    "\n",
    "from Westreich 2012: positivity requires that there are treated and untreated individuals at every combination of the values of the confounders at every combination of the values of the confounders in the observed data.\n",
    "\n",
    "Positivity can be violated if, for example, medical protocols demand that a doctor always treat a patient if a risk factor falls below a certain threshold.  there are both exposed and unexposed individuals at every level of the confounders (Cole, Hernan, 2008). If positivity is violated because doctors have protocols to treat patients when L falls below a certain level, then positivity is violated. If the structural bias occurs within levels of a time-dependent confounder then restriction or censoring may lead to bias whether one uses weighting or other methods (Cole and Hernan 2008). In fact, weighted estimates are more sensitive to random zeroes (Cole, Hernan, 2008)\n",
    "\n",
    "When positivity is violated, the weights are undefined because .... In this case the weights may lead to a biased estimate of the causal effect of A on Y. Examining the extent of this bias in MSMs will be the central purpose of this thesis.\n",
    "\n",
    "#### Definition 2: positivity\n",
    "\n",
    "(Appendix 2 Cole and Hernan 2008): Positivity states that there is a non-zero (i.e. positive) probability of receiving every level of exposure $X_{ij}$ for every combination of values of exposure and covariate histories that occur in individuals histories.\n",
    "\n",
    "In section V we will consider violations of positivity to examine the extent of the bias introduced into the estimates when the assumption of positivity is not met. The inverse probability weights are equal to the stanilized weights, only if positivity holds. If positovity does not hold, the weights are undefined and may result in biased estimates of the causal effects. \n",
    "\n",
    "As described above, the model only holds as long as key assumptions are met, and one of these is positivity. Introducing violations of positivity can be achieved by censoring observations.\n",
    "\n",
    "- Point out difficulty in simulation from this model.\n",
    "- talk about blocking a back door from U to Y?\n",
    "- Pearl and Robins (1995) probabilistic evaluation of sequential plans from causal models with hidden variables.\n",
    "- investigate the finite sample properties of positivity violations.\n",
    "\n",
    "You can diagnose exactly what positivity is and discuss the definition etc. But to give an inuitive example, think about how it links back to a situation where sicker patients receive treatment compared to others. So in the \"sick\" strata of the CD4 count **ALL** patients receive treatment which inflates the IPTW. This also affects how we think about the associational versus causal models. The causal effect might be 50/50 but because sicker patients get treatment the mortality ratio in the treated group is likely to be higher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static vs. Dynamic Strategies\n",
    "\n",
    "So far we have considered static strategies, in this section we desbcribe the differences between static and dynamic strategies. A static stratgey is one where the value where the values that $A$ will take depend on  for A is represented as:\n",
    "\n",
    "Where a(t) = 1 if the strategy specifies that a is to take the value 1 at time $t$. In contrast, a dynamic regime is any well-specified way of adjusting the choice of the next decision(treatment or dose to administer) in the light of previous information constitutes a dynamic decision (or treatment) strategy (Didelez arxiv).\n",
    "\n",
    "To our knowledge positivity violations in the dynamic case have not been considered in the literature.\n",
    "\n",
    "Hernan etal (2005) for looking at comparing dynamic regimes using artificial censoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related work\n",
    "\n",
    "Bryan et al paper fixes teh vector L at the beginning which means A never affects L which don't make no sense.\n",
    "\n",
    "Marginal structural models \n",
    "1. On MSMs - mainly hernan and Robins work\n",
    "2. On other work into positivity\n",
    "3. On Simulating from MSMs\n",
    "\n",
    "Why is practicality important? Cole paper highlights practical advice to practictioners. positivity can be violated in a practical setting because of two few strata, it can be the result of protocols in a clinical setting and it can be seen as a trade-off between exchangeability (and we need more measured predictors to maintain exchangeability) and positivity where more predictors leads to more likely a zero problem.\n",
    "\n",
    "Marginal Structural Models\n",
    "\n",
    "\n",
    "The aim is to be able to simulate the survival function of a desired MSM under the intervention $do(\\bar a)$\n",
    "\n",
    "Several studies have developed algorithms for simulating data from marginal structural models in the presence of time dependent confounding. An early example is \\citet{bryan2004} who study estimators of the causal effec of a time dependent treatment on survival outcomes. They compare naive estimators with IPTW and a treatment orthogonalized estimator which is also developed in the paper. This study shares similarities with \\citet{Havercroft2012} \n",
    "\n",
    "- stay on treatment after treatment starts\n",
    "- treatment regime is determined by t* (starting point of treatment because it is a vector of {0, 0, 0, 1, 1, 1}\n",
    "- they motivate a logistic model for the haxard function, they use a discrete equivalent to the hazrd function (link to citation about farington study.)\n",
    "- the survival time U is directly linked to the survival outcome -> here it is good to provide more inuition.\n",
    "- need to understand why it is linked.\n",
    "\n",
    "    Work that proposes a different method to solve the same problem.\n",
    "    Work that uses the same proposed method to solve a different problem.\n",
    "    A method that is similar to your method that solves a relatively similar problem.\n",
    "    A discussion of a set of related problems that covers your problem domain.\n",
    "\n",
    "These previous studies have lacked a systematic investigation of positivity violations in a simulation setting. It is unknown, for example, how large an effect a violation of positivity has and how it is affected by the sample siz, threshold etc etc.\n",
    "\n",
    "1. marginal structural models literature\n",
    "\n",
    "\n",
    "\n",
    "2. Simulating from marginal structural models\n",
    "3. positivity\n",
    "4. connect limited work on simulating from models to positivity\n",
    "5. Explain why we choose the Havercroft simulation of the others, specifically why it helps us to incorporate violations of positivity in our analysis.\n",
    "\n",
    "Bryan etal 2010 have a similar focus as Havercroft 2012 in that they develop an algorithm for simulation from a given MSM and they use this algorithm to compare IPTW methods to naive regression methods.\n",
    "\n",
    "Several studies have considered simulation from margial structural models. The finite-sample properties of marginal structural proportional hazards models has been considered by \\citet{Westreich2009}. \n",
    "\n",
    "- What is their focus?\n",
    "- how do they simulate\n",
    "- what do they find (in terms of MSE, SE, etc.\n",
    "- how does it differ from HD (2012)\n",
    "\n",
    "Young etal (2014) also provide a simulation algorithm for simulating longitudinal data from a known Cox MSM. \n",
    "\n",
    "- What is their focus?\n",
    "\n",
    "to compare IPW and standard regression based techniques. This is not the subject of this thesis.\n",
    "\n",
    "Comparing IPW and standard regression based estimates in the absence of model misspecification. This allows for complete isolation of any type of bias. This approach involves simulating data from a standard parametrization of the likelihood and solving for the underlying Cox MSM\n",
    "\n",
    "- how do they simulate\n",
    "\n",
    "\n",
    "\n",
    "- what do they find (in terms of MSE, SE, etc.\n",
    "- how does it differ from HD (2012)\n",
    "\n",
    "we describe an approach to Cox MSM data generation that allows for a comparison of the bias of IPW estimates versus that of standard regression-based estimates in the complete absence of model misspecification\n",
    "\n",
    "- could do this section in comparison to Havercroft and didelez, explain how their algorithm works first and then describe the related work by linking differences in their algorithm to earlier or other algorithms.\n",
    "\n",
    "ALgorithm:\n",
    "\n",
    "1. generate survival under no treatment from a weibull\n",
    "2. generate survival times under the ten non-zero treatment levels.\n",
    "3. \n",
    "\n",
    "Bryan et al (2010)\n",
    "Cole (2008) for more of a discussion about positivity, with an actual observed data example.\n",
    "While Cole (2008) have looked at positivity in an observational setting, to our knowledge no study has looked at positivity violations within a simulation setting.\n",
    "\n",
    "Cole and Hernan 2008 have examined the four assumptions underlying IPW using a study on real data from the HAART SWISS study. \n",
    "\n",
    "1. 2 paper on simulation + Bryan paper - why this simulation method is different from other sim papers\n",
    "2. related work - Judea pearl, Robins, econometrics, related and broader literature on MSM\n",
    "3. positivity long discussion in Hernan and cole and the warnings but no simulation study in that paper.\n",
    "They study the positivity but not the effect and there is nothing in the havercroft paper on this either.\n",
    "\n",
    "- \n",
    "- Major point is that there are a number of ways of simulating from marginal structural models. But, we need one where we can mess with the positivity. Other methods are not suitable for this.\n",
    "- G formula simulation\n",
    "- exposure, confounder feedback loop\n",
    "- treatment\n",
    "- outcome variable\n",
    "- causal effect of the treatment variable on the outcome. \n",
    "- external intervention\n",
    "- Explain the do notation, and what presicely is meant in the case  \n",
    "- used to estimate the joint effect of time dependent treatments on survival\n",
    "- Need to stronly link the time dependent confounding to the MSM, do we choose this class of models because of their relationship with time dep confounding? Yes, marginal strictural models are used with TDC\n",
    "- The causal graph helps (according to Pearl pp. 40) to bridge statistics into causality\n",
    "- There is a key part to this, which is that we do not observe confounding, this seems to be what motivates the use of the MSM class of models.\n",
    "- Counterfactuals need to be addressed here to make it clear this is not the purpose of the thesis.\n",
    "\n",
    "- Robins (2000) have demonstrated that in the presense of time dependent confounders, standard approaches for asjusting for confounding are biased.\n",
    "- A covariate that is a risk factor for, or predictor of the event of interest (Y) (from Robins 2000). This defines a time dependent covariate\n",
    "- And also past exposure determines the level of the covariate.\n",
    "- Works under a set of assumptions (consistency, exchanchability, positivity and no mispecification of the model used to estimate the weights\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "\n",
    "## Simulating from a static MSM\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "We wish to simulate survival data in discrete time $t = 0, \\dots, T$ for $n$ subjects. At baseline $t=0$ all subjects are assumed to be at risk of failure so that $Y_0 = 0$. For each time period $t = 0, \\dots, T$ a subject may either be on treatment,  $A_t = 1$, or not on treatment, $A_t = 0$. All patients are assumed to be not on treatment before the study begins. Once a patient commences treatment, they remain on treatment in all subsequent periods until failure or the end of follow-up. In each time period $L_t$ is the value of a covariate measured at time $t$. In the simulated data, $L_t$ behaves in a similar manner to CD4 counts such that a low value of $L_t$ represents a more severe illness and hence a higher probability of both tratemnt and failure in the following period. In addition to $L_t$, the variable $U_t$ represents subject specific general health at time $t$. Although we will simulate $U_t$, in a real world application $U_t$ is an unmeasured confounder which  \n",
    "\n",
    "Each time period is either a check up visit or is between two check up visits. If $t$ is a check-up visit and treatment has not yet commenced, $L_t$ is measured and a decision is made on whether to commence treatment. Between visits, treatment remains unchanged at the value recorded at the previous visit. Similarly, $L_t$ which is only measured when $t$ is a visit, alos remains unchanged.\n",
    "\n",
    "We represent the history of a random variable with an over bar. For example, the vector representing the treatment history of the variable A is represented by $\\bar A = [a_0, a_1, \\dots, a_m]$ where $m=T$ if the subject survives until the end of follow-up, or $m < T$ otherwise. Prior to basline both $A = 0$ for all subjects.\n",
    "\n",
    "- explain what $U$ is and how it relates to the simulation design/algorithm\n",
    "- Be more specific on $Y$\n",
    "- L_t is a measured confounder\n",
    "- U_t is an unmeasured confounder.\n",
    "\n",
    "## Simulation Algorithm\n",
    "\n",
    "### Algorithm \n",
    "\n",
    "Next, we describe the algorithm used to simulate data from our chosen marginal structural model under time dependent confounding. In the following section we discuss in detail how the algorithm works and the salient features for this thesis. The algorithm is taken from \\citet{Havercroft2012} who generate data on $n$ patients, for $k$ time periods. The outer loop in the following algorithm $i \\in {1, \\dots, n}$ , refers to the patients while the inner loop $t \\in {1, \\dots, T}$ refers to the subsject specific time periods from baseline to failure or the end of the study. There will be at least one, and at most $T$ records for each patient.\n",
    "\n",
    "\\begin{algorithm}[H]\n",
    "\\SetAlgoLined\n",
    "\\KwResult{Marginal Structural Model Under Time Dependent Confounding}\n",
    " \\For{i in 1, \\dots , n}{\n",
    "  $U_{0, i} \\sim U[0, 1]$\\\\\n",
    "  $\\epsilon_{0, i} \\sim N(\\mu, \\sigma^2)$\\\\\n",
    "  $L_{0, i} \\gets F^{-1}_{\\Gamma(k,\\theta)}(U_{i, 0}) + \\epsilon_{0, i}$\\\\\n",
    "  $A_{-1, i} \\gets 0$\\\\\n",
    "  $A_{0, i} \\gets Bern(expit(\\theta_0 + \\theta_2 (L_{0, i} - 500)))$\\\\\n",
    "  \\If{$A_{0, i}= 1$}{\n",
    "   $T^* \\gets 0$;\n",
    "  }\n",
    "  $\\lambda_{0, i} \\gets expit(\\gamma_0 + \\gamma_2 A_{0, i})$\\\\\n",
    "  \\eIf{$\\lambda_{0, i} \\ge U_{0, i}$}{\n",
    "   $Y_{1, i} \\gets 0$\\\\\n",
    "   }{\n",
    "   $Y_{1, i} \\gets 1$\\\\\n",
    "  }\n",
    "  \\For{$k in 1, \\dots , T$}{\n",
    "   \\If{$Y_{t, i} = 0$}{\n",
    "    $\\Delta_{t, i} \\sim N(\\mu_2, \\sigma^2_2)$\\\\\n",
    "    $U_{t, i} \\gets min(1, max(0, U_{t-1, i} + \\Delta_{t, i}))$\\\\\n",
    "    \\eIf{$t \\neq 0\\ (mod\\ k)$}{\n",
    "     $L_{t, i} \\gets L_{t-1, i}$\\\\\n",
    "     $A_{t, i} \\gets A_{t-1, i}$\\\\\n",
    "     }{\n",
    "     $\\epsilon_{t, i} \\sim N(100(U_{t, i}-2), \\sigma^2)$\\\\\n",
    "     $L_{t, i} \\gets max(0, L_{t-1, i} + 150A_{t-k,i}(1-A_{t-k-1,i}) + \\epsilon_{t, i})$\\\\\n",
    "     \\eIf{$A_{t-1, i} = 0$}{\n",
    "      $A_{t, i} \\sim Bern(expit(\\theta_0 + \\theta_1t + \\theta_2(L_{t, i}-500)))$\\\\\n",
    "      }{\n",
    "      $A_{t, i} \\gets 1$\\\\\n",
    "     }\n",
    "     \\If{$A_{t, i} = 1 \\and A_{t-k, i} = 0$}{\n",
    "      $T^* \\gets t$\\\\\n",
    "     }\n",
    "    }\n",
    "    $\\lambda_{t, i} \\gets expit()\\gamma_0 + \\gamma_1[(1 - A_{t, i})t + A_{t, i}T^*] + \\gamma_2 A_{t, i} + \\gamma_3 A_{t, i}(t-T^*))$\\\\\n",
    "    \\eIf{$1 - \\prod_{\\tau=0}^t(1 - \\lambda_{\\tau, i}) \\ge U_{0, i}$}{\n",
    "     $Y_{t+1, i} = 1$\\\\     \n",
    "    }{\n",
    "     $Y_{t+1, i} = 0$\\\\\n",
    "    }\n",
    "   }\n",
    "  }\n",
    " }\n",
    " \\caption{Simulation Algoirthm MSM}\n",
    "\\end{algorithm}\n",
    "\n",
    "Within the inner loop ($t \\in {1, \\dots, T}$) we see that the data is only updated at time $t \\neq 0\\ (mod\\ k)$, where $k$ refers to evenly spaced check-up visits. If $t$ is not a check-up visit the values of $A_t$ and $L_t$ are the same as in $t-1$. When $t$ is a visit $A_t$ and $L_t$ are updated.\n",
    "\n",
    "- if treatment has been commenced then a subject may feel extra benefit if more time has elapsed since treatment began\n",
    "- L_t affects A_t and also Y_t\n",
    "- explain starting values for A and Y are all zero (except L maybe)\n",
    "\n",
    "In order to operationalize the Algorithm 1 we need to choose parameters for $()$. In their paper \\citet{Havercroft2012} use values that simulate data with a close resemblance to the Swiss HIV Cohort Study. We postpone disussion of the patameters in Algorithm 1 to section 2.4. We just need to state that we follow their parameters because this is not the focus of this thesis.\t\n",
    "\n",
    "### Discussion of how algorithm works \n",
    "\n",
    "The algorithm of \\citet{Havercroft2012} works by factorizing the joint density of the histories of the four variables in the analysis.  \n",
    "\n",
    "- Important is that the form of the MSM is not specified intil the last stage\n",
    "- role of $U_{0, i}$\n",
    "- How does positivity enter the analysis?\n",
    "- Why this model is important in terms of positivity.\n",
    "\n",
    "\n",
    "## Constructing IPT weights\n",
    "\n",
    "Inverse Probability of Treatment weights can be used to adjust for measured confounding and selection bias in marginal structural models. Link back to pseudo population idea in previous section. This method relies on four assumptions consistency, exchangeability, positivity and no mispecification of the model used to estimate the weights \\citet{Cole2008}. Unstabilized weights are defined as:\n",
    "\n",
    "$$w_{t,i} = \\frac{1}{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "Where the denominator is the probability that the subject received the particular treatment history that they were observed to receive up to time $t$, given their prior observed treatment and covariate histories (Havercroft, Didelez, 2012). The probabilities $p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})$ may vary greatly between subjects when the covariate history is strongly asscoaited with treatment. In terms of the resulting pseudopopulation, very small values of the unstabilized weights for some subjects would result in a small number of observations dominating the weighted analysis. The result is that the IPTW estimator of the coefficients will have a large variance, and will fail to be normally distributed. This variability can be mitigated by using the following stabilized weights \n",
    "\n",
    "$$sw_{it} = \\frac{\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i})} {\\prod_{\\tau=0} ^ t p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i})}$$ \n",
    "\n",
    "In the case that there is no confounding the denominator probabiliies in the stabilized weights reduce to $p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i})$ and $sw_{it}=1$ so that each subject contributes the same weight. In the case of confounding this will not be the case and the stabilized weight will vary around 1. \n",
    "\n",
    "In practice, we estimate the weights from the data using a pooled logistic model for the numerator and denominator probabilities. The histories of the treatment and covariates are included in the probabilities. In practice Specifically, following Havercroft and Didelez (2012), we estimate the model where the visit is only the visits every check up time. Between check ups both the treatment and covariate remain the same. Other ways of doing this include a spline function over the months to create a smooth function between the visits. Another difference might be to use a coxph function instead of logistic function\n",
    "\n",
    "$$logit\\ p_{\\tau} (A_{\\tau, i}\\ |\\ \\bar A_{\\tau-1, i}, \\bar L_{\\tau, i}) = \\alpha_0 + \\alpha_1 k + alpha_2 a_{k-1} + \\dots + alpha_k a_0 + $$\n",
    "\n",
    "We have several options for estimating these weights. We could use a coxph model, or a logistic model.\n",
    "\n",
    "\n",
    "## Simulation Set-up\n",
    "\n",
    "We follow the simulation set-up of Havercroft, Didelez (2012) which is based on parameters that closely match the Swiss HIV Cohort Study (HAART). \n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "- check the distribution of the weights that come out of the model (see Cole 2008). This would allow us to see weight model mispecifications. Not a problem in the simuation case.\n",
    "- compare the bias, se, MSE, and 95% confidence interval\n",
    "- compare all of these in the positivity violation and non-positivty violation case.\n",
    "\n",
    "- explain to some extent monte-carlo standard error.\n",
    "- we don't confirm the results of the havercroft of Bryan papers, instead refer readers to these papers to see how IPTW outperforms the naive estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Case\n",
    "\n",
    "## The problem of simulating from a MSM under a dynamic strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violations of Positivity\n",
    "\n",
    "The motivation for using the algorith of \\citet{Havercroft2012} is that we have control over how $L$ affects $Y$, so we can itroduce positivity using a threshold. In other algorithms there would be a direct link between $L$ and $Y$, this would be a problem because altering treatment decisions based on $L$ would affect $Y$ directly.  \n",
    "- creating an artificial population in which positivity is violated in specific ways.\n",
    "\n",
    "## Extended discussion of algorithm linking to positivity\n",
    "\n",
    "As described in the introduction, one assumption of the model is that there is a non-zero probability of the event occuring at every startum of the covariate.\n",
    "\n",
    "- When previous covariates like CD4 count are strongly associated with treatment the probabilities in the denominator of the ustabilized weights may vary greatly. Because we are foricing positivity by using a treatment rule when L falls below a threshold and A is then eaual to one, we create a strong association between A and L -> hence the unstabilized weights would vary. (Robins et al 2000 pp. 553)\n",
    "- present the algorithm again with positivity violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Conclusion\n",
    "\n",
    "\n",
    "\n",
    "## Limitations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
